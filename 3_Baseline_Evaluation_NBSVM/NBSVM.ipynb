{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard imports for `ktrain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ktrain\n",
    "from ktrain import text\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build test and training sets & define preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 50000\n",
    "MAXLEN = 8000\n",
    "NGRAM_RANGE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8 (if wrong, set manually)\n",
      "language: en\n",
      "Word Counts: 17531\n",
      "Nrows: 647\n",
      "647 train sequences\n",
      "train sequence lengths:\n",
      "\tmean : 1501\n",
      "\t95percentile : 3939\n",
      "\t99percentile : 7668\n",
      "x_train shape: (647,8000)\n",
      "y_train shape: (647, 2)\n",
      "Is Multi-Label? False\n",
      "81 test sequences\n",
      "test sequence lengths:\n",
      "\tmean : 1510\n",
      "\t95percentile : 4141\n",
      "\t99percentile : 5507\n",
      "x_test shape: (81,8000)\n",
      "y_test shape: (81, 2)\n"
     ]
    }
   ],
   "source": [
    "train, test, preproc = text.texts_from_csv('../data/train_80_10_10.csv',\n",
    "                                          'cleaned_contents',\n",
    "                                          label_columns=['Discrimination_Label'],\n",
    "                                          val_filepath='../data/val_80_10_10.csv',\n",
    "                                          max_features=NUM_WORDS,\n",
    "                                          maxlen=MAXLEN,\n",
    "                                          ngram_range=NGRAM_RANGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "compiling word ID features...\n",
      "maxlen is 8000\n",
      "building document-term matrix... this may take a few moments...\n",
      "rows: 1-647\n",
      "computing log-count ratios...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "model = text.text_classifier('nbsvm', train, preproc=preproc)\n",
    "learner = ktrain.get_learner(model, train_data=train, val_data=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 8000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 8000, 2)      35064       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 8000, 1)      17532       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 2, 1)         0           embedding[0][0]                  \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2)            0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 2)            0           flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 52,596\n",
      "Trainable params: 17,532\n",
      "Non-trainable params: 35,064\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learner.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find a good initial learning rate\n",
    "\n",
    "This is a method that was developed at the Naval Research Laboratory.  It's been promoted by Jeremy Howard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulating training for different learning rates... this may take a few moments...\n",
      "Train on 647 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ktrain/core.py:476: UserWarning: max_epochs is being set to 5 since steps per epoch is small. If you wish to estimate LR using more epochs, set max_epochs manually.\n",
      "  'If you wish to estimate LR using more epochs, set max_epochs manually.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647/647 [==============================] - 1s 1ms/sample - loss: 0.8620 - accuracy: 0.6121\n",
      "Epoch 2/5\n",
      "647/647 [==============================] - 0s 506us/sample - loss: 0.8262 - accuracy: 0.6151\n",
      "Epoch 3/5\n",
      "647/647 [==============================] - 0s 485us/sample - loss: 0.6357 - accuracy: 0.6461\n",
      "Epoch 4/5\n",
      "647/647 [==============================] - 0s 457us/sample - loss: 0.4190 - accuracy: 0.8733\n",
      "Epoch 5/5\n",
      "416/647 [==================>...........] - ETA: 0s - loss: 5.5935 - accuracy: 0.8462\n",
      "\n",
      "done.\n",
      "Visually inspect loss plot and select learning rate associated with falling loss\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.lr_find(show_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.001...\n",
      "Train on 647 samples, validate on 81 samples\n",
      "Epoch 1/1024\n",
      "647/647 [==============================] - 1s 1ms/sample - loss: 0.6765 - accuracy: 0.6414 - val_loss: 0.6353 - val_accuracy: 0.6543\n",
      "Epoch 2/1024\n",
      "647/647 [==============================] - 0s 520us/sample - loss: 0.4936 - accuracy: 0.7697 - val_loss: 0.6586 - val_accuracy: 0.6790\n",
      "Epoch 3/1024\n",
      "544/647 [========================>.....] - ETA: 0s - loss: 0.4153 - accuracy: 0.8290\n",
      "Epoch 00003: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n",
      "647/647 [==============================] - 0s 488us/sample - loss: 0.4119 - accuracy: 0.8346 - val_loss: 0.6698 - val_accuracy: 0.6790\n",
      "Epoch 4/1024\n",
      "647/647 [==============================] - 0s 484us/sample - loss: 0.3556 - accuracy: 0.8810 - val_loss: 0.6448 - val_accuracy: 0.6420\n",
      "Epoch 5/1024\n",
      "544/647 [========================>.....] - ETA: 0s - loss: 0.3328 - accuracy: 0.9026\n",
      "Epoch 00005: Reducing Max LR on Plateau: new max lr will be 0.00025 (if not early_stopping).\n",
      "647/647 [==============================] - 0s 471us/sample - loss: 0.3328 - accuracy: 0.9011 - val_loss: 0.6628 - val_accuracy: 0.6667\n",
      "Epoch 6/1024\n",
      "544/647 [========================>.....] - ETA: 0s - loss: 0.3125 - accuracy: 0.9210Restoring model weights from the end of the best epoch.\n",
      "647/647 [==============================] - 0s 491us/sample - loss: 0.3128 - accuracy: 0.9150 - val_loss: 0.6452 - val_accuracy: 0.6296\n",
      "Epoch 00006: early stopping\n",
      "Weights from best epoch have been loaded into model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdcc495a128>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.autofit(1e-3, early_stopping=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine results on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.36      0.46        33\n",
      "           1       0.66      0.85      0.75        48\n",
      "\n",
      "    accuracy                           0.65        81\n",
      "   macro avg       0.65      0.61      0.60        81\n",
      "weighted avg       0.65      0.65      0.63        81\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[12, 21],\n",
       "       [ 7, 41]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.validate(class_names=preproc.get_classes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test_80_10_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = list(test.cleaned_contents)\n",
    "y_test = np.array(test.Discrimination_Label, dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ktrain.get_predictor(learner.model, preproc=preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.38      0.54        37\n",
      "           1       0.65      0.98      0.78        44\n",
      "\n",
      "    accuracy                           0.70        81\n",
      "   macro avg       0.79      0.68      0.66        81\n",
      "weighted avg       0.78      0.70      0.67        81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_hat = np.array(predictor.predict(x_test), dtype='int64')\n",
    "print(classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=1\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.506</b>, score <b>0.022</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.088\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 83.72%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.066\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(0, 100.00%, 72.01%); opacity: 0.92\" title=\"-0.049\">as</span><span style=\"opacity: 0.80\"> the accused is the </span><span style=\"background-color: hsl(120, 100.00%, 74.49%); opacity: 0.91\" title=\"0.043\">sole</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 71.16%); opacity: 0.93\" title=\"-0.051\">breadwinner</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 68.08%); opacity: 0.94\" title=\"-0.059\">for</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 60.00%); opacity: 1.00\" title=\"-0.081\">his</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.51%); opacity: 0.85\" title=\"-0.019\">family</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 75.62%); opacity: 0.90\" title=\"-0.040\">i</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.88%); opacity: 0.83\" title=\"0.011\">reduce</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.70%); opacity: 0.85\" title=\"-0.020\">his</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.12%); opacity: 0.84\" title=\"0.018\">sentence</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.12%); opacity: 0.80\" title=\"0.002\">by</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 68.71%); opacity: 0.94\" title=\"-0.057\">two</span><span style=\"opacity: 0.80\"> years.</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.explain('As the accused is the sole breadwinner for his family, I reduce his sentence by two years.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run five experiments - retain results and save the best model based on performance on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights have been reset.\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.001...\n",
      "Train on 647 samples, validate on 81 samples\n",
      "Epoch 1/1024\n",
      "647/647 [==============================] - 0s 491us/sample - loss: 0.7341 - accuracy: 0.5997 - val_loss: 0.6352 - val_accuracy: 0.6420\n",
      "Epoch 2/1024\n",
      "647/647 [==============================] - 0s 485us/sample - loss: 0.5128 - accuracy: 0.7512 - val_loss: 0.6396 - val_accuracy: 0.6667\n",
      "Epoch 3/1024\n",
      "544/647 [========================>.....] - ETA: 0s - loss: 0.3995 - accuracy: 0.8419\n",
      "Epoch 00003: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n",
      "647/647 [==============================] - 0s 508us/sample - loss: 0.4037 - accuracy: 0.8470 - val_loss: 0.7572 - val_accuracy: 0.7037\n",
      "Epoch 4/1024\n",
      "647/647 [==============================] - 0s 495us/sample - loss: 0.3349 - accuracy: 0.9088 - val_loss: 0.6203 - val_accuracy: 0.6667\n",
      "Epoch 5/1024\n",
      "647/647 [==============================] - 0s 531us/sample - loss: 0.3148 - accuracy: 0.9042 - val_loss: 0.7015 - val_accuracy: 0.7037\n",
      "Epoch 6/1024\n",
      "544/647 [========================>.....] - ETA: 0s - loss: 0.3076 - accuracy: 0.9283\n",
      "Epoch 00006: Reducing Max LR on Plateau: new max lr will be 0.00025 (if not early_stopping).\n",
      "647/647 [==============================] - 0s 479us/sample - loss: 0.2998 - accuracy: 0.9335 - val_loss: 0.6555 - val_accuracy: 0.6543\n",
      "Epoch 7/1024\n",
      "647/647 [==============================] - 0s 483us/sample - loss: 0.2806 - accuracy: 0.9351 - val_loss: 0.6398 - val_accuracy: 0.6543\n",
      "Epoch 8/1024\n",
      "544/647 [========================>.....] - ETA: 0s - loss: 0.2732 - accuracy: 0.9338\n",
      "Epoch 00008: Reducing Max LR on Plateau: new max lr will be 0.000125 (if not early_stopping).\n",
      "647/647 [==============================] - 0s 492us/sample - loss: 0.2731 - accuracy: 0.9351 - val_loss: 0.6613 - val_accuracy: 0.6543\n",
      "Epoch 9/1024\n",
      "544/647 [========================>.....] - ETA: 0s - loss: 0.2655 - accuracy: 0.9467Restoring model weights from the end of the best epoch.\n",
      "647/647 [==============================] - 0s 495us/sample - loss: 0.2658 - accuracy: 0.9444 - val_loss: 0.6673 - val_accuracy: 0.6543\n",
      "Epoch 00009: early stopping\n",
      "Weights from best epoch have been loaded into model.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.45      0.53        33\n",
      "           1       0.68      0.81      0.74        48\n",
      "\n",
      "    accuracy                           0.67        81\n",
      "   macro avg       0.65      0.63      0.63        81\n",
      "weighted avg       0.66      0.67      0.65        81\n",
      "\n",
      "Model weights have been reset.\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.001...\n",
      "Train on 647 samples, validate on 81 samples\n",
      "Epoch 1/1024\n",
      "647/647 [==============================] - 0s 481us/sample - loss: 0.6948 - accuracy: 0.6538 - val_loss: 0.6344 - val_accuracy: 0.6667\n",
      "Epoch 2/1024\n",
      "647/647 [==============================] - 0s 502us/sample - loss: 0.4643 - accuracy: 0.7898 - val_loss: 0.6540 - val_accuracy: 0.6790\n",
      "Epoch 3/1024\n",
      "647/647 [==============================] - 0s 476us/sample - loss: 0.3796 - accuracy: 0.8655 - val_loss: 0.6137 - val_accuracy: 0.6667\n",
      "Epoch 4/1024\n",
      "647/647 [==============================] - 0s 491us/sample - loss: 0.3220 - accuracy: 0.9088 - val_loss: 0.6407 - val_accuracy: 0.6667\n",
      "Epoch 5/1024\n",
      "544/647 [========================>.....] - ETA: 0s - loss: 0.2918 - accuracy: 0.9210\n",
      "Epoch 00005: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n",
      "647/647 [==============================] - 0s 501us/sample - loss: 0.2873 - accuracy: 0.9274 - val_loss: 0.6965 - val_accuracy: 0.7037\n",
      "Epoch 6/1024\n",
      "647/647 [==============================] - 0s 466us/sample - loss: 0.2611 - accuracy: 0.9459 - val_loss: 0.6376 - val_accuracy: 0.6420\n",
      "Epoch 7/1024\n",
      "544/647 [========================>.....] - ETA: 0s - loss: 0.2490 - accuracy: 0.9393\n",
      "Epoch 00007: Reducing Max LR on Plateau: new max lr will be 0.00025 (if not early_stopping).\n",
      "647/647 [==============================] - 0s 484us/sample - loss: 0.2505 - accuracy: 0.9413 - val_loss: 0.6764 - val_accuracy: 0.6790\n",
      "Epoch 8/1024\n",
      "544/647 [========================>.....] - ETA: 0s - loss: 0.2402 - accuracy: 0.9577Restoring model weights from the end of the best epoch.\n",
      "647/647 [==============================] - 0s 483us/sample - loss: 0.2387 - accuracy: 0.9583 - val_loss: 0.6687 - val_accuracy: 0.6543\n",
      "Epoch 00008: early stopping\n",
      "Weights from best epoch have been loaded into model.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.45      0.53        33\n",
      "           1       0.68      0.81      0.74        48\n",
      "\n",
      "    accuracy                           0.67        81\n",
      "   macro avg       0.65      0.63      0.63        81\n",
      "weighted avg       0.66      0.67      0.65        81\n",
      "\n",
      "Model weights have been reset.\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.001...\n",
      "Train on 647 samples, validate on 81 samples\n",
      "Epoch 1/1024\n",
      "647/647 [==============================] - 0s 495us/sample - loss: 0.7181 - accuracy: 0.6213 - val_loss: 0.6403 - val_accuracy: 0.6543\n",
      "Epoch 2/1024\n",
      "647/647 [==============================] - 0s 528us/sample - loss: 0.4880 - accuracy: 0.7697 - val_loss: 0.6770 - val_accuracy: 0.6667\n",
      "Epoch 3/1024\n",
      "544/647 [========================>.....] - ETA: 0s - loss: 0.3854 - accuracy: 0.8548\n",
      "Epoch 00003: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n",
      "647/647 [==============================] - 0s 498us/sample - loss: 0.3787 - accuracy: 0.8578 - val_loss: 0.6642 - val_accuracy: 0.6543\n",
      "Epoch 4/1024\n",
      "647/647 [==============================] - 0s 494us/sample - loss: 0.3212 - accuracy: 0.9026 - val_loss: 0.6558 - val_accuracy: 0.6667\n",
      "Epoch 5/1024\n",
      "544/647 [========================>.....] - ETA: 0s - loss: 0.3003 - accuracy: 0.9173\n",
      "Epoch 00005: Reducing Max LR on Plateau: new max lr will be 0.00025 (if not early_stopping).\n",
      "647/647 [==============================] - 0s 483us/sample - loss: 0.3013 - accuracy: 0.9165 - val_loss: 0.6927 - val_accuracy: 0.6543\n",
      "Epoch 6/1024\n",
      "544/647 [========================>.....] - ETA: 0s - loss: 0.2874 - accuracy: 0.9338Restoring model weights from the end of the best epoch.\n",
      "647/647 [==============================] - 0s 491us/sample - loss: 0.2847 - accuracy: 0.9351 - val_loss: 0.6823 - val_accuracy: 0.6543\n",
      "Epoch 00006: early stopping\n",
      "Weights from best epoch have been loaded into model.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.27      0.39        33\n",
      "           1       0.65      0.92      0.76        48\n",
      "\n",
      "    accuracy                           0.65        81\n",
      "   macro avg       0.67      0.59      0.57        81\n",
      "weighted avg       0.67      0.65      0.61        81\n",
      "\n",
      "Model weights have been reset.\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.001...\n",
      "Train on 647 samples, validate on 81 samples\n",
      "Epoch 1/1024\n",
      "647/647 [==============================] - 0s 488us/sample - loss: 0.7192 - accuracy: 0.6430 - val_loss: 0.6922 - val_accuracy: 0.6790\n",
      "Epoch 2/1024\n",
      "647/647 [==============================] - 0s 487us/sample - loss: 0.4569 - accuracy: 0.8161 - val_loss: 0.6873 - val_accuracy: 0.7160\n",
      "Epoch 3/1024\n",
      "647/647 [==============================] - 0s 482us/sample - loss: 0.3701 - accuracy: 0.8887 - val_loss: 0.6830 - val_accuracy: 0.6914\n",
      "Epoch 4/1024\n",
      "647/647 [==============================] - 0s 491us/sample - loss: 0.3176 - accuracy: 0.9134 - val_loss: 0.6134 - val_accuracy: 0.6543\n",
      "Epoch 5/1024\n",
      "647/647 [==============================] - 0s 503us/sample - loss: 0.2843 - accuracy: 0.9274 - val_loss: 0.6826 - val_accuracy: 0.6790\n",
      "Epoch 6/1024\n",
      "544/647 [========================>.....] - ETA: 0s - loss: 0.2595 - accuracy: 0.9430\n",
      "Epoch 00006: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n",
      "647/647 [==============================] - 0s 478us/sample - loss: 0.2558 - accuracy: 0.9459 - val_loss: 0.6832 - val_accuracy: 0.6790\n",
      "Epoch 7/1024\n",
      "647/647 [==============================] - 0s 463us/sample - loss: 0.2346 - accuracy: 0.9583 - val_loss: 0.6659 - val_accuracy: 0.6667\n",
      "Epoch 8/1024\n",
      "544/647 [========================>.....] - ETA: 0s - loss: 0.2223 - accuracy: 0.9651\n",
      "Epoch 00008: Reducing Max LR on Plateau: new max lr will be 0.00025 (if not early_stopping).\n",
      "647/647 [==============================] - 0s 485us/sample - loss: 0.2247 - accuracy: 0.9614 - val_loss: 0.6727 - val_accuracy: 0.6790\n",
      "Epoch 9/1024\n",
      "640/647 [============================>.] - ETA: 0s - loss: 0.2172 - accuracy: 0.9656Restoring model weights from the end of the best epoch.\n",
      "647/647 [==============================] - 0s 507us/sample - loss: 0.2159 - accuracy: 0.9660 - val_loss: 0.6684 - val_accuracy: 0.6667\n",
      "Epoch 00009: early stopping\n",
      "Weights from best epoch have been loaded into model.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.39      0.48        33\n",
      "           1       0.67      0.83      0.74        48\n",
      "\n",
      "    accuracy                           0.65        81\n",
      "   macro avg       0.64      0.61      0.61        81\n",
      "weighted avg       0.65      0.65      0.64        81\n",
      "\n",
      "Model weights have been reset.\n",
      "reduce_on_plateau automatically enabled at patience=2\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.001...\n",
      "Train on 647 samples, validate on 81 samples\n",
      "Epoch 1/1024\n",
      "647/647 [==============================] - 0s 519us/sample - loss: 0.6862 - accuracy: 0.6553 - val_loss: 0.6917 - val_accuracy: 0.6420\n",
      "Epoch 2/1024\n",
      "647/647 [==============================] - 0s 478us/sample - loss: 0.4501 - accuracy: 0.8083 - val_loss: 0.6369 - val_accuracy: 0.6790\n",
      "Epoch 3/1024\n",
      "647/647 [==============================] - 0s 494us/sample - loss: 0.3726 - accuracy: 0.8624 - val_loss: 0.6473 - val_accuracy: 0.6543\n",
      "Epoch 4/1024\n",
      "647/647 [==============================] - 0s 493us/sample - loss: 0.3161 - accuracy: 0.9073 - val_loss: 0.6239 - val_accuracy: 0.6667\n",
      "Epoch 5/1024\n",
      "647/647 [==============================] - 0s 495us/sample - loss: 0.2770 - accuracy: 0.9335 - val_loss: 0.7115 - val_accuracy: 0.6790\n",
      "Epoch 6/1024\n",
      "640/647 [============================>.] - ETA: 0s - loss: 0.2528 - accuracy: 0.9484\n",
      "Epoch 00006: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n",
      "647/647 [==============================] - 0s 519us/sample - loss: 0.2523 - accuracy: 0.9490 - val_loss: 0.6555 - val_accuracy: 0.6420\n",
      "Epoch 7/1024\n",
      "647/647 [==============================] - 0s 485us/sample - loss: 0.2302 - accuracy: 0.9552 - val_loss: 0.7001 - val_accuracy: 0.6790\n",
      "Epoch 8/1024\n",
      "544/647 [========================>.....] - ETA: 0s - loss: 0.2233 - accuracy: 0.9669\n",
      "Epoch 00008: Reducing Max LR on Plateau: new max lr will be 0.00025 (if not early_stopping).\n",
      "647/647 [==============================] - 0s 482us/sample - loss: 0.2206 - accuracy: 0.9660 - val_loss: 0.6541 - val_accuracy: 0.6543\n",
      "Epoch 9/1024\n",
      "544/647 [========================>.....] - ETA: 0s - loss: 0.2123 - accuracy: 0.9577Restoring model weights from the end of the best epoch.\n",
      "647/647 [==============================] - 0s 488us/sample - loss: 0.2130 - accuracy: 0.9598 - val_loss: 0.6679 - val_accuracy: 0.6667\n",
      "Epoch 00009: early stopping\n",
      "Weights from best epoch have been loaded into model.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.48      0.54        33\n",
      "           1       0.69      0.79      0.74        48\n",
      "\n",
      "    accuracy                           0.67        81\n",
      "   macro avg       0.65      0.64      0.64        81\n",
      "weighted avg       0.66      0.67      0.66        81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {'acc': [], 'precision': [], 'recall': [], 'F1': []}\n",
    "max_acc = 0\n",
    "\n",
    "for i in range(5):\n",
    "    learner.reset_weights()\n",
    "    learner.autofit(1e-3, early_stopping=5)\n",
    "    \n",
    "    # Validation set - use this to pick the \"best\" model to avoid cheating\n",
    "    val_results = learner.validate(class_names=preproc.get_classes())\n",
    "    accv = val_results.diagonal().sum() / val_results.sum()\n",
    "    if accv > max_acc:\n",
    "        predictor.save('models/nbsvm')\n",
    "    \n",
    "    predictor = ktrain.get_predictor(learner.model, preproc=preproc)\n",
    "    y_hat = np.array(predictor.predict(x_test), dtype='int64')\n",
    "    cm = confusion_matrix(y_test, y_hat)\n",
    "    # Test set\n",
    "    TP = cm[1,1]\n",
    "    TN = cm[0,0]\n",
    "    FP = cm[1,0]\n",
    "    FN = cm[0,1]\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    results['acc'].append(acc)\n",
    "    results['precision'].append(TP / (TP + FP))\n",
    "    results['recall'].append(TP / (TP + FN))\n",
    "    results['F1'].append(2 * TP / (2 * TP + FP + FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.6987654320987654\n",
      "precision 0.9045454545454545\n",
      "recall 0.6649280622846472\n",
      "F1 0.76549136433685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(metric + ' ' + str(np.array(results[metric]).mean())) for metric in ['acc', 'precision', 'recall', 'F1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predictor = ktrain.load_predictor('models/nbsvm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=1\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.702</b>, score <b>0.857</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.884\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 98.27%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.027\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(0, 100.00%, 77.90%); opacity: 0.89\" title=\"-0.095\">as</span><span style=\"opacity: 0.80\"> the accused </span><span style=\"background-color: hsl(0, 100.00%, 95.88%); opacity: 0.81\" title=\"-0.009\">is</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.88%); opacity: 0.81\" title=\"-0.009\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 61.81%); opacity: 0.99\" title=\"0.208\">sole</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"0.223\">breadwinner</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.52%); opacity: 0.83\" title=\"0.037\">for</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 78.03%); opacity: 0.88\" title=\"0.095\">his</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.16%); opacity: 0.85\" title=\"0.059\">family</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 95.32%); opacity: 0.81\" title=\"-0.010\">i</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 75.16%); opacity: 0.90\" title=\"0.113\">reduce</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 64.71%); opacity: 0.97\" title=\"0.186\">his</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 71.59%); opacity: 0.92\" title=\"0.137\">sentence</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.00%); opacity: 0.80\" title=\"-0.001\">by</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.88%); opacity: 0.84\" title=\"-0.045\">two</span><span style=\"opacity: 0.80\"> years</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_predictor.explain('As the accused is the sole breadwinner for his family, I reduce his sentence by two years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
