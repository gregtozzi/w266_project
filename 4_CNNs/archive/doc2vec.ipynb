{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregorytozzi/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('x', '')\n",
    "    return text\n",
    "train.cleaned_contents = train.cleaned_contents.apply(cleanText)\n",
    "test.cleaned_contents = test.cleaned_contents.apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['cleaned_contents']), tags=[r.Discrimination_Label]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['cleaned_contents']), tags=[r.Discrimination_Label]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>cleaned_contents</th>\n",
       "      <th>Discrimination_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73277</td>\n",
       "      <td>sentence\\n\\n\\t1.\\tyou are charged as follows:\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79776</td>\n",
       "      <td>sentence\\n\\n\\t1.\\tjosefa kotobalavu, you were ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75870</td>\n",
       "      <td>sentence\\n\\n1. the director of public prosecut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79299</td>\n",
       "      <td>sentence\\n\\n\\t1.\\tmohommed nabi ud- dean, you ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80603</td>\n",
       "      <td>judgment of the court\\n\\nbackground\\n\\n[1] the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   docid                                   cleaned_contents  \\\n",
       "0  73277  sentence\\n\\n\\t1.\\tyou are charged as follows:\\...   \n",
       "1  79776  sentence\\n\\n\\t1.\\tjosefa kotobalavu, you were ...   \n",
       "2  75870  sentence\\n\\n1. the director of public prosecut...   \n",
       "3  79299  sentence\\n\\n\\t1.\\tmohommed nabi ud- dean, you ...   \n",
       "4  80603  judgment of the court\\n\\nbackground\\n\\n[1] the...   \n",
       "\n",
       "   Discrimination_Label  \n",
       "0                     0  \n",
       "1                     1  \n",
       "2                     1  \n",
       "3                     1  \n",
       "4                     0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['sentence', '1.', 'imanueli', 'senikuba', 'you', 'have', 'been', 'found', 'guilty', 'and', 'convicted', 'of', 'the', 'following', 'offence', 'for', 'which', 'you', 'were', 'charged', 'statement', 'of', 'offence', 'rape', 'contrary', 'to', 'section', '207', 'and', 'and', 'of', 'the', 'crimes', 'act', 'no', '44', 'of', '2009.', 'particulars', 'of', 'offence', 'imanueli', 'senikuba', 'sometime', 'between', '14th', 'day', 'of', 'august', '2015', 'and', 'the', '31st', 'day', 'of', 'august', '2015', 'at', 'vitina', 'village', 'in', 'dogotuki', 'in', 'the', 'northern', 'division', 'penetrated', 'the', 'vulva', 'of', 'sl', 'child', 'under', 'the', 'age', 'of', '13', 'years', 'with', 'his', 'tongue', '2.', 'you', 'pleaded', 'not', 'guilty', 'to', 'the', 'charge', 'and', 'the', 'ensuing', 'trial', 'lasted', 'for', 'days', 'the', 'complainant', 'sl', 'amelia', 'the', 'grandmother', 'of', 'sl', 'and', 'police', 'officer', 'who', 'was', 'involved', 'with', 'the', 'arrest', 'and', 'charge', 'of', 'the', 'accused', 'have', 'given', 'evidence', 'for', 'the', 'prosecution', 'while', 'you', 'offered', 'evidence', 'in', 'support', 'of', 'your', 'denial', 'of', 'charge', '3.', 'at', 'the', 'conclusion', 'of', 'the', 'evidence', 'and', 'after', 'the', 'directions', 'given', 'in', 'the', 'summing', 'up', 'the', 'three', 'assessors', 'unanimously', 'found', 'you', 'guilty', 'to', 'the', 'count', 'of', 'rape', 'having', 'reviewed', 'the', 'evidence', 'this', 'court', 'decided', 'to', 'accept', 'the', 'unanimous', 'opinion', 'of', 'the', 'assessors', 'and', 'found', 'you', 'guilty', 'and', 'convicted', 'you', 'of', 'the', 'said', 'charge', '4.', 'it', 'was', 'proved', 'during', 'the', 'trial', 'that', 'between', 'the', '14th', 'day', 'of', 'august', '2015', 'and', 'the', '31st', 'day', 'of', 'august', '2015', 'at', 'vitina', 'village', 'in', 'dogotuki', 'you', 'penetrated', 'the', 'vulva', 'of', 'sl', 'child', 'under', 'the', 'age', 'of', '13', 'years', 'with', 'your', 'tongue', '5.', 'you', 'are', 'an', 'immediate', 'neighbour', 'of', 'the', 'complainant', 'at', 'vitina', 'village', 'the', 'complainant', 'was', 'only', '09', 'years', 'of', 'age', 'at', 'the', 'time', 'you', 'committed', 'the', 'above', 'offence', 'on', 'her', 'her', 'date', 'of', 'birth', 'being', '21', 'september', '2006', 'and', 'as', 'such', 'she', 'was', 'juvenile', '6.', 'the', 'complainant', 'clearly', 'testified', 'as', 'to', 'how', 'you', 'penetrated', 'her', 'vulva', 'with', 'your', 'tongue', 'you', 'were', 'an', 'adult', 'member', 'of', 'the', 'village', 'having', 'grand', 'children', 'in', 'similar', 'age', 'of', 'the', 'sl', 'as', 'you', 'stated', 'in', 'evidence', 'the', 'child', 'sl', 'called', 'you', 'kuku', 'grandfather', 'and', 'used', 'to', 'come', 'to', 'your', 'place', 'to', 'play', 'with', 'your', 'grandchildren', 'the', 'child', 'sl', 'looked', 'to', 'you', 'as', 'trusted', 'adult', 'and', 'instead', 'you', 'lured', 'her', 'into', 'your', 'room', 'and', 'raped', 'her', 'by', 'your', 'shameful', 'act', 'you', 'have', 'robbed', 'the', 'innocence', 'of', '09', 'year', 'old', 'child', '7.', 'even', 'though', 'victim', 'impact', 'assessment', 'report', 'has', 'not', 'been', 'obtained', 'the', 'court', 'having', 'observed', 'the', 'child', 'while', 'giving', 'evidence', 'notes', 'that', 'the', 'child', 'has', 'gone', 'through', 'an', 'immense', 'psychological', 'trauma', 'it', 'was', 'apparent', 'that', 'she', 'felt', 'ashamed', 'the', 'reason', 'for', 'shame', 'was', 'because', 'she', 'had', 'to', 'return', 'to', 'court', 'and', 'narrate', 'the', 'incident', 'all', 'over', 'again', '8.', 'section', 'of', 'the', 'sentencing', 'and', 'penalties', 'act', 'no', '42', 'of', '2009', 'sentencing', 'and', 'penalties', 'act', 'stipulates', 'the', 'relevant', 'factors', 'that', 'court', 'should', 'take', 'into', 'account', 'during', 'the', 'sentencing', 'process', 'have', 'duly', 'considered', 'these', 'factors', 'in', 'determining', 'the', 'sentence', 'to', 'be', 'imposed', 'on', 'you', '9.', 'the', 'offence', 'of', 'rape', 'in', 'terms', 'of', 'section', '207', 'of', 'the', 'crimes', 'act', 'no', '44', 'of', '2009', 'carries', 'maimum', 'penalty', 'of', 'imprisonment', 'for', 'life', '10.', 'the', 'severity', 'of', 'the', 'offence', 'of', 'rape', 'was', 'highlighted', 'by', 'the', 'fiji', 'court', 'of', 'appeal', 'in', 'the', 'case', 'of', 'mohammed', 'kasim', 'v.', 'the', 'state', '1994', 'fjca', '25', 'aau', '21', 'of', '93', '27', 'may', '1994', 'where', 'it', 'was', 'stated', '....', 'it', 'must', 'be', 'recognized', 'by', 'the', 'courts', 'that', 'the', 'crime', 'of', 'rape', 'has', 'become', 'altogether', 'too', 'frequent', 'and', 'that', 'the', 'sentences', 'imposed', 'by', 'the', 'courts', 'for', 'that', 'crime', 'must', 'more', 'nearly', 'reflect', 'the', 'understandable', 'public', 'outrage.', '11.', 'in', 'the', 'case', 'of', 'state', 'v.', 'marawa', '2004', 'fjhc', '338', 'hac', '16t', 'of', '2003s', '23', 'april', '2004', 'his', 'lordship', 'justice', 'anthony', 'gates', 'stated', 'parliament', 'has', 'prescribed', 'the', 'sentence', 'of', 'life', 'imprisonment', 'for', 'rape', 'rape', 'is', 'the', 'most', 'serious', 'seual', 'offence', 'the', 'courts', 'have', 'reflected', 'increasing', 'public', 'intolerance', 'for', 'this', 'crime', 'by', 'hardening', 'their', 'hearts', 'to', 'offenders', 'and', 'meting', 'out', 'harsher', 'sentences', '12.', 'in', 'the', 'state', 'lasaro', 'turagabeci', 'and', 'others', 'supra', 'pain', 'had', 'said', 'the', 'courts', 'have', 'made', 'it', 'clear', 'that', 'rapists', 'will', 'be', 'dealt', 'with', 'severely', 'rape', 'is', 'generally', 'regarded', 'as', 'one', 'of', 'the', 'gravest', 'seual', 'offences', 'it', 'violates', 'and', 'degrades', 'fellow', 'human', 'being', 'the', 'physical', 'and', 'emotional', 'consequences', 'to', 'the', 'victim', 'are', 'likely', 'to', 'be', 'severe', 'the', 'courts', 'must', 'protect', 'women', 'from', 'such', 'degradation', 'and', 'trauma', 'the', 'increasing', 'prevalence', 'of', 'such', 'offending', 'in', 'the', 'community', 'calls', 'for', 'deterrent', 'sentences.', '13.', 'his', 'lordship', 'justice', 'daniel', 'goundar', 'in', 'the', 'case', 'of', 'state', 'v.', 'av', '2009', 'fjhc', '24', 'hac', '192', 'of', '2008', 'february', '2009', 'observed', '....', 'rape', 'is', 'the', 'most', 'serious', 'form', 'of', 'seual', 'assault', 'in', 'this', 'case', 'child', 'was', 'raped', 'society', 'can', 'not', 'condone', 'any', 'form', 'of', 'seual', 'assaults', 'on', 'children', 'children', 'are', 'our', 'future', 'the', 'courts', 'have', 'positive', 'obligation', 'under', 'the', 'constitution', 'to', 'protect', 'the', 'vulnerable', 'from', 'any', 'form', 'of', 'violence', 'or', 'seual', 'abuse', 'seual', 'offenders', 'must', 'be', 'deterred', 'from', 'committing', 'this', 'kind', 'of', 'offences', '14.', 'in', 'the', 'case', 'of', 'anand', 'abhay', 'raj', 'v.', 'state', '2014', 'fjsc', '12', 'cav', '03', 'of', '2014', '20', 'august', '2014', 'chief', 'justice', 'anthony', 'gates', 'with', 'justice', 'sathyaa', 'hettige', 'and', 'madam', 'justice', 'chandra', 'ekanayake', 'agreeing', 'endorsed', 'the', 'view', 'that', 'rape', 'of', 'juveniles', 'under', 'the', 'age', 'of', '18', 'years', 'must', 'attract', 'sentence', 'of', 'at', 'least', '10', 'years', 'and', 'the', 'acceptable', 'range', 'of', 'sentences', 'or', 'sentencing', 'tariff', 'is', 'between', '10', 'and', '16', 'years', 'imprisonment', '15.', 'in', 'determining', 'the', 'starting', 'point', 'within', 'the', 'said', 'tariff', 'the', 'court', 'of', 'appeal', 'in', 'laisiasa', 'koroivuki', 'v.', 'state', '2013', 'fjca', '15', 'aau', '0018', 'of', '2010', 'march', '2013', 'has', 'formulated', 'the', 'following', 'guiding', 'principles', 'in', 'selecting', 'starting', 'point', 'the', 'court', 'must', 'have', 'regard', 'to', 'an', 'objective', 'seriousness', 'of', 'the', 'offence', 'no', 'reference', 'should', 'be', 'made', 'to', 'the', 'mitigating', 'and', 'aggravating', 'factors', 'at', 'this', 'time', 'as', 'matter', 'of', 'good', 'practice', 'the', 'starting', 'point', 'should', 'be', 'picked', 'from', 'the', 'lower', 'or', 'middle', 'range', 'of', 'the', 'tariff', 'after', 'adjusting', 'for', 'the', 'mitigating', 'and', 'aggravating', 'factors', 'the', 'final', 'term', 'should', 'fall', 'within', 'the', 'tariff', 'if', 'the', 'final', 'term', 'falls', 'either', 'below', 'or', 'higher', 'than', 'the', 'tariff', 'then', 'the', 'sentencing', 'court', 'should', 'provide', 'reasons', 'why', 'the', 'sentence', 'is', 'outside', 'the', 'range.', '16.', 'in', 'the', 'light', 'of', 'the', 'above', 'guiding', 'principles', 'and', 'taking', 'into', 'consideration', 'the', 'objective', 'seriousness', 'of', 'the', 'offence', 'commence', 'your', 'sentence', 'at', '10', 'years', 'imprisonment', 'for', 'the', 'count', 'of', 'rape', '17.', 'the', 'aggravating', 'factors', 'are', 'as', 'follows', 'you', 'were', 'an', 'immediate', 'neighbour', 'of', 'the', 'complainant', 'being', 'so', 'you', 'should', 'have', 'protected', 'her', 'instead', 'you', 'have', 'breached', 'the', 'trust', 'epected', 'from', 'you', 'and', 'the', 'breach', 'was', 'gross', 'ii', 'there', 'was', 'large', 'disparity', 'in', 'age', 'between', 'you', 'and', 'the', 'complainant', 'the', 'complainant', 'was', 'merely', '09', 'years', 'of', 'age', 'at', 'the', 'time', 'you', 'committed', 'the', 'offence', 'on', 'her', 'at', 'the', 'time', 'you', 'were', '61', 'years', 'old', 'therefore', 'there', 'was', 'difference', 'in', 'age', 'of', '52', 'years', 'iii', 'you', 'took', 'advantage', 'of', 'the', 'complainant', 'vulnerability', 'helplessness', 'and', 'naivety', 'iv', 'you', 'have', 'eposed', 'the', 'innocent', 'mind', 'of', 'child', 'to', 'seual', 'activity', 'at', 'such', 'tender', 'age', '18.', 'considering', 'the', 'aforementioned', 'aggravating', 'factors', 'increase', 'your', 'sentence', 'by', 'further', 'years', 'now', 'your', 'sentence', 'is', '13', 'years', 'imprisonment', 'for', 'the', 'count', 'of', 'rape', '19.', 'imanueli', 'senikuba', 'you', 'are', 'now', '64', 'years', 'old', 'you', 'are', 'said', 'to', 'be', 'farmer', 'and', 'fisherman', 'however', 'these', 'are', 'all', 'personal', 'circumstances', 'and', 'can', 'not', 'be', 'considered', 'as', 'mitigating', 'circumstances', '20.', 'state', 'has', 'indicated', 'that', 'you', 'have', 'no', 'previous', 'convictions', 'or', 'pending', 'cases', 'therefore', 'you', 'are', 'first', 'offender', '21.', 'in', 'considering', 'that', 'you', 'are', 'first', 'time', 'offender', 'and', 'the', 'rest', 'of', 'the', 'mitigating', 'factors', 'submitted', 'on', 'your', 'behalf', 'deduct', 'two', 'years', 'from', 'the', 'above', '22.', 'accordingly', 'sentence', 'you', 'to', 'term', 'of', 'imprisonment', 'of', '11', 'years', 'pursuant', 'to', 'the', 'provisions', 'of', 'section', '18', 'of', 'the', 'sentencing', 'and', 'penalties', 'act', 'order', 'that', 'you', 'are', 'not', 'eligible', 'to', 'be', 'released', 'on', 'parole', 'until', 'you', 'serve', '09', 'years', 'of', 'that', 'sentence', '23.', 'section', '24', 'of', 'the', 'sentencing', 'and', 'penalties', 'act', 'reads', 'thus', 'if', 'an', 'offender', 'is', 'sentenced', 'to', 'term', 'of', 'imprisonment', 'any', 'period', 'of', 'time', 'during', 'which', 'the', 'offender', 'was', 'held', 'in', 'custody', 'prior', 'to', 'the', 'trial', 'of', 'the', 'matter', 'or', 'matters', 'shall', 'unless', 'court', 'otherwise', 'orders', 'be', 'regarded', 'by', 'the', 'court', 'as', 'period', 'of', 'imprisonment', 'already', 'served', 'by', 'the', 'offender.', '24.', 'you', 'have', 'been', 'in', 'remand', 'custody', 'for', 'this', 'case', 'from', '26', 'june', '2017', 'to', '13', 'july', '2017', 'when', 'you', 'were', 'granted', 'bail', 'by', 'the', 'magistrates', 'court', 'thereafter', 'you', 'have', 'been', 'in', 'remand', 'custody', 'since', '03rd', 'september', '2018', 'the', 'day', 'on', 'which', 'commenced', 'the', 'trial', 'in', 'this', 'case', 'accordingly', 'you', 'have', 'been', 'in', 'custody', 'for', 'period', 'of', 'about', '22', 'days', 'the', 'period', 'you', 'were', 'in', 'custody', 'shall', 'be', 'regarded', 'as', 'period', 'of', 'imprisonment', 'already', 'served', 'by', 'you', 'hold', 'that', 'period', 'of', 'month', 'should', 'be', 'considered', 'as', 'served', 'in', 'terms', 'of', 'the', 'provisions', 'of', 'section', '24', 'of', 'the', 'sentencing', 'and', 'penalties', 'act', '25.', 'in', 'the', 'result', 'you', 'are', 'sentenced', 'to', 'term', 'of', 'imprisonment', 'of', '11', 'years', 'with', 'non-', 'parole', 'period', 'of', '09', 'years', 'considering', 'the', 'time', 'you', 'have', 'spent', 'in', 'remand', 'the', 'time', 'remaining', 'to', 'be', 'served', 'is', 'as', 'follows', 'head', 'sentence', '10', 'years', 'and', '11', 'months', 'non-parole', 'period', '08', 'years', 'and', '11months', '26.', 'you', 'have', '30', 'days', 'to', 'appeal', 'to', 'the', 'court', 'of', 'appeal', 'if', 'you', 'so', 'wish', 'chamath', 's.', 'morais', 'judge', 'at', 'labasa', '06th', 'september', '2018', 'solicitors', 'for', 'the', 'state', 'office', 'of', 'the', 'director', 'of', 'public', 'prosecutions', 'labasa', 'solicitors', 'for', 'the', 'accused', 'office', 'of', 'the', 'legal', 'aid', 'commission', 'labasa', 'paclii', 'copyright', 'policy', 'disclaimers', 'privacy', 'policy', 'feedback', 'url'], tags=[0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged.values[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 647/647 [00:00<00:00, 697971.88it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 647/647 [00:00<00:00, 1176295.92it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1544516.04it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 952849.26it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1853630.25it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1288563.48it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1295947.80it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1133075.03it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 828362.24it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1241974.69it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1352798.95it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 965734.76it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1193891.20it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1666900.91it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1529715.16it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1473243.59it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1115836.63it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 848035.84it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1265725.13it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1055509.41it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1296566.98it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1569528.45it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1317977.02it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1544516.04it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1331557.75it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1342094.31it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1278245.26it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1198108.03it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1085051.85it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1400265.58it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1309707.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 17s, sys: 874 ms, total: 1min 18s\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.5555555555555556\n",
      "Testing F1 score: 0.5262626262626262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregorytozzi/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 647/647 [00:00<00:00, 1027144.09it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 647/647 [00:00<00:00, 744503.34it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1302166.36it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1296566.98it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1650678.03it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1662815.37it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1026755.46it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1137348.99it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 877089.43it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1273446.59it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1006197.51it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1119519.26it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1222944.88it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1076870.91it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1244823.25it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1195995.90it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1244823.25it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1151342.68it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1244823.25it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 948519.64it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1586040.14it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1296566.98it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1417823.77it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1593490.72it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1211479.77it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 987883.03it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1515195.25it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1457419.27it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1135445.48it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1544516.04it/s]\n",
      "100%|██████████| 647/647 [00:00<00:00, 1376821.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 40s, sys: 1.28 s, total: 1min 42s\n",
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dmm.alpha -= 0.002\n",
    "    model_dmm.min_alpha = model_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.691358024691358\n",
      "Testing F1 score: 0.6943587105624143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregorytozzi/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "y_train, X_train = vec_for_learning(model_dmm, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dmm, test_tagged)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
