{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICAAD TEST USING TRUNCATED INPUT OF 512 TOKENS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9xOw82pUtMr"
   },
   "source": [
    "#DistilBERT - First 512 tokens, Tensforflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3Cr3BkvY4cZ"
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "from transformers import BertTokenizer, BertConfig, TFBertModel, TFBertForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, DistilBertConfig, TFDistilBertModel, TFDistilBertForSequenceClassification\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SULTsBSBY4kC"
   },
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\"\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "bert_file = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(bert_file, do_lower_case=True)\n",
    "\n",
    "# can be up to 512 for BERT\n",
    "max_length = 512\n",
    "batch_size = 8\n",
    "epochs = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rpGdnaz9puCp",
    "outputId": "93c965cc-2c6a-4b0c-b89a-373e045e3b70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1Xqa_4ZY4Ro"
   },
   "outputs": [],
   "source": [
    "# TOKENIZE \n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "  return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label\n",
    "\n",
    "def tokenize_plus(df):\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    sentences = df['text'].values\n",
    "    labels = df['label'].values\n",
    "\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    token_type_ids_list = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in sentences:\n",
    "        inputs = tokenizer.encode_plus(sent, \n",
    "                                       add_special_tokens=True, \n",
    "                                       max_length=max_length, \n",
    "                                       truncation = True,\n",
    "                                       pad_to_max_length=True, \n",
    "                                       return_attention_mask=True,\n",
    "                                       return_token_type_ids=True)\n",
    "\n",
    "        input_ids_list.append(inputs['input_ids'])\n",
    "        attention_mask_list.append(inputs['attention_mask'])\n",
    "        token_type_ids_list.append(inputs['token_type_ids'])   \n",
    "    \n",
    "    label_list = df['label'].tolist()\n",
    "\n",
    "    #return np.asarray(input_ids_list, dtype='int32'), np.asarray(attention_mask_list, dtype='int32'), np.asarray(label_list, dtype='int32')\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3JbLwrtp3UE"
   },
   "outputs": [],
   "source": [
    "# Function to get data - GET THE DATA\n",
    "def get_data(fname):\n",
    "    df = pd.read_csv(fname)\n",
    "    df = df[['docid', 'cleaned_contents', 'Discrimination_Label']]\n",
    "    df = df.rename(columns = {'cleaned_contents':'text', 'Discrimination_Label':'label'})\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df\n",
    "\n",
    "train = get_data(train_file)\n",
    "df_test = get_data(test_file)\n",
    "msk = np.random.rand(len(train)) < 0.9\n",
    "df_train = train[msk]\n",
    "df_val = train[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1r0EMAlyp3Zv"
   },
   "outputs": [],
   "source": [
    "# TOKENIZE AND PUT INTO TENSORFLOW DATASET\n",
    "ds_train_encoded = tokenize_plus(df_train).shuffle(100).batch(batch_size)\n",
    "ds_val_encoded = tokenize_plus(df_val).batch(batch_size)\n",
    "ds_test_encoded = tokenize_plus(df_test).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "TfMyVI9KY4QJ",
    "outputId": "721d4ffa-b7e4-4e21-d185-3e788fbfccd3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_19', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# THE MODEL\n",
    "\n",
    "learning_rate = 2e-5\n",
    "number_of_epochs = epochs\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(bert_file)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "Ry2GzMrSUvyM",
    "outputId": "c86debe1-ad7b-43d8-cb83-9fe956fc5fab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,955,010\n",
      "Trainable params: 66,955,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "egfEcybzVPfx",
    "outputId": "376b41e1-34bf-4092-c1b0-55c6d56ef1d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "72/72 [==============================] - 22s 309ms/step - loss: 0.6867 - accuracy: 0.5610 - val_loss: 0.6717 - val_accuracy: 0.5753\n",
      "Epoch 2/3\n",
      "72/72 [==============================] - 21s 290ms/step - loss: 0.6529 - accuracy: 0.6446 - val_loss: 0.6689 - val_accuracy: 0.6301\n",
      "Epoch 3/3\n",
      "72/72 [==============================] - 21s 290ms/step - loss: 0.6077 - accuracy: 0.6725 - val_loss: 0.6262 - val_accuracy: 0.6164\n"
     ]
    }
   ],
   "source": [
    "# Train (fine tune) the model\n",
    "bert_history = model.fit(ds_train_encoded, epochs=epochs, validation_data=ds_val_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "6uVOO-RlX36H",
    "outputId": "94d358d5-7279-4a33-ab00-dd5ddbdf0cd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 2s 89ms/step - loss: 0.6384 - accuracy: 0.6728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6383978724479675, 0.6728395223617554]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EVALUATE THE MODEL\n",
    "model.evaluate(ds_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NS30BkOKq-3p"
   },
   "outputs": [],
   "source": [
    "# Get Predictions\n",
    "log_pred = model.predict(ds_test_encoded)\n",
    "y_pred = np.argmax(log_pred[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "AwxREqewqo6G",
    "outputId": "2206a70e-f069-49d4-dae7-97683cddf8d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBert\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.40      0.50        65\n",
      "           1       0.68      0.86      0.76        97\n",
      "\n",
      "    accuracy                           0.67       162\n",
      "   macro avg       0.67      0.63      0.63       162\n",
      "weighted avg       0.67      0.67      0.65       162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show classification report\n",
    "print(\"DistilBert\")\n",
    "print(classification_report(df_test['label'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "veYvAU8grdi-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_bthCAamrkuG"
   },
   "source": [
    "## **ICAAD TEST USING STACKED RESULTS WITH MEAN OUTPUT**\n",
    "\n",
    "DistilBert. \n",
    "- Split Documents into 100 token (arbitrary chunks). \n",
    "- Apply label to new chunks. \n",
    "- Run Model. \n",
    "- Take mean output for each document. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98Ku96MJIEpe"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOueMsEAsZVo"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertConfig, TFBertModel, TFBertForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, DistilBertConfig, TFDistilBertModel, TFDistilBertForSequenceClassification\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fP1j_Zlsdie"
   },
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\"\n",
    "\n",
    "#split_length = 100\n",
    "split_length = 510 # The max as we add two tokens\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "bert_file = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(bert_file, do_lower_case=True)\n",
    "\n",
    "# can be up to 512 for BERT\n",
    "max_length = 512\n",
    "batch_size = 8\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jSI75lAtr4VH"
   },
   "outputs": [],
   "source": [
    "# Function to get data - GET THE DATA\n",
    "def get_data(fname):\n",
    "    df = pd.read_csv(fname)\n",
    "    df = df[['docid', 'cleaned_contents', 'Discrimination_Label']]\n",
    "    df = df.rename(columns = {'cleaned_contents':'text', 'Discrimination_Label':'label'})\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LwyyrJ6b17Ba"
   },
   "outputs": [],
   "source": [
    "#Funtion to split tokens into arbitrary length token chunks\n",
    "def split_tokens(df):\n",
    "  split_tokens = []\n",
    "\n",
    "  for row in df['tokens']:\n",
    "      split_tokens.append([row[i:i + split_length] for i in range(0, len(row), split_length)] )\n",
    "\n",
    "  return split_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_DooYDt4Onci"
   },
   "outputs": [],
   "source": [
    "#Function to create dictionary from lists\n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "  return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D1sJZGD6J6wX"
   },
   "outputs": [],
   "source": [
    "# Function to explode out tokens seried into pre-defined chunk lengths and return as inputs to model \n",
    "def prepare_df(df):\n",
    "\n",
    "  # add special tokens to beginning and end (assuming Bert tokenizer)\n",
    "  for row in df['split_tokens']:\n",
    "        row.insert(0,101)\n",
    "        row.append(102)\n",
    "\n",
    "  # create our input lists\n",
    "  tokenized = df['split_tokens']\n",
    "  input_ids = np.array([i + [0]*(split_length+2-len(i)) for i in tokenized.values])\n",
    "  attention_mask = np.where(input_ids != 0, 1, 0)\n",
    "  token_type_ids = np.where(input_ids != 0, 0, 0)\n",
    "  labels = df['label'].tolist()\n",
    "\n",
    "  # convert to tensorflow dataset object and return\n",
    "  return tf.data.Dataset.from_tensor_slices((input_ids, attention_mask, token_type_ids, labels)).map(map_example_to_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MDDf1o68aqJs"
   },
   "outputs": [],
   "source": [
    "train = get_data(train_file)\n",
    "df_test = get_data(test_file)\n",
    "msk = np.random.rand(len(train)) < 0.9\n",
    "df_train = train[msk]\n",
    "df_val = train[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aBNi4plKv47v"
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "df_train['tokens'] = df_train['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=False,)))\n",
    "df_val['tokens'] = df_val['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=False,)))\n",
    "df_test['tokens'] = df_test['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=False,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "J7S5yPcFetlm",
    "outputId": "3618c0b2-d4cb-49dd-c77a-bdc3733b6cbd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Explode the dataframes so each document is now ~100 tokens liong\n",
    "df_train['split_tokens'] = split_tokens(df_train)\n",
    "df_train = df_train.explode('split_tokens').reset_index(drop=True)\n",
    "\n",
    "df_val['split_tokens'] = split_tokens(df_val)\n",
    "df_val = df_val.explode('split_tokens').reset_index(drop=True)\n",
    "\n",
    "df_test['split_tokens'] = split_tokens(df_test)\n",
    "df_test = df_test.explode('split_tokens').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ZIk4VHMwgGHE",
    "outputId": "3babb589-bf2a-49b0-acd7-296d9f552b8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2405\n",
      "335\n",
      "647\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_val))\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aeDPLZH9fFPu"
   },
   "outputs": [],
   "source": [
    "ds_encode_val =  prepare_df(df_val).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZRnxdVYfPZt"
   },
   "outputs": [],
   "source": [
    "ds_encode_train =  prepare_df(df_train).shuffle(100).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dYb48lDcfPsC"
   },
   "outputs": [],
   "source": [
    "ds_encode_test =  prepare_df(df_test).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "I9CjzC_ZhnKx",
    "outputId": "d21173fc-53f6-4fec-f2d5-8fd683cc1b90"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_projector', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_19', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# THE MODEL\n",
    "\n",
    "learning_rate = 2e-5\n",
    "number_of_epochs = epochs\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(bert_file)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "8V8g6AGZhsKS",
    "outputId": "f7077e2a-1c55-4d8c-8155-3d0d18b66193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,955,010\n",
      "Trainable params: 66,955,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "0-uZWreahw0V",
    "outputId": "6884eaba-528a-48ec-d7c8-74e79ae350a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "301/301 [==============================] - 201s 666ms/step - loss: 0.6600 - accuracy: 0.6054 - val_loss: 0.6402 - val_accuracy: 0.6299\n",
      "Epoch 2/3\n",
      "301/301 [==============================] - 200s 665ms/step - loss: 0.5963 - accuracy: 0.6748 - val_loss: 0.6785 - val_accuracy: 0.6179\n",
      "Epoch 3/3\n",
      "301/301 [==============================] - 200s 664ms/step - loss: 0.5514 - accuracy: 0.7160 - val_loss: 0.6143 - val_accuracy: 0.6896\n"
     ]
    }
   ],
   "source": [
    "# Train (fine tune) the model\n",
    "bert_history = model.fit(ds_encode_train, epochs=epochs, validation_data=ds_encode_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rh0GkTKOiBDs",
    "outputId": "706e9839-2cf6-4752-b20e-e1feaa1c32f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 16s 198ms/step - loss: 0.6635 - accuracy: 0.6105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6635349988937378, 0.6105100512504578]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EVALUATE THE MODEL\n",
    "model.evaluate(ds_encode_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fco7K2J5iF95"
   },
   "outputs": [],
   "source": [
    "# Get Predictions\n",
    "log_pred = model.predict(ds_encode_test)\n",
    "y_pred = np.argmax(log_pred[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "r5LMN5mQiGEz",
    "outputId": "40de7acf-9a21-4e61-a005-85d18874fbb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBert, chunk size =  510\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.83      0.68       326\n",
      "           1       0.69      0.38      0.49       321\n",
      "\n",
      "    accuracy                           0.61       647\n",
      "   macro avg       0.64      0.61      0.59       647\n",
      "weighted avg       0.64      0.61      0.59       647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show classification report\n",
    "print(\"DistilBert, chunk size = \", split_length)\n",
    "print(classification_report(df_test['label'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BAQ8pVCSlfVx",
    "outputId": "a37ff08b-300a-4113-fa81-d56a77304813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470\n",
      "177\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(y_pred == 0))\n",
    "print(np.count_nonzero(y_pred == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pO1IqfQslU2-"
   },
   "outputs": [],
   "source": [
    "df_test['y_pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWTUMmN3ouRX"
   },
   "outputs": [],
   "source": [
    "df_test['y_pos'] = np.where(df_test['y_pred'] == 1, 1, 0)\n",
    "df_test['y_neg'] = np.where(df_test['y_pred'] == 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "onapQhMSnMkM",
    "outputId": "9fa10607-47fc-41f0-92f3-9fb808ba55db"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>split_tokens</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_pos</th>\n",
       "      <th>y_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80646</td>\n",
       "      <td>SENTENCE\\n\\n[Name of the victim is suppressed....</td>\n",
       "      <td>1</td>\n",
       "      <td>[6251, 1031, 2171, 1997, 1996, 6778, 2003, 137...</td>\n",
       "      <td>[101, 6251, 1031, 2171, 1997, 1996, 6778, 2003...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80646</td>\n",
       "      <td>SENTENCE\\n\\n[Name of the victim is suppressed....</td>\n",
       "      <td>1</td>\n",
       "      <td>[6251, 1031, 2171, 1997, 1996, 6778, 2003, 137...</td>\n",
       "      <td>[101, 2308, 2013, 2107, 16627, 1998, 12603, 10...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80646</td>\n",
       "      <td>SENTENCE\\n\\n[Name of the victim is suppressed....</td>\n",
       "      <td>1</td>\n",
       "      <td>[6251, 1031, 2171, 1997, 1996, 6778, 2003, 137...</td>\n",
       "      <td>[101, 3755, 2030, 19601, 1025, 1019, 1012, 200...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80646</td>\n",
       "      <td>SENTENCE\\n\\n[Name of the victim is suppressed....</td>\n",
       "      <td>1</td>\n",
       "      <td>[6251, 1031, 2171, 1997, 1996, 6778, 2003, 137...</td>\n",
       "      <td>[101, 1013, 2286, 14397, 3669, 2072, 1024, 938...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81372</td>\n",
       "      <td>JUDGMENT\\n\\nThis is an appeal against convicti...</td>\n",
       "      <td>1</td>\n",
       "      <td>[8689, 2023, 2003, 2019, 5574, 2114, 10652, 19...</td>\n",
       "      <td>[101, 8689, 2023, 2003, 2019, 5574, 2114, 1065...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   docid                                               text  ...  y_pos y_neg\n",
       "0  80646  SENTENCE\\n\\n[Name of the victim is suppressed....  ...      1     0\n",
       "1  80646  SENTENCE\\n\\n[Name of the victim is suppressed....  ...      0     1\n",
       "2  80646  SENTENCE\\n\\n[Name of the victim is suppressed....  ...      1     0\n",
       "3  80646  SENTENCE\\n\\n[Name of the victim is suppressed....  ...      1     0\n",
       "4  81372  JUDGMENT\\n\\nThis is an appeal against convicti...  ...      0     1\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPe2BGVlnPoJ"
   },
   "outputs": [],
   "source": [
    "y_pos_sum = df_test.groupby(['docid'], as_index=False)['y_pos'].sum().rename(columns = {'y_pos':'y_pos_sum'})\n",
    "y_pos_mean = df_test.groupby(['docid'], as_index=False)['y_pos'].mean().rename(columns = {'y_pos':'y_pos_mean'})\n",
    "y_neg_sum = df_test.groupby(['docid'], as_index=False)['y_neg'].sum().rename(columns = {'y_neg':'y_neg_sum'})\n",
    "y_neg_mean = df_test.groupby(['docid'], as_index=False)['y_neg'].mean().rename(columns = {'y_neg':'y_neg_mean'})\n",
    "\n",
    "chunks = df_test.groupby(['docid'], as_index=False)['y_pos'].count().rename(columns = {'y_pos':'chunk_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5uPEyR9b6oNF"
   },
   "outputs": [],
   "source": [
    "lab = df_test.groupby(['docid'], as_index=False)['label'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AheAJZaQnzlf"
   },
   "outputs": [],
   "source": [
    "results = y_pos_sum.join(y_pos_mean['y_pos_mean']).join(y_neg_sum['y_neg_sum']).join(y_neg_mean['y_neg_mean']).join(chunks['chunk_count']).join(lab['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "SGe8KwhdrvSX",
    "outputId": "37b1d114-ef2b-4186-d9e5-1ade510ab386"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>y_pos_sum</th>\n",
       "      <th>y_pos_mean</th>\n",
       "      <th>y_neg_sum</th>\n",
       "      <th>y_neg_mean</th>\n",
       "      <th>chunk_count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70139</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70164</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70302</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70320</td>\n",
       "      <td>3</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70405</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   docid  y_pos_sum  y_pos_mean  y_neg_sum  y_neg_mean  chunk_count  label\n",
       "0  70139          2    0.500000          2    0.500000            4      1\n",
       "1  70164          0    0.000000          5    1.000000            5      1\n",
       "2  70302          1    0.333333          2    0.666667            3      0\n",
       "3  70320          3    0.750000          1    0.250000            4      0\n",
       "4  70405          2    0.333333          4    0.666667            6      1"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Naf0fTgt7Qlr"
   },
   "outputs": [],
   "source": [
    "results.to_csv('bert_chunk_results.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_TensorFlow_various.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
