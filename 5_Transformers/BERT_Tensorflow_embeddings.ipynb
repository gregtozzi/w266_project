{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get DistilBert Embeddings to use as features in downstream models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EG0yd3HsYgjR"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2SPqLrOFY0fr",
    "outputId": "74e17d50-233d-48cc-d508-b8f1309f8ace"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "from transformers import DistilBertTokenizer, DistilBertConfig, TFDistilBertModel, TFDistilBertForSequenceClassification\n",
    "\n",
    "import gc\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import pickle\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Dense, Masking\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Dense, Input, concatenate, Layer, Lambda, Dropout, Activation\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, TensorBoard\n",
    "from keras import layers\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M6Tl5F6lY7Jo"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "VlzOtpKtY4uk",
    "outputId": "6bc00ace-61d4-4337-96e5-87a22cd8545e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'activation_13', 'vocab_layer_norm', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_19', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_file = \"train_80_10_10.csv\"\n",
    "test_file = \"test_80_10_10.csv\"\n",
    "val_file = \"val_80_10_10.csv\"\n",
    "\n",
    "skip_lines = 6\n",
    "max_length = 200\n",
    "split_length = max_length - 2\n",
    "\n",
    "\n",
    "# DistilBert\n",
    "bert_file = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(bert_file, do_lower_case=True)\n",
    "bert_model = TFDistilBertForSequenceClassification.from_pretrained(bert_file)\n",
    "\n",
    "# Model Training\n",
    "batch_size = 8\n",
    "epochs = 3\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c1LmabBdY-PE",
    "outputId": "54c75128-7ebe-4045-d053-996aa9a213a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-cj5VD5zZBGk"
   },
   "outputs": [],
   "source": [
    "# Function to get data\n",
    "def get_data(fname):\n",
    "    df = pd.read_csv(fname)\n",
    "    df = df[['docid', 'cleaned_contents', 'Discrimination_Label']]\n",
    "    df = df.rename(columns = {'cleaned_contents':'text', 'Discrimination_Label':'label'})\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oo1OnHmMZBM6"
   },
   "outputs": [],
   "source": [
    "# Function to tokenize data and return tensors for input ids, attention mask and labels\n",
    "def tokenize_plus(df):\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    sentences = df['text'].values\n",
    "    labels = df['label'].values\n",
    "\n",
    "    input_ids = []\n",
    "    input_masks = []\n",
    "    input_segments = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in sentences:\n",
    "        inputs = tokenizer.encode_plus(sent, \n",
    "                                       add_special_tokens=True, \n",
    "                                       max_length=max_length, \n",
    "                                       truncation = True,\n",
    "                                       pad_to_max_length=True, \n",
    "                                       return_attention_mask=True,\n",
    "                                       return_token_type_ids=True)\n",
    "\n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        input_masks.append(inputs['attention_mask'])\n",
    "        input_segments.append(inputs['token_type_ids'])   \n",
    "    \n",
    "    labels = df['label']\n",
    "\n",
    "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(labels, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "86ZIJjrcZBTG"
   },
   "outputs": [],
   "source": [
    "# function to split text into smaller chunks of 200 words, overlapping by 50 words\n",
    "def get_split(text1):\n",
    "  l_total = []\n",
    "  l_parcial = []\n",
    "  if len(text1.split())//150 >0:\n",
    "    n = len(text1.split())//150\n",
    "  else: \n",
    "    n = 1\n",
    "  for w in range(n):\n",
    "    if w == 0:\n",
    "      l_parcial = text1.split()[:200]\n",
    "      l_total.append(\" \".join(l_parcial))\n",
    "    else:\n",
    "      l_parcial = text1.split()[w*150:w*150 + 200]\n",
    "      l_total.append(\" \".join(l_parcial))\n",
    "  return l_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ziAr6sTSZBSR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NbFA03LSZM5k"
   },
   "source": [
    "# Data Prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ffRloI5nZBKH"
   },
   "outputs": [],
   "source": [
    "# GET THE DATA\n",
    "df_train = get_data(train_file)\n",
    "df_test = get_data(test_file)\n",
    "df_val = get_data(val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wbP3RzNnZTSz"
   },
   "outputs": [],
   "source": [
    "#remove double new lines\n",
    "df_train['text'] = df_train['text'].replace('\\n\\s*\\n', '\\n',regex=True)\n",
    "df_test['text'] = df_test['text'].replace('\\n\\s*\\n', '\\n',regex=True)\n",
    "df_val['text'] = df_val['text'].replace('\\n\\s*\\n', '\\n',regex=True)\n",
    "\n",
    "# strip last n lines\n",
    "df_train['text'] = df_train.apply(lambda L: L.text.rsplit(\"\\n\",skip_lines)[0], axis=1)\n",
    "df_test['text'] = df_test.apply(lambda L: L.text.rsplit(\"\\n\",skip_lines)[0], axis=1)\n",
    "df_val['text'] = df_val.apply(lambda L: L.text.rsplit(\"\\n\",skip_lines)[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gSwe_v2BZTWq"
   },
   "outputs": [],
   "source": [
    "# split the texts into  chunks & # explode the dataframe\n",
    "df_train['text_split'] = df_train['text'].apply(get_split)\n",
    "df_train = df_train.explode('text_split')\n",
    "\n",
    "# split the texts into   chunks & # explode the dataframe\n",
    "df_val['text_split'] = df_val['text'].apply(get_split)\n",
    "df_val = df_val.explode('text_split')\n",
    "\n",
    "# split the texts into   chunks & # explode the dataframe\n",
    "df_test['text_split'] = df_test['text'].apply(get_split)\n",
    "df_test = df_test.explode('text_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "PCCdZDSqZiib",
    "outputId": "022bf550-2e59-4a90-9474-f380a2aa8d08"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255849</td>\n",
       "      <td>SENTENCE\\n• In a judgment delivered on 9 May 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>SENTENCE • In a judgment delivered on 9 May 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255849</td>\n",
       "      <td>SENTENCE\\n• In a judgment delivered on 9 May 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>that she blacked out. • She came to in a frien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255849</td>\n",
       "      <td>SENTENCE\\n• In a judgment delivered on 9 May 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>quality life. Those who find themselves on the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255849</td>\n",
       "      <td>SENTENCE\\n• In a judgment delivered on 9 May 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>with you as she was asleep. You then raped her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>288617</td>\n",
       "      <td>SENTENCE\\n• ELIZABETH GOLMAN, you were charged...</td>\n",
       "      <td>1</td>\n",
       "      <td>SENTENCE • ELIZABETH GOLMAN, you were charged ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    docid  ...                                         text_split\n",
       "0  255849  ...  SENTENCE • In a judgment delivered on 9 May 20...\n",
       "0  255849  ...  that she blacked out. • She came to in a frien...\n",
       "0  255849  ...  quality life. Those who find themselves on the...\n",
       "0  255849  ...  with you as she was asleep. You then raped her...\n",
       "1  288617  ...  SENTENCE • ELIZABETH GOLMAN, you were charged ...\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4IkGpWSXZd9i"
   },
   "outputs": [],
   "source": [
    "# get the tokens\n",
    "# Get tokenized labels\n",
    "# input_ids, attention_masks, labels = tokenize_plus(df_train)\n",
    "val_input_ids, val_attention_masks, val_labels = tokenize_plus(df_val)\n",
    "test_input_ids, test_attention_masks, test_labels = tokenize_plus(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NAYq1FWH_Ysy"
   },
   "outputs": [],
   "source": [
    "#Reload the ones we saved earlier for train\n",
    "with open('input_ids.pkl', 'rb') as f: input_ids = pickle.load(f)\n",
    "with open('attention_masks.pkl', 'rb') as f: attention_masks = pickle.load(f)\n",
    "with open('labels.pkl', 'rb') as f: labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cNG0GzwyZeAA"
   },
   "outputs": [],
   "source": [
    "#That last step took longer than it should, lets save the train output just in case...\n",
    "#with open('test_last_hidden_states.pkl', 'wb') as f:  pickle.dump(input_ids, f)\n",
    "#with open('attention_masks.pkl', 'wb') as f: pickle.dump(attention_masks, f)\n",
    "#with open('labels.pkl', 'wb') as f: pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "thoqvzT-ZuHY"
   },
   "source": [
    "# Model to get token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Gxd35DEtZxrx",
    "outputId": "510d10eb-0923-4e9e-dd27-c36739bad827"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'activation_13', 'vocab_layer_norm', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
    "config.output_hidden_states = False\n",
    "transformer_model = TFDistilBertModel.from_pretrained(bert_file, config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRIx-Cn4ZeC-"
   },
   "outputs": [],
   "source": [
    "#embedding mode\n",
    "input_ids_in = tf.keras.layers.Input(shape=(200,), name='input_token', dtype='int32')\n",
    "input_masks_in = tf.keras.layers.Input(shape=(200,), name='masked_token', dtype='int32') \n",
    "embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n",
    "# we cannot  do mean pooling here on the embedding token because our data is vertical. \n",
    "# We would have to apply the latter functions to smash back together...\n",
    "# probably doable\n",
    "model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = cls_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "SNmTfteudmZf",
    "outputId": "52d2a404-a22c-4b7f-96ca-cadd43e3032f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_token (InputLayer)        [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masked_token (InputLayer)       [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model (TFDistilB ((None, 200, 768),)  66362880    input_token[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 66,362,880\n",
      "Trainable params: 66,362,880\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FpysgjIAaLq3"
   },
   "source": [
    "## Run Model on train, test and val to get feature embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hA68e5xzdlRv"
   },
   "outputs": [],
   "source": [
    "last_hidden_states = model.predict([input_ids, attention_masks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MuSq5ZpWFTeq",
    "outputId": "77f8f70e-7ff9-4136-d341-243b1271ae31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_last_hidden_states.pkl.sav']"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "filename = 'train_last_hidden_states.pkl.sav'\n",
    "joblib.dump(last_hidden_states, filename)  \n",
    "#with open('train_last_hidden_states.pkl', 'wb') as f: pickle.dump(last_hidden_states, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZuplN2wb4Ld"
   },
   "outputs": [],
   "source": [
    "test_last_hidden_states = model.predict([test_input_ids, test_attention_masks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-YGjhJ7yGZWk",
    "outputId": "bdce3893-2f05-4b55-b572-5136d96189b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_last_hidden_states.pkl.sav']"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "filename = 'test_last_hidden_states.pkl.sav'\n",
    "joblib.dump(test_last_hidden_states, filename) \n",
    "#with open('test_last_hidden_states.pkl', 'wb') as f: pickle.dump(test_last_hidden_states, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umVk59bmb4yz"
   },
   "outputs": [],
   "source": [
    "val_last_hidden_states = model.predict([val_input_ids, val_attention_masks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BhD_aMJY__Os",
    "outputId": "913f3047-4f86-4656-d956-fb67bd8d25e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['val_last_hidden_states.pkl.sav']"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "filename = 'val_last_hidden_states.pkl.sav'\n",
    "joblib.dump(val_last_hidden_states, filename) \n",
    "#with open('val_last_hidden_states.pkl', 'wb') as f: pickle.dump(val_last_hidden_states, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "20Jch0GaAMda",
    "outputId": "39ae6c70-fec9-47b3-d1db-d6dd8c764323"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1405"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QHL_yNOAhJDr"
   },
   "source": [
    "## Flatten the dataframes so that each row is a doc, and each feature is a list of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "wxRvag7U8AAh",
    "outputId": "2a75bdb7-fcd8-4f0e-af76-a6b2faf4b2db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6004\n",
      "768\n",
      "761\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_test))\n",
    "print(len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yN7ykDplfMhx"
   },
   "outputs": [],
   "source": [
    "df_train['feature_split'] = last_hidden_states[:,0,:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S890GgEuajfH"
   },
   "outputs": [],
   "source": [
    "# Put the data back together again - not the feature weights represent\n",
    "df_train = df_train.groupby(['docid', 'text', 'label']).agg(sum).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBjeuNdCix5Q"
   },
   "outputs": [],
   "source": [
    "# Divide up the feature_split column into equal 768 chunks\n",
    "df_train['features'] = df_train['feature_split'].apply((lambda x: [x[i:i + 768] for i in range(0, len(x), 768)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mflF-VWHZ5ZW"
   },
   "outputs": [],
   "source": [
    "df_test['feature_split'] = test_last_hidden_states[:,0,:].tolist()\n",
    "df_test = df_test.groupby(['docid', 'text', 'label']).agg(sum).reset_index()\n",
    "df_test['features'] = df_test['feature_split'].apply((lambda x: [x[i:i + 768] for i in range(0, len(x), 768)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "znjZv4WeaT5j"
   },
   "outputs": [],
   "source": [
    "df_val['feature_split'] = val_last_hidden_states[:,0,:].tolist()\n",
    "df_val = df_val.groupby(['docid', 'text', 'label']).agg(sum).reset_index()\n",
    "df_val['features'] = df_val['feature_split'].apply((lambda x: [x[i:i + 768] for i in range(0, len(x), 768)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xnXZ1JmU2kLS"
   },
   "outputs": [],
   "source": [
    "df_lstm_train = df_train[['features','label']].copy()\n",
    "df_lstm_test = df_test[['features','label']].copy()\n",
    "df_lstm_val = df_val[['features','label']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xTc-WvHiab3S",
    "outputId": "2747d716-2414-4306-bc52-5f2fb9dc0a2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((647, 2), (81, 2), (81, 2))"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check shapes of our new inputs\n",
    "df_lstm_train.shape, df_lstm_test.shape, df_lstm_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLgL25buWJqU"
   },
   "outputs": [],
   "source": [
    "df_lstm_train['mean_features'] = df_lstm_train['features'].apply((lambda x: [np.mean(x, axis=0)][0]))\n",
    "df_lstm_test['mean_features'] = df_lstm_test['features'].apply((lambda x: [np.mean(x, axis=0)][0]))\n",
    "df_lstm_val['mean_features'] = df_lstm_val['features'].apply((lambda x: [np.mean(x, axis=0)][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IMHAicm3LQB3"
   },
   "outputs": [],
   "source": [
    "df_lstm_train['max_features'] = df_lstm_train['features'].apply((lambda x: [np.max(x, axis=0)][0]))\n",
    "df_lstm_test['max_features'] = df_lstm_test['features'].apply((lambda x: [np.max(x, axis=0)][0]))\n",
    "df_lstm_val['max_features'] = df_lstm_val['features'].apply((lambda x: [np.max(x, axis=0)][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lAuRyinsHqcS",
    "outputId": "7c1c9cb4-e61c-4ac4-90b8-e16e48ae4a2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lstm_train.sav']"
      ]
     },
     "execution_count": 163,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(df_lstm_train, 'lstm_train.sav') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "meYcOS_uHrBd",
    "outputId": "7938c24a-73e0-4ed1-bc4f-5e7312f3975c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lstm_test.sav']"
      ]
     },
     "execution_count": 164,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(df_lstm_test, 'lstm_test.sav') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5gXaQ-rjHrWQ",
    "outputId": "44af171a-2639-4971-f5a6-bd2c3f309454"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lstm_val.sav']"
      ]
     },
     "execution_count": 165,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(df_lstm_val, 'lstm_val.sav') "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Bert_multi_chunk_TF2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
