{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "\n",
    "from utils import *\n",
    "from Custom_Dataset_Class import ICAADDataset1\n",
    "from Bert_Classification import Bert_Classification_Model\n",
    "from RoBERT import RoBERT_Model\n",
    "\n",
    "from BERT_Hierarchical import BERT_Hierarchical_Model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and segmentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing:\n",
    "The preprocessing step goes as follows:\n",
    "1. Remove all documents with fewer than 250 tokens. We want to concentrate only on long texts\n",
    "2. Consolidate the classes by combining those that are similar. (e.g.: \"Credit card\" or \"prepaid card\" complaints) :\n",
    " * Credit reporting‘ to ‘Credit reporting, credit repair services, or other personal consumerreports‘.\n",
    " * ‘Credit card‘ to ‘Credit card or prepaid card‘.\n",
    " * ‘Payday loan‘ to ‘Payday loan, title loan or personal loan‘1\n",
    " * ‘Virtual currency‘ to ‘Money transfer, virtual currency or money servic\n",
    "3. Remove all non-word characters\n",
    "4. Encode the labels\n",
    "5. Split the dataset in train set (80%) and validation set (20%).\n",
    "\n",
    "### 2. Segmentation and tokenization:\n",
    "First, each complaint is split into 200-token chunk with an overlap of 50 between each of them. This means that the last 50 tokens of a segment are the first 50 of the next segment.  \n",
    "Then, each segment is tokenized using BERT's tokenizer. This is needed for two main reasons:\n",
    "1. BERT's vocabulary is not made of just English words, but also subwords and single characters\n",
    "2. BERT does not take raw string as inputs. It needs:\n",
    "    - token ids: those values allow BERT to retrieve the tensor representation of a token\n",
    "    - input mask: a tensor of 0s and 1s that shows whether a token should be ignored (0) or not (1) by BERT\n",
    "    - segment ids: those are used to tell BERT what tokens form the first sentence and the second sentence (in the next sentence prediction task)  \n",
    "    \n",
    "The parameter `MAX_SEQ_LENGTH` ensures that any tokenized sentence longer that that number (200 in this case) will be truncated.  \n",
    "Each returned segment is given the same class as the document containing it.\n",
    "\n",
    "PyTorch offers the `Dataset` and `DataLoader` classes that make it easier to group the data reading and preparation operations while decreasing the memory usage in case the dataset is large.  \n",
    "We implemented the above steps in the `Custom_Dataset_Class.py` file. Our dataset class `ConsumerComplaintsDataset1` has a constructor (`__init__`) taking the necessary parameters to load the .csv file, segment the documents, and then tokenize them. It also preprocesses the data as explained in the preprocessing section.    \n",
    "The two other important methods are the following:\n",
    "- `__len__` returns the number of documents\n",
    "- `__getitem__` is the method where most of the work is done. It takes a tensor of idx values and returns the tokenized data:\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "As said above, there is differents approaches for overflowing tokens, we added the differents strategies (described in [the following paper](https://arxiv.org/abs/1905.05583)) via the use of the parameter `approach` in the initialisation of Consumer Complaints Dataset class, here is the differents value of the `approach` parameter to handle that situation :\n",
    "- **all**: overflowing tokens from a document are used to create new 200 token chunk with 50 tokens overlap between them\n",
    "- **head**: overflowing tokens are truncated. Only the first 200 tokens are used.\n",
    "- **tail**: only the last 200 tokens are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      "Nettoyage des données\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TRAIN_BATCH_SIZE=8\n",
    "EPOCH=3\n",
    "validation_split = .1\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "MIN_LEN=249\n",
    "MAX_LEN = 100000\n",
    "CHUNK_LEN=200\n",
    "OVERLAP_LEN=50\n",
    "#MAX_LEN=10000000\n",
    "#MAX_SIZE_DATASET=1000\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "#dataset=ConsumerComplaintsDataset1(\n",
    "dataset=ICAADDataset1(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    #max_size_dataset=MAX_SIZE_DATASET,\n",
    "    overlap_len=OVERLAP_LEN)\n",
    "\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "valid_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=valid_sampler,\n",
    "    collate_fn=my_collate1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fine-tuning on the 200 tokens chunks:\n",
    "BERT is fine-tuned on the 200 tokens chunks\n",
    "\n",
    "In our implementation, we put a neural network on top of the pooled output from BERT, each BERT's input token has a embeding as an output, the embedding of the `CLS` token (the first token) corresponds to the pooled output of the all sentence input (the 200 tokens chunk in our case).\n",
    "\n",
    "The neural network is composed of a dense layer with a SoftMax activation function.\n",
    "\n",
    "This Model corresponds to the class `Bert_Classification_Model` defined in the file `Bert_Classification.py`. This class inherits from `torch.nn.Module` which is the parent of all neural network models.\n",
    "\n",
    "Then, in the function `train_loop_fun1` defined in `utils.py`, for each batch, the list of dictionaries containing the values for the token_ids, masks, token_type_ids, and targets are respectively concatenated into `torch.tensors` which are then fed into the model in order to get predictions and apply backpropagation according to the Cross Entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First segmetation approach: all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this approach, we will consider each chunk of 200 tokens as a new document, so if a document is split into 3 chunk of 200, tokens we will consider each chunks as a new document with the same label.\\\n",
    "We use this approach to fine tune BERT, so this model will be used as an input for RoBERT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/each_Chunk_as_Document.png](img/each_Chunk_as_Document.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lr=3e-5#1e-3\n",
    "num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
    "\n",
    "model=Bert_Classification_Model().to(device)\n",
    "optimizer=AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert_Classification_Model(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== EPOCH 1 / 3 ===============\n",
      "\n",
      "___ batch index = 0 / 82 (0.00%), loss = 0.6782, time = 76.44 secondes ___\n",
      "\n",
      "*** avg_loss : 0.68, time : ~161.0 min (9685.20 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 0.74, time : 135.78 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.5430711610486891, 'nb exemple': 801, 'true_prediction': 435, 'false_prediction': 366}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 2 / 3 ===============\n",
      "\n",
      "___ batch index = 0 / 82 (0.00%), loss = 0.7360, time = 53.20 secondes ___\n",
      "\n",
      "*** avg_loss : 0.65, time : ~126.0 min (7617.40 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 0.70, time : 150.18 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.5767790262172284, 'nb exemple': 801, 'true_prediction': 462, 'false_prediction': 339}\n",
      "\t§§ model has been saved §§\n",
      "\n",
      "=============== EPOCH 3 / 3 ===============\n",
      "\n",
      "___ batch index = 0 / 82 (0.00%), loss = 0.6710, time = 116.83 secondes ___\n",
      "\n",
      "*** avg_loss : 0.64, time : ~122.0 min (7370.65 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 0.72, time : 139.84 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.5755305867665418, 'nb exemple': 801, 'true_prediction': 461, 'false_prediction': 340}\n",
      "\t§§ model has been saved §§\n"
     ]
    }
   ],
   "source": [
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "for epoch in range(EPOCH):\n",
    "    t0 = time.time()    \n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=train_loop_fun1(train_data_loader, model, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tm\n",
    "    \n",
    "    p=eval_loop_fun1(valid_data_loader, model, device)\n",
    "    print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")\n",
    "    tmp_evaluate=evaluate(target.reshape(-1), output)\n",
    "    print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    val_acc.append(tmp_evaluate['accuracy'])\n",
    "    val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(\"\\t§§ model has been saved §§\")\n",
    "    torch.save(model, f\"model1/model_epoch{epoch+1}.pt\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7e33cdc5d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(np.array([[np.mean(x) for x in batches_losses], [np.mean(x) for x in val_losses]]).T,\n",
    "                   columns=['Training', 'Validation']).plot(title=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7e3402cd90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(np.array(val_acc).T,\n",
    "                   columns=['Validation']).plot(title=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the Hierarchical methods applies to the ouputs of the Bert model (the embbeding)\n",
    "The input text is firstly divided into k = L/510 fractions, which is fed into BERT to obtain the representation of the k text fractions. The representation of each fraction is the hidden state of the `[CLS]` tokens of the last layer. Then we use mean pooling, max pooling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Pooling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this approach, we average the embedding of all k chunks across each dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/Mean_Pooling_Hierarchical.png](img/Mean_Pooling_Hierarchical.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      "Nettoyage des données\n",
      "\n",
      "=============== EPOCH 1 / 3 ===============\n",
      "\n",
      "___ batch index = 0 / 219 (0.00%), loss = 0.3630, time = 9.59 secondes ___\n",
      "\n",
      "*** avg_loss : 0.69, time : ~426.0 min (25613.06 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 0.71, time : 128.04 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.5138888888888888, 'nb exemple': 72, 'true_prediction': 37, 'false_prediction': 35}\n",
      "\t§§ the Hierarchical mean pooling model has been saved §§\n",
      "\n",
      "=============== EPOCH 2 / 3 ===============\n",
      "\n",
      "___ batch index = 0 / 219 (0.00%), loss = 0.4849, time = 14.38 secondes ___\n",
      "\n",
      "*** avg_loss : 0.70, time : ~85.0 min (5113.74 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 0.70, time : 123.28 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.5138888888888888, 'nb exemple': 72, 'true_prediction': 37, 'false_prediction': 35}\n",
      "\t§§ the Hierarchical mean pooling model has been saved §§\n",
      "\n",
      "=============== EPOCH 3 / 3 ===============\n",
      "\n",
      "___ batch index = 0 / 219 (0.00%), loss = 0.6644, time = 19.92 secondes ___\n",
      "\n",
      "*** avg_loss : 0.69, time : ~87.0 min (5269.80 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 0.73, time : 136.68 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.5138888888888888, 'nb exemple': 72, 'true_prediction': 37, 'false_prediction': 35}\n",
      "\t§§ the Hierarchical mean pooling model has been saved §§\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE=3\n",
    "EPOCH=3\n",
    "validation_split = .1\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "MIN_LEN=249\n",
    "MAX_LEN = 100000\n",
    "CHUNK_LEN=200\n",
    "OVERLAP_LEN=50\n",
    "#MAX_LEN=10000000\n",
    "#MAX_SIZE_DATASET=1000\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "dataset=ICAADDataset1(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    #max_size_dataset=MAX_SIZE_DATASET,\n",
    "    overlap_len=OVERLAP_LEN)\n",
    "\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "valid_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=valid_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "\n",
    "device=\"cpu\"\n",
    "lr=3e-5#1e-3\n",
    "num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
    "\n",
    "pooling_method=\"mean\"\n",
    "model_hierarchical=BERT_Hierarchical_Model(pooling_method=pooling_method).to(device)\n",
    "optimizer=AdamW(model_hierarchical.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "for epoch in range(EPOCH):\n",
    "    t0 = time.time()    \n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=rnn_train_loop_fun1(train_data_loader, model_hierarchical, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=rnn_eval_loop_fun1(valid_data_loader, model_hierarchical, device)\n",
    "    print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")    \n",
    "    tmp_evaluate=evaluate(target.reshape(-1), output)\n",
    "    print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    val_acc.append(tmp_evaluate['accuracy'])\n",
    "    val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(f\"\\t§§ the Hierarchical {pooling_method} pooling model has been saved §§\")\n",
    "    torch.save(model_hierarchical, f\"model_hierarchical/{pooling_method}_pooling/model_{pooling_method}_pooling_epoch{epoch+1}.pt\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(fname):\n",
    "    df = pd.read_csv(fname)\n",
    "    df = df[['cleaned_contents', 'Discrimination_Label']]\n",
    "    df = df.rename(columns = {'cleaned_contents':'text', 'Discrimination_Label':'label'})\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_preload = get_data('test_80_10_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_preload.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettoyage des données\n"
     ]
    }
   ],
   "source": [
    "dataset=ICAADDataset1(\n",
    "    file_location=\"test.csv\",\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    #max_size_dataset=MAX_SIZE_DATASET,\n",
    "    overlap_len=OVERLAP_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data indices for training and validation splits:\n",
    "test_indices = list(range(len(dataset)))\n",
    "test_sampler = SubsetRandomSampler(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=test_sampler,\n",
    "    collate_fn=my_collate1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT_Hierarchical_Model(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put model in evaluation mode\n",
    "model_hierarchical.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, target, val_losses_tmp=rnn_eval_loop_fun1(test_data_loader, model_hierarchical, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hireachical Mean Pooling\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        36\n",
      "           1       0.55      1.00      0.71        44\n",
      "\n",
      "    accuracy                           0.55        80\n",
      "   macro avg       0.28      0.50      0.35        80\n",
      "weighted avg       0.30      0.55      0.39        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "print(\"Hireachical Mean Pooling\")\n",
    "print(classification_report(dataset.label, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Pooling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this approach, we take the maximum embedding of all the k chunks across each dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/Max_Pooling_Hierarchical.png](img/Max_Pooling_Hierarchical.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      "Nettoyage des données\n",
      "\n",
      "=============== EPOCH 1 / 3 ===============\n",
      "\n",
      "___ batch index = 0 / 219 (0.00%), loss = 0.7650, time = 74.61 secondes ___\n",
      "\n",
      "*** avg_loss : 0.69, time : ~88.0 min (5322.42 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 0.66, time : 131.73 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.6527777777777778, 'nb exemple': 72, 'true_prediction': 47, 'false_prediction': 25}\n",
      "\t§§ the Hierarchical max pooling model has been saved §§\n",
      "\n",
      "=============== EPOCH 2 / 3 ===============\n",
      "\n",
      "___ batch index = 0 / 219 (0.00%), loss = 0.7522, time = 43.24 secondes ___\n",
      "\n",
      "*** avg_loss : 0.67, time : ~86.0 min (5182.24 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 0.65, time : 128.52 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.6944444444444444, 'nb exemple': 72, 'true_prediction': 50, 'false_prediction': 22}\n",
      "\t§§ the Hierarchical max pooling model has been saved §§\n",
      "\n",
      "=============== EPOCH 3 / 3 ===============\n",
      "\n",
      "___ batch index = 0 / 219 (0.00%), loss = 0.6293, time = 17.67 secondes ___\n",
      "\n",
      "*** avg_loss : 0.64, time : ~90.0 min (5424.26 sec) ***\n",
      "\n",
      "==> evaluation : avg_loss = 0.63, time : 139.89 sec\n",
      "\n",
      "=====>\t{'accuracy': 0.5833333333333334, 'nb exemple': 72, 'true_prediction': 42, 'false_prediction': 30}\n",
      "\t§§ the Hierarchical max pooling model has been saved §§\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE=3\n",
    "EPOCH=3\n",
    "validation_split = .1\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "MIN_LEN=249\n",
    "MAX_LEN = 100000\n",
    "CHUNK_LEN=200\n",
    "OVERLAP_LEN=50\n",
    "#MAX_LEN=10000000\n",
    "#MAX_SIZE_DATASET=1000\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "\n",
    "dataset=ICAADDataset1(\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    #max_size_dataset=MAX_SIZE_DATASET,\n",
    "    overlap_len=OVERLAP_LEN)\n",
    "\n",
    "\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "valid_data_loader=DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    sampler=valid_sampler,\n",
    "    collate_fn=my_collate1)\n",
    "\n",
    "\n",
    "device=\"cpu\"\n",
    "lr=3e-5#1e-3\n",
    "num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n",
    "\n",
    "pooling_method=\"max\"\n",
    "model_hierarchical=BERT_Hierarchical_Model(pooling_method=pooling_method).to(device)\n",
    "optimizer=AdamW(model_hierarchical.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = 0,\n",
    "                                        num_training_steps = num_training_steps)\n",
    "val_losses=[]\n",
    "batches_losses=[]\n",
    "val_acc=[]\n",
    "for epoch in range(EPOCH):\n",
    "    t0 = time.time()    \n",
    "    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n",
    "    batches_losses_tmp=rnn_train_loop_fun1(train_data_loader, model_hierarchical, optimizer, device)\n",
    "    epoch_loss=np.mean(batches_losses_tmp)\n",
    "    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n",
    "    t1=time.time()\n",
    "    output, target, val_losses_tmp=rnn_eval_loop_fun1(valid_data_loader, model_hierarchical, device)\n",
    "    print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")    \n",
    "    tmp_evaluate=evaluate(target.reshape(-1), output)\n",
    "    print(f\"=====>\\t{tmp_evaluate}\")\n",
    "    val_acc.append(tmp_evaluate['accuracy'])\n",
    "    val_losses.append(val_losses_tmp)\n",
    "    batches_losses.append(batches_losses_tmp)\n",
    "    print(f\"\\t§§ the Hierarchical {pooling_method} pooling model has been saved §§\")\n",
    "    torch.save(model_hierarchical, f\"model_hierarchical/{pooling_method}_pooling/model_{pooling_method}_pooling_epoch{epoch+1}.pt\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT_Hierarchical_Model(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put model in evaluation mode\n",
    "model_hierarchical.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettoyage des données\n"
     ]
    }
   ],
   "source": [
    "dataset=ICAADDataset1(\n",
    "    file_location=\"test.csv\",\n",
    "    tokenizer=bert_tokenizer,\n",
    "    min_len=MIN_LEN,\n",
    "    max_len=MAX_LEN,\n",
    "    chunk_len=CHUNK_LEN,\n",
    "    #max_size_dataset=MAX_SIZE_DATASET,\n",
    "    overlap_len=OVERLAP_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, target, val_losses_tmp=rnn_eval_loop_fun1(test_data_loader, model_hierarchical, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hireachical Mean Pooling\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.14      0.22        36\n",
      "           1       0.56      0.91      0.70        44\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.56      0.52      0.46        80\n",
      "weighted avg       0.56      0.56      0.48        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Hireachical Mean Pooling\")\n",
    "print(classification_report(dataset.label, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266",
   "language": "python",
   "name": "w266"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
