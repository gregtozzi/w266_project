{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "26jul_Bert-last512.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9xOw82pUtMr",
        "colab_type": "text"
      },
      "source": [
        "## **ICAAD DATA - Last n tokens, some manipulation of data**\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YwXs_GX5R3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3Cr3BkvY4cZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imports\n",
        "from transformers import BertTokenizer, BertConfig, TFBertModel, TFBertForSequenceClassification\n",
        "from transformers import DistilBertTokenizer, DistilBertConfig, TFDistilBertModel, TFDistilBertForSequenceClassification\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SULTsBSBY4kC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configs\n",
        "\n",
        "train_file = \"train_80_10_10.csv\"\n",
        "test_file = \"test_80_10_10.csv\"\n",
        "val_file = \"val_80_10_10.csv\"\n",
        "\n",
        "skip_lines = 6\n",
        "split_length = 510 # The max as we add two tokens\n",
        "\n",
        "# BERT CONFIG\n",
        "\n",
        "# BERT BASE\n",
        "#bert_file = 'bert-base-uncased'\n",
        "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "#bert_model = TFBertForSequenceClassification.from_pretrained(bert_file)\n",
        "\n",
        "# DistilBert\n",
        "bert_file = 'distilbert-base-uncased'\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(bert_file, do_lower_case=True)\n",
        "bert_model = TFDistilBertForSequenceClassification.from_pretrained(bert_file)\n",
        "\n",
        "max_length = 512\n",
        "\n",
        "# Model Training\n",
        "batch_size = 8\n",
        "epochs = 3\n",
        "learning_rate = 2e-5\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpGdnaz9puCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3JbLwrtp3UE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to get data\n",
        "def get_data(fname):\n",
        "    df = pd.read_csv(fname)\n",
        "    df = df[['docid', 'cleaned_contents', 'Discrimination_Label']]\n",
        "    df = df.rename(columns = {'cleaned_contents':'text', 'Discrimination_Label':'label'})\n",
        "    df.reset_index(inplace=True, drop=True)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mue4b1T7X2Tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Funtion to get the last 510 tokens only\n",
        "def end_tokens(df):\n",
        "  end_tokens = []\n",
        "\n",
        "  for row in df['tokens']:\n",
        "    end_tokens.append([row][0][-510:])\n",
        "  return end_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw99bv1UcSwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to create dictionary from lists for use in preparing tensorflow model input\n",
        "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
        "  return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_masks,\n",
        "  }, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUlXLcMccWmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to explode out tokens seried into pre-defined chunk lengths and return as inputs to model \n",
        "def prepare_df(df):\n",
        "\n",
        "  # add special tokens to beginning and end (assuming Bert tokenizer)\n",
        "  for row in df['end_tokens']:\n",
        "        row.insert(0,101)\n",
        "        row.append(102)\n",
        "\n",
        "  # create our input lists\n",
        "  tokenized = df['end_tokens']\n",
        "  input_ids = np.array([i + [0]*(split_length+2-len(i)) for i in tokenized.values])\n",
        "  attention_mask = np.where(input_ids != 0, 1, 0)\n",
        "  token_type_ids = np.where(input_ids != 0, 0, 0)\n",
        "  labels = df['label'].tolist()\n",
        "\n",
        "  # convert to tensorflow dataset object and return\n",
        "  return tf.data.Dataset.from_tensor_slices((input_ids, attention_mask, token_type_ids, labels)).map(map_example_to_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwisNUBwTu3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GET THE DATA\n",
        "df_train = get_data(train_file)\n",
        "df_test = get_data(test_file)\n",
        "df_val = get_data(val_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVG05sijWnuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove double new lines\n",
        "df_train['text'] = df_train['text'].replace('\\n\\s*\\n', '\\n',regex=True)\n",
        "df_test['text'] = df_test['text'].replace('\\n\\s*\\n', '\\n',regex=True)\n",
        "df_val['text'] = df_val['text'].replace('\\n\\s*\\n', '\\n',regex=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL7W514VTiGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# strip last n lines\n",
        "df_train['text'] = df_train.apply(lambda L: L.text.rsplit(\"\\n\",skip_lines)[0], axis=1)\n",
        "df_test['text'] = df_test.apply(lambda L: L.text.rsplit(\"\\n\",skip_lines)[0], axis=1)\n",
        "df_val['text'] = df_val.apply(lambda L: L.text.rsplit(\"\\n\",skip_lines)[0], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAeFvR3pX2hX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize data - capture all tokens. %%capture supresses message about length being too long\n",
        "%%capture \n",
        "df_train['tokens'] = df_train['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=False,)))\n",
        "df_val['tokens'] = df_val['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=False,)))\n",
        "df_test['tokens'] = df_test['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=False,)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfngcgK7YvyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grab just the last 510 tokens (that is where the magic happens!)\n",
        "df_train['end_tokens'] = end_tokens(df_train)\n",
        "df_val['end_tokens'] = end_tokens(df_val)\n",
        "df_test['end_tokens'] = end_tokens(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HltWBmIxXeSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create model input tensorflow dataset\n",
        "ds_encode_val =  prepare_df(df_val).batch(batch_size)\n",
        "ds_encode_train =  prepare_df(df_train).batch(batch_size)\n",
        "ds_encode_test =  prepare_df(df_test).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtu152VcXenq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# THE MODEL\n",
        "model = bert_model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
        "\n",
        "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_sm-CO-no-5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## ALTERNATIVE MODEL - FOR ILLUSTRATION PURPOSES ONLY\n",
        "###  NOT ACTIVELY USED\n",
        "#### NOTE HAS DIFFERENT INPUT - EXPECTS INPUT_ID, ATTENTION_MASK SEPERATELY INSTEAD OF IN A DATASET\n",
        "\n",
        "```\n",
        "config = DistilBertConfig.from_pretrained(\"distilbert-base-cased\", \n",
        "                                          dropout=0.2, \n",
        "                                          attention_dropout=0.2)\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(max_len,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(max_len,), name='masked_token', dtype='int32') \n",
        "\n",
        "transformer_model = TFDistilBertModel.from_pretrained('distilbert-base-cased', config = config)\n",
        "embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "cls_token = embedding_layer[:,0,:]\n",
        "\n",
        "X = tf.keras.layers.BatchNormalization()(cls_token)\n",
        "X = tf.keras.layers.Dense(64, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "outputs = tf.keras.layers.Dense(1, activation='softmax')(X)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = outputs)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxtdssuulI5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-a0CVxCXemV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYRe_1wddHnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train (fine tune) the model\n",
        "bert_history = model.fit(ds_encode_train, epochs=epochs, validation_data=ds_encode_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGb2sDf2dIBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EVALUATE THE MODEL\n",
        "model.evaluate(ds_encode_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysg4tk7_dH_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Predictions\n",
        "log_pred = model.predict(ds_encode_test)\n",
        "y_pred = np.argmax(log_pred[0], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w507VC5fm2th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show classification report\n",
        "#print(\"Bert base, standard inputs, chunk size = \", split_length)\n",
        "#print(classification_report(df_test['label'], y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7mw1JCudHwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show classification report\n",
        "print(\"DistilBert, standard inputs, chunk size = \", split_length)\n",
        "print(classification_report(df_test['label'], y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}