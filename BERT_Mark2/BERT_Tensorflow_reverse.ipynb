{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9xOw82pUtMr"
   },
   "source": [
    "## **ICAAD DATA - Last n tokens, some manipulation of data**\n",
    "\n",
    "> Indented block\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5YwXs_GX5R3l"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3Cr3BkvY4cZ"
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "from transformers import BertTokenizer, BertConfig, TFBertModel, TFBertForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, DistilBertConfig, TFDistilBertModel, TFDistilBertForSequenceClassification\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SULTsBSBY4kC"
   },
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "train_file = \"../data/train_80_10_10.csv\"\n",
    "test_file = \"../data/test_80_10_10.csv\"\n",
    "val_file = \"../data/val_80_10_10.csv\"\n",
    "\n",
    "skip_lines = 6\n",
    "split_length = 510 # The max as we add two tokens\n",
    "\n",
    "# BERT CONFIG\n",
    "\n",
    "# BERT BASE\n",
    "#bert_file = 'bert-base-uncased'\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "#bert_model = TFBertForSequenceClassification.from_pretrained(bert_file)\n",
    "\n",
    "# DistilBert\n",
    "bert_file = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(bert_file, do_lower_case=True)\n",
    "bert_model = TFDistilBertForSequenceClassification.from_pretrained(bert_file)\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "# Model Training\n",
    "batch_size = 8\n",
    "epochs = 3\n",
    "learning_rate = 2e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rpGdnaz9puCp"
   },
   "outputs": [],
   "source": [
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3JbLwrtp3UE"
   },
   "outputs": [],
   "source": [
    "# Function to get data\n",
    "def get_data(fname):\n",
    "    df = pd.read_csv(fname)\n",
    "    df = df[['docid', 'cleaned_contents', 'Discrimination_Label']]\n",
    "    df = df.rename(columns = {'cleaned_contents':'text', 'Discrimination_Label':'label'})\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mue4b1T7X2Tl"
   },
   "outputs": [],
   "source": [
    "#Funtion to get the last 510 tokens only\n",
    "def end_tokens(df):\n",
    "  end_tokens = []\n",
    "\n",
    "  for row in df['tokens']:\n",
    "    end_tokens.append([row][0][-510:])\n",
    "  return end_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jw99bv1UcSwS"
   },
   "outputs": [],
   "source": [
    "#Function to create dictionary from lists for use in preparing tensorflow model input\n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "  return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MUlXLcMccWmB"
   },
   "outputs": [],
   "source": [
    "# Function to explode out tokens seried into pre-defined chunk lengths and return as inputs to model \n",
    "def prepare_df(df):\n",
    "\n",
    "  # add special tokens to beginning and end (assuming Bert tokenizer)\n",
    "  for row in df['end_tokens']:\n",
    "        row.insert(0,101)\n",
    "        row.append(102)\n",
    "\n",
    "  # create our input lists\n",
    "  tokenized = df['end_tokens']\n",
    "  input_ids = np.array([i + [0]*(split_length+2-len(i)) for i in tokenized.values])\n",
    "  attention_mask = np.where(input_ids != 0, 1, 0)\n",
    "  token_type_ids = np.where(input_ids != 0, 0, 0)\n",
    "  labels = df['label'].tolist()\n",
    "\n",
    "  # convert to tensorflow dataset object and return\n",
    "  return tf.data.Dataset.from_tensor_slices((input_ids, attention_mask, token_type_ids, labels)).map(map_example_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwisNUBwTu3Z"
   },
   "outputs": [],
   "source": [
    "# GET THE DATA\n",
    "df_train = get_data(train_file)\n",
    "df_test = get_data(test_file)\n",
    "df_val = get_data(val_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PVG05sijWnuk"
   },
   "outputs": [],
   "source": [
    "#remove double new lines\n",
    "df_train['text'] = df_train['text'].replace('\\n\\s*\\n', '\\n',regex=True)\n",
    "df_test['text'] = df_test['text'].replace('\\n\\s*\\n', '\\n',regex=True)\n",
    "df_val['text'] = df_val['text'].replace('\\n\\s*\\n', '\\n',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FL7W514VTiGS"
   },
   "outputs": [],
   "source": [
    "# strip last n lines\n",
    "df_train['text'] = df_train.apply(lambda L: L.text.rsplit(\"\\n\",skip_lines)[0], axis=1)\n",
    "df_test['text'] = df_test.apply(lambda L: L.text.rsplit(\"\\n\",skip_lines)[0], axis=1)\n",
    "df_val['text'] = df_val.apply(lambda L: L.text.rsplit(\"\\n\",skip_lines)[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XAeFvR3pX2hX"
   },
   "outputs": [],
   "source": [
    "# Tokenize data - capture all tokens. %%capture supresses message about length being too long\n",
    "%%capture \n",
    "df_train['tokens'] = df_train['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=False,)))\n",
    "df_val['tokens'] = df_val['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=False,)))\n",
    "df_test['tokens'] = df_test['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=False,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tfngcgK7YvyR"
   },
   "outputs": [],
   "source": [
    "# Grab just the last 510 tokens (that is where the magic happens!)\n",
    "df_train['end_tokens'] = end_tokens(df_train)\n",
    "df_val['end_tokens'] = end_tokens(df_val)\n",
    "df_test['end_tokens'] = end_tokens(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HltWBmIxXeSH"
   },
   "outputs": [],
   "source": [
    "# Create model input tensorflow dataset\n",
    "ds_encode_val =  prepare_df(df_val).batch(batch_size)\n",
    "ds_encode_train =  prepare_df(df_train).batch(batch_size)\n",
    "ds_encode_test =  prepare_df(df_test).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wtu152VcXenq"
   },
   "outputs": [],
   "source": [
    "# THE MODEL\n",
    "model = bert_model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k_sm-CO-no-5"
   },
   "source": [
    "\n",
    "## ALTERNATIVE MODEL - FOR ILLUSTRATION PURPOSES ONLY\n",
    "###  NOT ACTIVELY USED\n",
    "#### NOTE HAS DIFFERENT INPUT - EXPECTS INPUT_ID, ATTENTION_MASK SEPERATELY INSTEAD OF IN A DATASET\n",
    "\n",
    "```\n",
    "config = DistilBertConfig.from_pretrained(\"distilbert-base-cased\", \n",
    "                                          dropout=0.2, \n",
    "                                          attention_dropout=0.2)\n",
    "\n",
    "input_ids_in = tf.keras.layers.Input(shape=(max_len,), name='input_token', dtype='int32')\n",
    "input_masks_in = tf.keras.layers.Input(shape=(max_len,), name='masked_token', dtype='int32') \n",
    "\n",
    "transformer_model = TFDistilBertModel.from_pretrained('distilbert-base-cased', config = config)\n",
    "embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n",
    "cls_token = embedding_layer[:,0,:]\n",
    "\n",
    "X = tf.keras.layers.BatchNormalization()(cls_token)\n",
    "X = tf.keras.layers.Dense(64, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.2)(X)\n",
    "outputs = tf.keras.layers.Dense(1, activation='softmax')(X)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = outputs)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nxtdssuulI5m"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-a0CVxCXemV"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYRe_1wddHnK"
   },
   "outputs": [],
   "source": [
    "# Train (fine tune) the model\n",
    "bert_history = model.fit(ds_encode_train, epochs=epochs, validation_data=ds_encode_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGb2sDf2dIBX"
   },
   "outputs": [],
   "source": [
    "# EVALUATE THE MODEL\n",
    "model.evaluate(ds_encode_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ysg4tk7_dH_M"
   },
   "outputs": [],
   "source": [
    "# Get Predictions\n",
    "log_pred = model.predict(ds_encode_test)\n",
    "y_pred = np.argmax(log_pred[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w507VC5fm2th"
   },
   "outputs": [],
   "source": [
    "# Show classification report\n",
    "#print(\"Bert base, standard inputs, chunk size = \", split_length)\n",
    "#print(classification_report(df_test['label'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7mw1JCudHwx"
   },
   "outputs": [],
   "source": [
    "# Show classification report\n",
    "print(\"DistilBert, standard inputs, chunk size = \", split_length)\n",
    "print(classification_report(df_test['label'], y_pred))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "26jul_Bert-last512.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "w266",
   "language": "python",
   "name": "w266"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
