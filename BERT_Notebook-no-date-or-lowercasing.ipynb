{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "import torch # a tensor library\n",
    "# the huggingface transformers library (pre-trained deep learning for NLP models)\n",
    "# run !pip install transformers in a Jupyter Notebook cell\n",
    "import transformers as ppb \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SENTENCE\\n\\n\\t1.\\tYou are charged as follows:\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SENTENCE\\n\\n\\t1.\\tJOSEFA KOTOBALAVU, you were ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SENTENCE\\n\\n1. The Director of Public Prosecut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SENTENCE\\n\\n\\t1.\\tMOHOMMED NABI UD- DEAN, you ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JUDGMENT OF THE COURT\\n\\nBackground\\n\\n[1] The...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  SENTENCE\\n\\n\\t1.\\tYou are charged as follows:\\...      0\n",
       "1  SENTENCE\\n\\n\\t1.\\tJOSEFA KOTOBALAVU, you were ...      1\n",
       "2  SENTENCE\\n\\n1. The Director of Public Prosecut...      1\n",
       "3  SENTENCE\\n\\n\\t1.\\tMOHOMMED NABI UD- DEAN, you ...      1\n",
       "4  JUDGMENT OF THE COURT\\n\\nBackground\\n\\n[1] The...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"../w266_project/data/train.csv\")\n",
    "test_data = pd.read_csv(\"../w266_project/data/test.csv\")\n",
    "df = pd.concat([train_data,test_data])\n",
    "df['len_txt'] =df.cleaned_contents.apply(lambda x: len(x.split()))\n",
    "df = df[df.len_txt >249]\n",
    "df = df[df.len_txt <20000]\n",
    "df = df[['cleaned_contents', 'Discrimination_Label']]\n",
    "df = df.rename(columns = {'cleaned_contents':'text', 'Discrimination_Label':'label'})\n",
    "#lower case to help remove dates\n",
    "#df['text'] = df['text'].str.lower()\n",
    "#remove dates\n",
    "#df['text'] = pd.Series(re.sub(r'(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)([\\s]{1,3})?([0-9]{1,2})(.{1,3})?((,)|(.))?([\\s]{1,3})?([0-9]{4})|([0-9]{1,2})(.{1,3})?([\\s]{1,3})?(day)?([\\s]{1,3})?(of)?([\\s]{1,3})?(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)((,)|(.))?(\\s{1,3})?([0-9]{4})|(first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth|eleventh|twelfth|thirteenth|fourteenth|fifteenth|sixteenth|seventeenth|eighteenth|nineteenth|twentieth|twenty-first|twenty-second|twenty-third|twenty-fourth|twenty-fifth|twenty-sixth|twenty-seventh|twenty-eighth|twenty-ninth|thirtieth|thirty-first)([\\s]{1,3})?(day)?([\\s]{1,3})?(of)?([\\s]{1,3})?(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)((,)|(.))?(\\s{1,3})?([0-9]{4})|(\\b[0-9]{1,2}(\\-|\\/)[0-9]{1,2}(\\-|\\/)[0-9]{2,4}\\b)|(\\b[0-9]{2,4}(\\-|\\/)[0-9]{1,2}(\\-|\\/)[0-9]{1,2}\\b)', '[DATE]', i) for i in df['text'])\n",
    "#remove special character\n",
    "#df['text'] = pd.Series(re.sub(\"'\", \"\", i) for i in df['text'])\n",
    "#df['text'] = pd.Series(re.sub(\"(\\\\W)+\", \" \", i) for i in df['text'])\n",
    "\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[DATE]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "txt = \"fourth july 2020\"\n",
    "\n",
    "re.sub(r'(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)([\\s]{1,3})?([0-9]{1,2})(.{1,3})?((,)|(.))?([\\s]{1,3})?([0-9]{4})|([0-9]{1,2})(.{1,3})?([\\s]{1,3})?(day)?([\\s]{1,3})?(of)?([\\s]{1,3})?(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)((,)|(.))?(\\s{1,3})?([0-9]{4})|(first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth|eleventh|twelfth|thirteenth|fourteenth|fifteenth|sixteenth|seventeenth|eighteenth|nineteenth|twentieth|twenty-first|twenty-second|twenty-third|twenty-fourth|twenty-fifth|twenty-sixth|twenty-seventh|twenty-eighth|twenty-ninth|thirtieth|thirty-first)([\\s]{1,3})?(day)?([\\s]{1,3})?(of)?([\\s]{1,3})?(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)((,)|(.))?(\\s{1,3})?([0-9]{4})|(\\b[0-9]{1,2}(\\-|\\/)[0-9]{1,2}(\\-|\\/)[0-9]{2,4}\\b)|(\\b[0-9]{2,4}(\\-|\\/)[0-9]{1,2}(\\-|\\/)[0-9]{1,2}\\b)', '[DATE]', txt) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10676"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REduce to 512 tokens - does not work as some sentences are still longer)\n",
    "#df['start_txt'] = df['text'].apply(lambda x: ' '.join(x.split()[:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trained BERT model and Bert tokenizer\n",
    "vocab, config, and model files will be downloaded from https://s3.amazonaws.com/models.huggingface.co/bert/ to /home/rdadmin/.cache/torch/transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# we need a BERT model and a BERT tokenizer\n",
    "# initialize the empty model and tokenizer objects\n",
    "# we are going to use the Hugging Face's DistilBert model\n",
    "BERT_model_class,BERT_tokenizer_class,BERT_pre_trained_weights = (ppb.DistilBertModel, # the pre-trained DistillBERT model\n",
    "                                                                  ppb.DistilBertTokenizer,\n",
    "                                                                  'distilbert-base-uncased') # the type of DistilBERT model\n",
    "\n",
    "# use the next line instead, if you want (Google's) BERT instead of DistillBERT\n",
    "#BERT_model_class,BERT_tokenizer_class,BERT_pre_trained_weights = (ppb.BertModel, ppb.BertTokenizer,'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer weights/values for the desired type of BERT model into their respective objects\n",
    "tokenizer = BERT_tokenizer_class.from_pretrained(BERT_pre_trained_weights)\n",
    "\n",
    "#model1 is a pytorch BERT model\n",
    "model1 = BERT_model_class.from_pretrained(BERT_pre_trained_weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before DistilBERT can process this as input, weâ€™ll need to make all the vectors the same size by padding \n",
    "shorter sentences with the token id 0. After the padding, we have a matrix/tensor that is ready to be passed to BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  SENTENCE\\n\\n\\t1.\\tYou are charged as follows:\\...      0\n",
      "1  SENTENCE\\n\\n\\t1.\\tJOSEFA KOTOBALAVU, you were ...      1\n",
      "2  SENTENCE\\n\\n1. The Director of Public Prosecut...      1\n",
      "3  SENTENCE\\n\\n\\t1.\\tMOHOMMED NABI UD- DEAN, you ...      1\n",
      "4  JUDGMENT OF THE COURT\\n\\nBackground\\n\\n[1] The...      0\n",
      "\n",
      "\n",
      "0    [101, 6251, 1015, 1012, 2017, 2024, 5338, 2004...\n",
      "1    [101, 6251, 1015, 1012, 12947, 2050, 12849, 34...\n",
      "2    [101, 6251, 1015, 1012, 1996, 2472, 1997, 2270...\n",
      "3    [101, 6251, 1015, 1012, 9587, 23393, 7583, 658...\n",
      "4    [101, 8689, 1997, 1996, 2457, 4281, 1031, 1015...\n",
      "Name: text, dtype: object\n",
      "\n",
      "\n",
      "(806,)\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "tokenized = df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512)))\n",
    "# This turns every sentence into a list of IDs\n",
    "# tokenized is apandas Series object: <class 'pandas.core.series.Series'>\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "print(tokenized.head())\n",
    "print('\\n')\n",
    "print(tokenized.shape) #(6920,) a 1D pandas Series\n",
    "print(type(tokenized)) #<class 'pandas.core.series.Series'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized.values.shape: (806,)\n",
      "max sentence length is : 512\n",
      "padded.shape: (806, 512)\n"
     ]
    }
   ],
   "source": [
    "print(f'tokenized.values.shape: {tokenized.values.shape}')\n",
    "# find the length of the longest sentence in the dataset\n",
    "max_len = 0\n",
    "for i in tokenized.values:  #tokenized.values is of type #<class 'numpy.ndarray'>\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "print(f'max sentence length is : {max_len}')\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "print(f'padded.shape: {padded.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6251, 1015, 1012, 2017, 2024, 5338, 2004, 4076, 1024, 2034, 4175, 4861, 1997, 15226, 9040, 1024, 10043, 2000, 5433, 17332, 1998, 5018, 1997, 1996, 18476, 3642, 1010, 6178, 2459, 1012, 3327, 2015, 1997, 15226, 14262, 14545, 2401, 3972, 2050, 2090, 1996, 3083, 2254, 1010, 2289, 2000, 17089, 2154, 1997, 2285, 1010, 2289, 1010, 2012, 23233, 12260, 2072, 2352, 1010, 12436, 8525, 24861, 2721, 1010, 11937, 19722, 2050, 1999, 1996, 2530, 2407, 1010, 2018, 22300, 2482, 12032, 3716, 1997, 1050, 2078, 1010, 2302, 2014, 9619, 1012, 2117, 4175, 4861, 1997, 15226, 9040, 1024, 10043, 2000, 2930, 19843, 1006, 1015, 1007, 1998, 1006, 1016, 1007, 1006, 1037, 1007, 1997, 1996, 6997, 10037, 4008, 1997, 2268, 1012, 3327, 2015, 1997, 15226, 14262, 14545, 2401, 3972, 2050, 2090, 1996, 3083, 2251, 2262, 2000, 1996, 17089, 2251, 2262, 1010, 2012, 23233, 12260, 2072, 2352, 1010, 12436, 8525, 24861, 2721, 1010, 11937, 19722, 2050, 1999, 1996, 2530, 2407, 21653, 1996, 12436, 20876, 1997, 1050, 2078, 1010, 2007, 2010, 19085, 1010, 2302, 2014, 9619, 1012, 1015, 1012, 2006, 15538, 2281, 2286, 2017, 12254, 5905, 2000, 2119, 5571, 2114, 2017, 1998, 4914, 1996, 12654, 1997, 8866, 2006, 4343, 2285, 2286, 1012, 1016, 1012, 1996, 12654, 1997, 8866, 7864, 2011, 1996, 2110, 9517, 2163, 2004, 4076, 1024, 1996, 5496, 1010, 14262, 14545, 2401, 3972, 2050, 2003, 4229, 2086, 1997, 23233, 12260, 2072, 2352, 1010, 12436, 8525, 24861, 2721, 1010, 11937, 19722, 2050, 1012, 1996, 17612, 4630, 1999, 2023, 2553, 2003, 2028, 1050, 2078, 1010, 2184, 2086, 1997, 1996, 2168, 2352, 1012, 1996, 17612, 4630, 2001, 2184, 2086, 2214, 1010, 14118, 1999, 23233, 12260, 2072, 3234, 2082, 2043, 1996, 2034, 5043, 2165, 2173, 1012, 2016, 2071, 9131, 2008, 1999, 1996, 2095, 2289, 2076, 1996, 2117, 2744, 2082, 1998, 1999, 3327, 2006, 1996, 2154, 2016, 2106, 2025, 2175, 2000, 2082, 1996, 17612, 4630, 2001, 2409, 2011, 2014, 2388, 2000, 2175, 1998, 2031, 2014, 7198, 1999, 1996, 2314, 2279, 2000, 1996, 2352, 1012, 2016, 2187, 2014, 2188, 2894, 2000, 2031, 2014, 7198, 1998, 2006, 1996, 2126, 2067, 2016, 2777, 1996, 5496, 3564, 2279, 2000, 1996, 2314, 2924, 1012, 1996, 5496, 5411, 1996, 17612, 4630, 1998, 2356, 2014, 2000, 12673, 2014, 2000, 1996, 19739, 12462, 8983, 2000, 8145, 19739, 12462, 2015, 1012, 1996, 17612, 4630, 9480, 1996, 5496, 1998, 2628, 2032, 1010, 3402, 1996, 5496, 2409, 2014, 2000, 2644, 1998, 4682, 2091, 2006, 1996, 2598, 1012, 2002, 2059, 5411, 2014, 2000, 6366, 2014, 4253, 1998, 7420, 2014, 2065, 2016, 2987, 1005, 1056, 2059, 2002, 2097, 3786, 2014, 2007, 1037, 19739, 12462, 6293, 1012, 1996, 17612, 4630, 2001, 6015, 2043, 2016, 2657, 2023, 1998, 2628, 1996, 5496, 1005, 1055, 8128, 1012, 2043, 1996, 17612, 4630, 2288, 6151, 16119, 2002, 2387, 1996, 5496, 16916, 2091, 1998, 2478, 2010, 4416, 2000, 15385, 2014, 12436, 20876, 1012, 1996, 5496, 2059, 4895, 5831, 11469, 2010, 1093, 6471, 1998, 2059, 12889, 2010, 7019, 19085, 2046, 1996, 17612, 4630, 1005, 1055, 12436, 20876, 1012, 2016, 2001, 1999, 2307, 3255, 1998, 2318, 6933, 1012, 2016, 2001, 13346, 2004, 1996, 5496, 2006, 2327, 1997, 2014, 1012, 2016, 2071, 2514, 2008, 2014, 12436, 102]\n",
      "\n",
      "\n",
      "[  101  6251  1015  1012  2017  2024  5338  2004  4076  1024  2034  4175\n",
      "  4861  1997 15226  9040  1024 10043  2000  5433 17332  1998  5018  1997\n",
      "  1996 18476  3642  1010  6178  2459  1012  3327  2015  1997 15226 14262\n",
      " 14545  2401  3972  2050  2090  1996  3083  2254  1010  2289  2000 17089\n",
      "  2154  1997  2285  1010  2289  1010  2012 23233 12260  2072  2352  1010\n",
      " 12436  8525 24861  2721  1010 11937 19722  2050  1999  1996  2530  2407\n",
      "  1010  2018 22300  2482 12032  3716  1997  1050  2078  1010  2302  2014\n",
      "  9619  1012  2117  4175  4861  1997 15226  9040  1024 10043  2000  2930\n",
      " 19843  1006  1015  1007  1998  1006  1016  1007  1006  1037  1007  1997\n",
      "  1996  6997 10037  4008  1997  2268  1012  3327  2015  1997 15226 14262\n",
      " 14545  2401  3972  2050  2090  1996  3083  2251  2262  2000  1996 17089\n",
      "  2251  2262  1010  2012 23233 12260  2072  2352  1010 12436  8525 24861\n",
      "  2721  1010 11937 19722  2050  1999  1996  2530  2407 21653  1996 12436\n",
      " 20876  1997  1050  2078  1010  2007  2010 19085  1010  2302  2014  9619\n",
      "  1012  1015  1012  2006 15538  2281  2286  2017 12254  5905  2000  2119\n",
      "  5571  2114  2017  1998  4914  1996 12654  1997  8866  2006  4343  2285\n",
      "  2286  1012  1016  1012  1996 12654  1997  8866  7864  2011  1996  2110\n",
      "  9517  2163  2004  4076  1024  1996  5496  1010 14262 14545  2401  3972\n",
      "  2050  2003  4229  2086  1997 23233 12260  2072  2352  1010 12436  8525\n",
      " 24861  2721  1010 11937 19722  2050  1012  1996 17612  4630  1999  2023\n",
      "  2553  2003  2028  1050  2078  1010  2184  2086  1997  1996  2168  2352\n",
      "  1012  1996 17612  4630  2001  2184  2086  2214  1010 14118  1999 23233\n",
      " 12260  2072  3234  2082  2043  1996  2034  5043  2165  2173  1012  2016\n",
      "  2071  9131  2008  1999  1996  2095  2289  2076  1996  2117  2744  2082\n",
      "  1998  1999  3327  2006  1996  2154  2016  2106  2025  2175  2000  2082\n",
      "  1996 17612  4630  2001  2409  2011  2014  2388  2000  2175  1998  2031\n",
      "  2014  7198  1999  1996  2314  2279  2000  1996  2352  1012  2016  2187\n",
      "  2014  2188  2894  2000  2031  2014  7198  1998  2006  1996  2126  2067\n",
      "  2016  2777  1996  5496  3564  2279  2000  1996  2314  2924  1012  1996\n",
      "  5496  5411  1996 17612  4630  1998  2356  2014  2000 12673  2014  2000\n",
      "  1996 19739 12462  8983  2000  8145 19739 12462  2015  1012  1996 17612\n",
      "  4630  9480  1996  5496  1998  2628  2032  1010  3402  1996  5496  2409\n",
      "  2014  2000  2644  1998  4682  2091  2006  1996  2598  1012  2002  2059\n",
      "  5411  2014  2000  6366  2014  4253  1998  7420  2014  2065  2016  2987\n",
      "  1005  1056  2059  2002  2097  3786  2014  2007  1037 19739 12462  6293\n",
      "  1012  1996 17612  4630  2001  6015  2043  2016  2657  2023  1998  2628\n",
      "  1996  5496  1005  1055  8128  1012  2043  1996 17612  4630  2288  6151\n",
      " 16119  2002  2387  1996  5496 16916  2091  1998  2478  2010  4416  2000\n",
      " 15385  2014 12436 20876  1012  1996  5496  2059  4895  5831 11469  2010\n",
      "  1093  6471  1998  2059 12889  2010  7019 19085  2046  1996 17612  4630\n",
      "  1005  1055 12436 20876  1012  2016  2001  1999  2307  3255  1998  2318\n",
      "  6933  1012  2016  2001 13346  2004  1996  5496  2006  2327  1997  2014\n",
      "  1012  2016  2071  2514  2008  2014 12436   102]\n",
      "\n",
      "len(padded[0]): 512\n"
     ]
    }
   ],
   "source": [
    "print(tokenized[0])\n",
    "print('\\n')\n",
    "print(padded[0])\n",
    "print(f'\\nlen(padded[0]): {len(padded[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking - If we directly send padded to BERT, that would slightly confuse it. We need to create another\n",
    "variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what\n",
    "attention_mask is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(806, 512)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask[0])\n",
    "print(attention_mask[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting from numpy.ndarray to pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model Runtime: 0:16:36.761608\n"
     ]
    }
   ],
   "source": [
    "#create a tensor for the attention_mask\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "#We now create an input tensor out of the padded token matrix, and send that to DistilBERT\n",
    "input_ids = torch.tensor(padded) \n",
    "#the model() function runs our sentences through BERT. The results of the processing will be returned into last_hidden_states.\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "with torch.no_grad(): #deactivates autograd engine\n",
    "    last_hidden_states = model1(input_ids, attention_mask=attention_mask) #transformers.modeling_distilbert.DistilBertModel\n",
    "    # we could also simply do this\n",
    "    #last_hidden_states = model1(input_ids)\n",
    "print(f'BERT Model Runtime: {datetime.datetime.now() - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this step, last_hidden_states holds the outputs of DistilBERT. It is a tuple with the shape\n",
    "(number of examples, max number of tokens in the sequence, number of hidden units in the DistilBERT model).\n",
    "In our case, this will be 806 , 512 (which is the number of tokens in the longest sequence from the 806 examples),\n",
    "768 (the number of hidden units in the DistilBERT model). T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([806, 512, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(last_hidden_states) # tuple - the model output is a tuple\n",
    "type(last_hidden_states[0]) # torch.Tensor - First element of that tuple is the output tensor \n",
    "last_hidden_states[0].shape # 806 X 512 X 768 cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence.\n",
    "The way BERT does sentence classification, is that it adds a token called [CLS] (for classification) at the beginning\n",
    "of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
    "so we select that slice of the cube and discard everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(806, 768)\n"
     ]
    }
   ],
   "source": [
    "#cube slicing\n",
    "# [:,0,:] means we need all rows,column 0 (first output - for the CLS token),all depth of the cube (all hidden units)\n",
    "#also convert back from torch tensors to numpy because sklearn works with numpy.ndarray and not tensors\n",
    "features = last_hidden_states[0][:,0,:].numpy() # features for the logistic regression model - look at the image\n",
    "\n",
    "print(type(features))\n",
    "print(features.shape) # a slice of the cube depth (3D) is 2D - (806, 768)\n",
    "# we have 806  sentences and so we will have 806 sentence embeddings\n",
    "# each sentence embedding will have the size of the hidden units for each token in the last layer of BERT, which is 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "806\n"
     ]
    }
   ],
   "source": [
    "labels = df['label']\n",
    "\n",
    "print(type(labels))\n",
    "print(len(labels)) #806 labels as expected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL # 2 - train/test split\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=42, shuffle=True) # split is 20% test and 80% train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features.shape: (644, 768)\n",
      "test_features.shape: (162, 768)\n",
      "train_labels.shape: (644,)\n",
      "test_labels.shape: (162,)\n"
     ]
    }
   ],
   "source": [
    "type(train_features) #numpy.ndarray\n",
    "print(f'train_features.shape: {train_features.shape}') # 75%\n",
    "print(f'test_features.shape: {test_features.shape}') # 25%\n",
    "print(f'train_labels.shape: {train_labels.shape}')\n",
    "print(f'test_labels.shape: {test_labels.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Model  - sklearn Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'C': array([1.00000000e-04, 5.26325263e+00, 1.05264053e+01, 1.57895579e+01,\n",
      "       2.10527105e+01, 2.63158632e+01, 3.15790158e+01, 3.68421684e+01,\n",
      "       4.21053211e+01, 4.73684737e+01, 5.26316263e+01, 5.78947789e+01,\n",
      "       6.31579316e+01, 6.84210842e+01, 7.36842368e+01, 7.89473895e+01,\n",
      "       8.42105421e+01, 8.94736947e+01, 9.47368474e+01, 1.00000000e+02])}\n",
      "Grid Search time: 0:00:05.643411\n",
      "best parameters:  {'C': 94.73684736842105}\n",
      "best scrores:  0.6133236434108527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'C': np.linspace(0.0001, 100, 20)} #try 20 values between 1e-4 and 1e+2\n",
    "print(f'parameters: {parameters}')\n",
    "grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "\n",
    "begin_time = datetime.datetime.now()\n",
    "grid_search.fit(train_features, train_labels)\n",
    "print(f'Grid Search time: {datetime.datetime.now() - begin_time}')\n",
    "\n",
    "print('best parameters: ', grid_search.best_params_)\n",
    "print('best scrores: ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train a sklearn model with the best parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Runtime: 0:00:00.149891\n"
     ]
    }
   ],
   "source": [
    "# train a logistic regression model\n",
    "lr_clf = LogisticRegression(C=grid_search.best_params_['C'])\n",
    "start_time = datetime.datetime.now()\n",
    "lr_clf.fit(train_features, train_labels)\n",
    "print(f'Logistic Regression Model Runtime: {datetime.datetime.now() - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5802469135802469"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate MODEL #2\n",
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier score: 0.498 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "#How good is this score? What can we compare it against? Let's first look at a dummy classifier:\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier()\n",
    "\n",
    "scores = cross_val_score(clf, train_features, train_labels)\n",
    "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266",
   "language": "python",
   "name": "w266"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
