{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "import torch # a tensor library\n",
    "# the huggingface transformers library (pre-trained deep learning for NLP models)\n",
    "# run !pip install transformers in a Jupyter Notebook cell\n",
    "import transformers as ppb \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence 1 you are charged as follows first co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence 1 josefa kotobalavu you were charged ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence 1 the director of public prosecution ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence 1 mohommed nabi ud dean you were conv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>judgment of the court background 1 the appella...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  sentence 1 you are charged as follows first co...      0\n",
       "1  sentence 1 josefa kotobalavu you were charged ...      1\n",
       "2  sentence 1 the director of public prosecution ...      1\n",
       "3  sentence 1 mohommed nabi ud dean you were conv...      1\n",
       "4  judgment of the court background 1 the appella...      0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"../w266_project/data/train.csv\")\n",
    "test_data = pd.read_csv(\"../w266_project/data/test.csv\")\n",
    "df = pd.concat([train_data,test_data])\n",
    "df['len_txt'] =df.cleaned_contents.apply(lambda x: len(x.split()))\n",
    "df = train_raw[train_raw.len_txt >249]\n",
    "df = train_raw[train_raw.len_txt <20000]\n",
    "df = df[['cleaned_contents', 'Discrimination_Label']]\n",
    "df = df.rename(columns = {'cleaned_contents':'text', 'Discrimination_Label':'label'})\n",
    "#lower case to help remove dates\n",
    "df['text'] = df['text'].str.lower()\n",
    "#remove dates\n",
    "df['text'] = pd.Series(re.sub(r'(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)([\\s]{1,3})?([0-9]{1,2})(.{1,3})?((,)|(.))?([\\s]{1,3})?([0-9]{4})|([0-9]{1,2})(.{1,3})?([\\s]{1,3})?(day)?([\\s]{1,3})?(of)?([\\s]{1,3})?(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)((,)|(.))?(\\s{1,3})?([0-9]{4})|(first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth|eleventh|twelfth|thirteenth|fourteenth|fifteenth|sixteenth|seventeenth|eighteenth|nineteenth|twentieth|twenty-first|twenty-second|twenty-third|twenty-fourth|twenty-fifth|twenty-sixth|twenty-seventh|twenty-eighth|twenty-ninth|thirtieth|thirty-first)([\\s]{1,3})?(day)?([\\s]{1,3})?(of)?([\\s]{1,3})?(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)((,)|(.))?(\\s{1,3})?([0-9]{4})|(\\b[0-9]{1,2}(\\-|\\/)[0-9]{1,2}(\\-|\\/)[0-9]{2,4}\\b)|(\\b[0-9]{2,4}(\\-|\\/)[0-9]{1,2}(\\-|\\/)[0-9]{1,2}\\b)', '[DATE]', i) for i in df['text'])\n",
    "#remove special character\n",
    "df['text'] = pd.Series(re.sub(\"'\", \"\", i) for i in df['text'])\n",
    "df['text'] = pd.Series(re.sub(\"(\\\\W)+\", \" \", i) for i in df['text'])\n",
    "\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10184"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REduce to 512 tokens - does not work as some sentences are still longer)\n",
    "#df['start_txt'] = df['text'].apply(lambda x: ' '.join(x.split()[:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trained BERT model and Bert tokenizer\n",
    "vocab, config, and model files will be downloaded from https://s3.amazonaws.com/models.huggingface.co/bert/ to /home/rdadmin/.cache/torch/transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# we need a BERT model and a BERT tokenizer\n",
    "# initialize the empty model and tokenizer objects\n",
    "# we are going to use the Hugging Face's DistilBert model\n",
    "BERT_model_class,BERT_tokenizer_class,BERT_pre_trained_weights = (ppb.DistilBertModel, # the pre-trained DistillBERT model\n",
    "                                                                  ppb.DistilBertTokenizer,\n",
    "                                                                  'distilbert-base-cased') # the type of DistilBERT model\n",
    "\n",
    "# use the next line instead, if you want (Google's) BERT instead of DistillBERT\n",
    "#BERT_model_class,BERT_tokenizer_class,BERT_pre_trained_weights = (ppb.BertModel, ppb.BertTokenizer,'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer weights/values for the desired type of BERT model into their respective objects\n",
    "tokenizer = BERT_tokenizer_class.from_pretrained(BERT_pre_trained_weights)\n",
    "\n",
    "#model1 is a pytorch BERT model\n",
    "model1 = BERT_model_class.from_pretrained(BERT_pre_trained_weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before DistilBERT can process this as input, weâ€™ll need to make all the vectors the same size by padding \n",
    "shorter sentences with the token id 0. After the padding, we have a matrix/tensor that is ready to be passed to BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  sentence 1 you are charged as follows first co...      0\n",
      "1  sentence 1 josefa kotobalavu you were charged ...      1\n",
      "2  sentence 1 the director of public prosecution ...      1\n",
      "3  sentence 1 mohommed nabi ud dean you were conv...      1\n",
      "4  judgment of the court background 1 the appella...      0\n",
      "\n",
      "\n",
      "0    [101, 5650, 122, 1128, 1132, 4601, 1112, 3226,...\n",
      "1    [101, 5650, 122, 179, 6787, 8057, 180, 12355, ...\n",
      "2    [101, 5650, 122, 1103, 1900, 1104, 1470, 12369...\n",
      "3    [101, 5650, 122, 182, 10559, 4165, 4611, 9468,...\n",
      "4    [101, 9228, 1104, 1103, 2175, 3582, 122, 1103,...\n",
      "Name: text, dtype: object\n",
      "\n",
      "\n",
      "(806,)\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "tokenized = df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512)))\n",
    "# This turns every sentence into a list of IDs\n",
    "# tokenized is apandas Series object: <class 'pandas.core.series.Series'>\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "print(tokenized.head())\n",
    "print('\\n')\n",
    "print(tokenized.shape) #(6920,) a 1D pandas Series\n",
    "print(type(tokenized)) #<class 'pandas.core.series.Series'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized.values.shape: (806,)\n",
      "max sentence length is : 512\n",
      "padded.shape: (806, 512)\n"
     ]
    }
   ],
   "source": [
    "print(f'tokenized.values.shape: {tokenized.values.shape}')\n",
    "# find the length of the longest sentence in the dataset\n",
    "max_len = 0\n",
    "for i in tokenized.values:  #tokenized.values is of type #<class 'numpy.ndarray'>\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "print(f'max sentence length is : {max_len}')\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "print(f'padded.shape: {padded.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 5650, 122, 1128, 1132, 4601, 1112, 3226, 1148, 5099, 4195, 1104, 16273, 9372, 11565, 1106, 4886, 18513, 1105, 4214, 1104, 1103, 8228, 1348, 3463, 6707, 1542, 2440, 1116, 1104, 16273, 14516, 16996, 20991, 3687, 1161, 1206, 1103, 141, 13821, 2036, 1106, 141, 13821, 2036, 1120, 9468, 6738, 6851, 1491, 191, 2980, 7563, 6094, 1742, 27629, 25247, 1161, 1107, 1103, 2466, 2417, 1125, 26079, 1610, 7050, 3044, 1104, 183, 1179, 1443, 1123, 9635, 1248, 5099, 4195, 1104, 16273, 9372, 11565, 1106, 2237, 21606, 122, 1105, 123, 170, 1104, 1103, 6969, 11903, 3140, 1104, 1371, 2440, 1116, 1104, 16273, 14516, 16996, 20991, 3687, 1161, 1206, 1103, 141, 13821, 2036, 1106, 1103, 141, 13821, 2036, 1120, 9468, 6738, 6851, 1491, 191, 2980, 7563, 6094, 1742, 27629, 25247, 1161, 1107, 1103, 2466, 2417, 23937, 1103, 191, 27547, 1605, 1104, 183, 1179, 1114, 1117, 21504, 1443, 1123, 9635, 122, 1113, 141, 13821, 2036, 1128, 12404, 5425, 1106, 1241, 4917, 1222, 1128, 1105, 4120, 1103, 14940, 1104, 9193, 1113, 141, 13821, 2036, 123, 1103, 14940, 1104, 9193, 7402, 1118, 1103, 1352, 12654, 2231, 1112, 3226, 1103, 4806, 14516, 16996, 20991, 3687, 1161, 1110, 3383, 1201, 1104, 9468, 6738, 6851, 1491, 191, 2980, 7563, 6094, 1742, 27629, 25247, 1161, 1103, 19073, 2861, 1107, 1142, 1692, 1110, 1141, 183, 1179, 1275, 1201, 1104, 1103, 1269, 1491, 1103, 19073, 2861, 1108, 1275, 1201, 1385, 15113, 1107, 9468, 6738, 6851, 5855, 14084, 1596, 1278, 1165, 1103, 1148, 4497, 1261, 1282, 1131, 1180, 9148, 1115, 1107, 1103, 1214, 1384, 1219, 1103, 1248, 1858, 1278, 1105, 1107, 2440, 1113, 1103, 1285, 1131, 1225, 1136, 1301, 1106, 1278, 1103, 19073, 2861, 1108, 1500, 1118, 1123, 1534, 1106, 1301, 1105, 1138, 1123, 10919, 1107, 1103, 2186, 1397, 1106, 1103, 1491, 1131, 1286, 1123, 1313, 2041, 1106, 1138, 1123, 10919, 1105, 1113, 1103, 1236, 1171, 1131, 1899, 1103, 4806, 2807, 1397, 1106, 1103, 2186, 3085, 1103, 4806, 4685, 1103, 19073, 2861, 1105, 1455, 1123, 1106, 12910, 1123, 1106, 1103, 176, 6718, 2497, 10085, 1106, 7822, 176, 6718, 11509, 1103, 19073, 2861, 9373, 1103, 4806, 1105, 1723, 1140, 2840, 1103, 4806, 1500, 1123, 1106, 1831, 1105, 4277, 1205, 1113, 1103, 1747, 1119, 1173, 4685, 1123, 1106, 5782, 1123, 3459, 1105, 6926, 1123, 1191, 1131, 2144, 1204, 1173, 1119, 1209, 3222, 1123, 1114, 170, 176, 6718, 2497, 6166, 1103, 19073, 2861, 1108, 5528, 1165, 1131, 1767, 1142, 1105, 1723, 1103, 4806, 1116, 7953, 1165, 1103, 19073, 2861, 1400, 5576, 18089, 1119, 1486, 1103, 4806, 19765, 1205, 1105, 1606, 1117, 3661, 1106, 19676, 1123, 191, 27547, 1605, 1103, 4806, 1173, 8362, 5303, 10438, 1117, 224, 6023, 1105, 1173, 13137, 1117, 6517, 21504, 1154, 1103, 19073, 7418, 191, 27547, 1605, 1131, 1108, 1107, 1632, 2489, 1105, 1408, 6675, 1131, 1108, 14120, 1112, 1103, 4806, 1113, 1499, 1104, 1123, 1131, 1180, 1631, 1115, 1123, 191, 27547, 1605, 1108, 4375, 1165, 1103, 4806, 2788, 1115, 1103, 19073, 2861, 1108, 6675, 1119, 6189, 1123, 1106, 1243, 1146, 1243, 4462, 1105, 1136, 1106, 12862, 2256, 1164, 1142, 4497, 1165, 1103, 19073, 2861, 1108, 3179, 1106, 1103, 2186, 1131, 1180, 1631, 8949, 1104, 1892, 102]\n",
      "\n",
      "\n",
      "[  101  5650   122  1128  1132  4601  1112  3226  1148  5099  4195  1104\n",
      " 16273  9372 11565  1106  4886 18513  1105  4214  1104  1103  8228  1348\n",
      "  3463  6707  1542  2440  1116  1104 16273 14516 16996 20991  3687  1161\n",
      "  1206  1103   141 13821  2036  1106   141 13821  2036  1120  9468  6738\n",
      "  6851  1491   191  2980  7563  6094  1742 27629 25247  1161  1107  1103\n",
      "  2466  2417  1125 26079  1610  7050  3044  1104   183  1179  1443  1123\n",
      "  9635  1248  5099  4195  1104 16273  9372 11565  1106  2237 21606   122\n",
      "  1105   123   170  1104  1103  6969 11903  3140  1104  1371  2440  1116\n",
      "  1104 16273 14516 16996 20991  3687  1161  1206  1103   141 13821  2036\n",
      "  1106  1103   141 13821  2036  1120  9468  6738  6851  1491   191  2980\n",
      "  7563  6094  1742 27629 25247  1161  1107  1103  2466  2417 23937  1103\n",
      "   191 27547  1605  1104   183  1179  1114  1117 21504  1443  1123  9635\n",
      "   122  1113   141 13821  2036  1128 12404  5425  1106  1241  4917  1222\n",
      "  1128  1105  4120  1103 14940  1104  9193  1113   141 13821  2036   123\n",
      "  1103 14940  1104  9193  7402  1118  1103  1352 12654  2231  1112  3226\n",
      "  1103  4806 14516 16996 20991  3687  1161  1110  3383  1201  1104  9468\n",
      "  6738  6851  1491   191  2980  7563  6094  1742 27629 25247  1161  1103\n",
      " 19073  2861  1107  1142  1692  1110  1141   183  1179  1275  1201  1104\n",
      "  1103  1269  1491  1103 19073  2861  1108  1275  1201  1385 15113  1107\n",
      "  9468  6738  6851  5855 14084  1596  1278  1165  1103  1148  4497  1261\n",
      "  1282  1131  1180  9148  1115  1107  1103  1214  1384  1219  1103  1248\n",
      "  1858  1278  1105  1107  2440  1113  1103  1285  1131  1225  1136  1301\n",
      "  1106  1278  1103 19073  2861  1108  1500  1118  1123  1534  1106  1301\n",
      "  1105  1138  1123 10919  1107  1103  2186  1397  1106  1103  1491  1131\n",
      "  1286  1123  1313  2041  1106  1138  1123 10919  1105  1113  1103  1236\n",
      "  1171  1131  1899  1103  4806  2807  1397  1106  1103  2186  3085  1103\n",
      "  4806  4685  1103 19073  2861  1105  1455  1123  1106 12910  1123  1106\n",
      "  1103   176  6718  2497 10085  1106  7822   176  6718 11509  1103 19073\n",
      "  2861  9373  1103  4806  1105  1723  1140  2840  1103  4806  1500  1123\n",
      "  1106  1831  1105  4277  1205  1113  1103  1747  1119  1173  4685  1123\n",
      "  1106  5782  1123  3459  1105  6926  1123  1191  1131  2144  1204  1173\n",
      "  1119  1209  3222  1123  1114   170   176  6718  2497  6166  1103 19073\n",
      "  2861  1108  5528  1165  1131  1767  1142  1105  1723  1103  4806  1116\n",
      "  7953  1165  1103 19073  2861  1400  5576 18089  1119  1486  1103  4806\n",
      " 19765  1205  1105  1606  1117  3661  1106 19676  1123   191 27547  1605\n",
      "  1103  4806  1173  8362  5303 10438  1117   224  6023  1105  1173 13137\n",
      "  1117  6517 21504  1154  1103 19073  7418   191 27547  1605  1131  1108\n",
      "  1107  1632  2489  1105  1408  6675  1131  1108 14120  1112  1103  4806\n",
      "  1113  1499  1104  1123  1131  1180  1631  1115  1123   191 27547  1605\n",
      "  1108  4375  1165  1103  4806  2788  1115  1103 19073  2861  1108  6675\n",
      "  1119  6189  1123  1106  1243  1146  1243  4462  1105  1136  1106 12862\n",
      "  2256  1164  1142  4497  1165  1103 19073  2861  1108  3179  1106  1103\n",
      "  2186  1131  1180  1631  8949  1104  1892   102]\n",
      "\n",
      "len(padded[0]): 512\n"
     ]
    }
   ],
   "source": [
    "print(tokenized[0])\n",
    "print('\\n')\n",
    "print(padded[0])\n",
    "print(f'\\nlen(padded[0]): {len(padded[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking - If we directly send padded to BERT, that would slightly confuse it. We need to create another\n",
    "variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what\n",
    "attention_mask is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(806, 512)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask[0])\n",
    "print(attention_mask[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting from numpy.ndarray to pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model Runtime: 0:19:38.247636\n"
     ]
    }
   ],
   "source": [
    "#create a tensor for the attention_mask\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "#We now create an input tensor out of the padded token matrix, and send that to DistilBERT\n",
    "input_ids = torch.tensor(padded) \n",
    "#the model() function runs our sentences through BERT. The results of the processing will be returned into last_hidden_states.\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "with torch.no_grad(): #deactivates autograd engine\n",
    "    last_hidden_states = model1(input_ids, attention_mask=attention_mask) #transformers.modeling_distilbert.DistilBertModel\n",
    "    # we could also simply do this\n",
    "    #last_hidden_states = model1(input_ids)\n",
    "print(f'BERT Model Runtime: {datetime.datetime.now() - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this step, last_hidden_states holds the outputs of DistilBERT. It is a tuple with the shape\n",
    "(number of examples, max number of tokens in the sequence, number of hidden units in the DistilBERT model).\n",
    "In our case, this will be 806 , 512 (which is the number of tokens in the longest sequence from the 806 examples),\n",
    "768 (the number of hidden units in the DistilBERT model). T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([806, 512, 768])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(last_hidden_states) # tuple - the model output is a tuple\n",
    "type(last_hidden_states[0]) # torch.Tensor - First element of that tuple is the output tensor \n",
    "last_hidden_states[0].shape # 806 X 512 X 768 cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence.\n",
    "The way BERT does sentence classification, is that it adds a token called [CLS] (for classification) at the beginning\n",
    "of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
    "so we select that slice of the cube and discard everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(806, 768)\n"
     ]
    }
   ],
   "source": [
    "#cube slicing\n",
    "# [:,0,:] means we need all rows,column 0 (first output - for the CLS token),all depth of the cube (all hidden units)\n",
    "#also convert back from torch tensors to numpy because sklearn works with numpy.ndarray and not tensors\n",
    "features = last_hidden_states[0][:,0,:].numpy() # features for the logistic regression model - look at the image\n",
    "\n",
    "print(type(features))\n",
    "print(features.shape) # a slice of the cube depth (3D) is 2D - (806, 768)\n",
    "# we have 806  sentences and so we will have 806 sentence embeddings\n",
    "# each sentence embedding will have the size of the hidden units for each token in the last layer of BERT, which is 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "806\n"
     ]
    }
   ],
   "source": [
    "labels = df['label']\n",
    "\n",
    "print(type(labels))\n",
    "print(len(labels)) #806 labels as expected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL # 2 - train/test split\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=42, shuffle=True) # split is 20% test and 80% train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features.shape: (644, 768)\n",
      "test_features.shape: (162, 768)\n",
      "train_labels.shape: (644,)\n",
      "test_labels.shape: (162,)\n"
     ]
    }
   ],
   "source": [
    "type(train_features) #numpy.ndarray\n",
    "print(f'train_features.shape: {train_features.shape}') # 75%\n",
    "print(f'test_features.shape: {test_features.shape}') # 25%\n",
    "print(f'train_labels.shape: {train_labels.shape}')\n",
    "print(f'test_labels.shape: {test_labels.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Model  - sklearn Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'C': array([1.00000000e-04, 5.26325263e+00, 1.05264053e+01, 1.57895579e+01,\n",
      "       2.10527105e+01, 2.63158632e+01, 3.15790158e+01, 3.68421684e+01,\n",
      "       4.21053211e+01, 4.73684737e+01, 5.26316263e+01, 5.78947789e+01,\n",
      "       6.31579316e+01, 6.84210842e+01, 7.36842368e+01, 7.89473895e+01,\n",
      "       8.42105421e+01, 8.94736947e+01, 9.47368474e+01, 1.00000000e+02])}\n",
      "Grid Search time: 0:00:03.620024\n",
      "best parameters:  {'C': 5.263252631578947}\n",
      "best scrores:  0.5978561046511628\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'C': np.linspace(0.0001, 100, 20)} #try 20 values between 1e-4 and 1e+2\n",
    "print(f'parameters: {parameters}')\n",
    "grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "\n",
    "begin_time = datetime.datetime.now()\n",
    "grid_search.fit(train_features, train_labels)\n",
    "print(f'Grid Search time: {datetime.datetime.now() - begin_time}')\n",
    "\n",
    "print('best parameters: ', grid_search.best_params_)\n",
    "print('best scrores: ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train a sklearn model with the best parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Runtime: 0:00:00.046943\n"
     ]
    }
   ],
   "source": [
    "# train a logistic regression model\n",
    "lr_clf = LogisticRegression(C=grid_search.best_params_['C'])\n",
    "start_time = datetime.datetime.now()\n",
    "lr_clf.fit(train_features, train_labels)\n",
    "print(f'Logistic Regression Model Runtime: {datetime.datetime.now() - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6172839506172839"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate MODEL #2\n",
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier score: 0.511 (+/- 0.08)\n"
     ]
    }
   ],
   "source": [
    "#How good is this score? What can we compare it against? Let's first look at a dummy classifier:\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier()\n",
    "\n",
    "scores = cross_val_score(clf, train_features, train_labels)\n",
    "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266",
   "language": "python",
   "name": "w266"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
