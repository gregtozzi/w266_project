{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import ktrain\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_cut(x, max_len):\n",
    "    \"\"\"\n",
    "    Either pads or cuts an document's embedding matrix.\n",
    "    \"\"\"\n",
    "    # Cut to the maximum length; cheaper than testing\n",
    "    x = x[0:max_len,:]\n",
    "    \n",
    "    # Pad with zeros\n",
    "    len_diff = max_len - len(x)\n",
    "    if len_diff > 0:\n",
    "        x = np.concatenate((x, np.zeros((len_diff, x.shape[1]))))\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedded_text(data_path, max_len='max',\n",
    "                        val_split=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Returns training, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        len -> either str or int.  If str, must be 'max'.\n",
    "               'max' indicates that the documents should be\n",
    "               padded to the max document length in the training\n",
    "               set.\n",
    "        val_split -> either float or bool.  If bool, must be\n",
    "                     False. If False, there is no true train/val\n",
    "                     split.  Rather, the test set is returned as\n",
    "                     the val set for consistency.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    fname = data_path\n",
    "    with open(fname, 'rb') as fp:\n",
    "        df = pickle.load(fp)\n",
    "    \n",
    "    # Break into train and test\n",
    "    train_mask = df.doc_use == 'train'\n",
    "    train = df[train_mask]\n",
    "    test = df[~train_mask]\n",
    "    \n",
    "    # Stack documents\n",
    "    train_embeddings = [np.stack(train.embeddings[train.docid == ID]) for ID in train.docid.unique()]\n",
    "    test_embeddings = [np.stack(test.embeddings[test.docid == ID]) for ID in test.docid.unique()]\n",
    "    \n",
    "    # Pad documents\n",
    "    if max_len == 'max':\n",
    "        max_len = max([len(doc) for doc in train_embeddings])\n",
    "    \n",
    "    x_train = [pad_or_cut(doc, max_len) for doc in train_embeddings]\n",
    "    x_train = np.stack(x_train)#.transpose(0, 2, 1)\n",
    "    \n",
    "    x_test = [pad_or_cut(doc, max_len) for doc in test_embeddings]\n",
    "    x_test = np.stack(x_test)#.transpose(0, 2, 1)\n",
    "    \n",
    "    y_train = train.groupby('docid').first()['label'].values\n",
    "    y_test = test.groupby('docid').first()['label'].values\n",
    "    \n",
    "    # Build a validation set from the training set\n",
    "    if val_split is False:\n",
    "        return x_train, y_train, x_test, y_test, x_test, y_test\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_split,\n",
    "                                                      random_state=random_state)\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_train, y_train,\n",
    "                   x_val, y_val,\n",
    "                   x_test, y_test,\n",
    "                   optimizer, loss, metrics,\n",
    "                   lr, eval_lr=False):\n",
    "    \"\"\"\n",
    "    Runs a model evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=metrics)\n",
    "    \n",
    "    learner = ktrain.get_learner(model,\n",
    "                                train_data=(x_train, y_train),\n",
    "                                val_data=(x_val, y_val))\n",
    "    learner.reset_weights()\n",
    "    \n",
    "    if eval_lr:\n",
    "        learner.lr_find(show_plot=True)\n",
    "        plt.show()\n",
    "    \n",
    "    learner.autofit(lr, reduce_on_plateau=5, early_stopping=10)\n",
    "    \n",
    "    y_hat = learner.model.predict(x_test).flatten() > 0.5\n",
    "    \n",
    "    print(classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val, x_test, y_test = build_embedded_text('../data/baseBert_embeddings_olap_200.pkl', val_split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.keras.layers.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "conv1 = tf.keras.layers.Conv1D(filters=32,\n",
    "                               kernel_size=1,\n",
    "                               activation='relu')(embeddings)\n",
    "maxpool1 = tf.keras.layers.MaxPool1D(pool_size=x_train.shape[1])(conv1)\n",
    "conv2 = tf.keras.layers.Conv1D(filters=32,\n",
    "                               kernel_size=2,\n",
    "                               activation='relu')(embeddings)\n",
    "maxpool2 = tf.keras.layers.MaxPool1D(pool_size=x_train.shape[1]-1)(conv2)\n",
    "\n",
    "conv3 = tf.keras.layers.Conv1D(filters=32,\n",
    "                               kernel_size=3,\n",
    "                               activation='relu')(embeddings)\n",
    "maxpool3 = tf.keras.layers.MaxPool1D(pool_size=x_train.shape[1]-2)(conv3)\n",
    "\n",
    "conv4 = tf.keras.layers.Conv1D(filters=32,\n",
    "                               kernel_size=4,\n",
    "                               activation='relu')(embeddings)\n",
    "maxpool4 = tf.keras.layers.MaxPool1D(pool_size=x_train.shape[1]-3)(conv4)\n",
    "\n",
    "conv5 = tf.keras.layers.Conv1D(filters=32,\n",
    "                               kernel_size=5,\n",
    "                               activation='relu')(embeddings)\n",
    "maxpool5 = tf.keras.layers.MaxPool1D(pool_size=x_train.shape[1]-4)(conv5)\n",
    "\n",
    "concat = tf.keras.layers.concatenate([maxpool1, maxpool2, maxpool3, maxpool4, maxpool5])\n",
    "\n",
    "dense1 = tf.keras.layers.Dense(64, activation='relu')(concat)\n",
    "dropout = tf.keras.layers.Dropout(0.1)(dense1)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-59600d54b7b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs=embeddings, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 109, 768)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 109, 32)      24608       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 108, 32)      49184       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 107, 32)      73760       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 106, 32)      98336       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 105, 32)      122912      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 32)        0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 32)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 32)        0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1, 32)        0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 1, 32)        0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1, 160)       0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1, 64)        10304       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1, 64)        0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 1)         65          dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 379,169\n",
      "Trainable params: 379,169\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights have been reset.\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 1e-05...\n",
      "Train on 641 samples, validate on 162 samples\n",
      "Epoch 1/1024\n",
      "641/641 [==============================] - 3s 5ms/sample - loss: 0.8953 - acc: 0.4197 - val_loss: 0.8082 - val_acc: 0.4012\n",
      "Epoch 2/1024\n",
      "641/641 [==============================] - 0s 588us/sample - loss: 0.7777 - acc: 0.4321 - val_loss: 0.7229 - val_acc: 0.4012\n",
      "Epoch 3/1024\n",
      "641/641 [==============================] - 0s 559us/sample - loss: 0.7046 - acc: 0.5023 - val_loss: 0.6862 - val_acc: 0.5309\n",
      "Epoch 4/1024\n",
      "641/641 [==============================] - 0s 556us/sample - loss: 0.6993 - acc: 0.5335 - val_loss: 0.6752 - val_acc: 0.6111\n",
      "Epoch 5/1024\n",
      "641/641 [==============================] - 0s 547us/sample - loss: 0.6920 - acc: 0.5429 - val_loss: 0.6725 - val_acc: 0.5988\n",
      "Epoch 6/1024\n",
      "641/641 [==============================] - 0s 538us/sample - loss: 0.6971 - acc: 0.5616 - val_loss: 0.6718 - val_acc: 0.5988\n",
      "Epoch 7/1024\n",
      "641/641 [==============================] - 0s 524us/sample - loss: 0.6874 - acc: 0.5897 - val_loss: 0.6725 - val_acc: 0.5988\n",
      "Epoch 8/1024\n",
      "641/641 [==============================] - 0s 537us/sample - loss: 0.6925 - acc: 0.5679 - val_loss: 0.6723 - val_acc: 0.5988\n",
      "Epoch 9/1024\n",
      "641/641 [==============================] - 0s 566us/sample - loss: 0.6884 - acc: 0.5710 - val_loss: 0.6733 - val_acc: 0.5988\n",
      "Epoch 10/1024\n",
      "641/641 [==============================] - 0s 540us/sample - loss: 0.6944 - acc: 0.5476 - val_loss: 0.6713 - val_acc: 0.5988\n",
      "Epoch 11/1024\n",
      "641/641 [==============================] - 0s 554us/sample - loss: 0.6910 - acc: 0.5757 - val_loss: 0.6709 - val_acc: 0.5988\n",
      "Epoch 12/1024\n",
      "641/641 [==============================] - 0s 582us/sample - loss: 0.6860 - acc: 0.5663 - val_loss: 0.6709 - val_acc: 0.5988\n",
      "Epoch 13/1024\n",
      "641/641 [==============================] - 0s 526us/sample - loss: 0.6992 - acc: 0.5460 - val_loss: 0.6711 - val_acc: 0.5988\n",
      "Epoch 14/1024\n",
      "641/641 [==============================] - 0s 543us/sample - loss: 0.7030 - acc: 0.5538 - val_loss: 0.6707 - val_acc: 0.5988\n",
      "Epoch 15/1024\n",
      "641/641 [==============================] - 0s 539us/sample - loss: 0.6891 - acc: 0.5663 - val_loss: 0.6706 - val_acc: 0.5988\n",
      "Epoch 16/1024\n",
      "641/641 [==============================] - 0s 541us/sample - loss: 0.6942 - acc: 0.5538 - val_loss: 0.6705 - val_acc: 0.5988\n",
      "Epoch 17/1024\n",
      "641/641 [==============================] - 0s 559us/sample - loss: 0.6843 - acc: 0.5616 - val_loss: 0.6705 - val_acc: 0.5988\n",
      "Epoch 18/1024\n",
      "641/641 [==============================] - 0s 555us/sample - loss: 0.6768 - acc: 0.5850 - val_loss: 0.6705 - val_acc: 0.5988\n",
      "Epoch 19/1024\n",
      "641/641 [==============================] - 0s 531us/sample - loss: 0.6882 - acc: 0.5741 - val_loss: 0.6712 - val_acc: 0.5988\n",
      "Epoch 20/1024\n",
      "641/641 [==============================] - 0s 540us/sample - loss: 0.6905 - acc: 0.5616 - val_loss: 0.6710 - val_acc: 0.5988\n",
      "Epoch 21/1024\n",
      "641/641 [==============================] - 0s 520us/sample - loss: 0.6810 - acc: 0.5788 - val_loss: 0.6721 - val_acc: 0.5988\n",
      "Epoch 22/1024\n",
      "641/641 [==============================] - 0s 542us/sample - loss: 0.6836 - acc: 0.5881 - val_loss: 0.6702 - val_acc: 0.5988\n",
      "Epoch 23/1024\n",
      "641/641 [==============================] - 0s 545us/sample - loss: 0.6854 - acc: 0.5850 - val_loss: 0.6702 - val_acc: 0.5988\n",
      "Epoch 24/1024\n",
      "641/641 [==============================] - 0s 560us/sample - loss: 0.6806 - acc: 0.5632 - val_loss: 0.6710 - val_acc: 0.5988\n",
      "Epoch 25/1024\n",
      "641/641 [==============================] - 0s 567us/sample - loss: 0.6842 - acc: 0.5725 - val_loss: 0.6729 - val_acc: 0.5926\n",
      "Epoch 26/1024\n",
      "641/641 [==============================] - 0s 531us/sample - loss: 0.6935 - acc: 0.5585 - val_loss: 0.6722 - val_acc: 0.5988\n",
      "Epoch 27/1024\n",
      "641/641 [==============================] - 0s 532us/sample - loss: 0.6829 - acc: 0.5647 - val_loss: 0.6714 - val_acc: 0.5988\n",
      "Epoch 28/1024\n",
      "641/641 [==============================] - 0s 557us/sample - loss: 0.6860 - acc: 0.5694 - val_loss: 0.6701 - val_acc: 0.5988\n",
      "Epoch 29/1024\n",
      "641/641 [==============================] - 0s 537us/sample - loss: 0.6826 - acc: 0.5694 - val_loss: 0.6717 - val_acc: 0.5988\n",
      "Epoch 30/1024\n",
      "641/641 [==============================] - 0s 537us/sample - loss: 0.6832 - acc: 0.5491 - val_loss: 0.6728 - val_acc: 0.6049\n",
      "Epoch 31/1024\n",
      "641/641 [==============================] - 0s 562us/sample - loss: 0.6937 - acc: 0.5476 - val_loss: 0.6702 - val_acc: 0.5988\n",
      "Epoch 32/1024\n",
      "641/641 [==============================] - 0s 563us/sample - loss: 0.6874 - acc: 0.5569 - val_loss: 0.6697 - val_acc: 0.5988\n",
      "Epoch 33/1024\n",
      "641/641 [==============================] - 0s 523us/sample - loss: 0.6834 - acc: 0.5897 - val_loss: 0.6699 - val_acc: 0.5988\n",
      "Epoch 34/1024\n",
      "641/641 [==============================] - 0s 527us/sample - loss: 0.6866 - acc: 0.5741 - val_loss: 0.6693 - val_acc: 0.5988\n",
      "Epoch 35/1024\n",
      "641/641 [==============================] - 0s 526us/sample - loss: 0.6905 - acc: 0.5569 - val_loss: 0.6714 - val_acc: 0.5988\n",
      "Epoch 36/1024\n",
      "641/641 [==============================] - 0s 541us/sample - loss: 0.6763 - acc: 0.5741 - val_loss: 0.6739 - val_acc: 0.6111\n",
      "Epoch 37/1024\n",
      "641/641 [==============================] - 0s 539us/sample - loss: 0.6824 - acc: 0.5647 - val_loss: 0.6706 - val_acc: 0.5988\n",
      "Epoch 38/1024\n",
      "641/641 [==============================] - 0s 537us/sample - loss: 0.6854 - acc: 0.5569 - val_loss: 0.6717 - val_acc: 0.6049\n",
      "Epoch 39/1024\n",
      "544/641 [========================>.....] - ETA: 0s - loss: 0.6813 - acc: 0.5625\n",
      "Epoch 00039: Reducing Max LR on Plateau: new max lr will be 5e-06 (if not early_stopping).\n",
      "641/641 [==============================] - 0s 519us/sample - loss: 0.6804 - acc: 0.5663 - val_loss: 0.6703 - val_acc: 0.5988\n",
      "Epoch 40/1024\n",
      "641/641 [==============================] - 0s 582us/sample - loss: 0.6757 - acc: 0.5913 - val_loss: 0.6701 - val_acc: 0.5988\n",
      "Epoch 41/1024\n",
      "641/641 [==============================] - 0s 532us/sample - loss: 0.6749 - acc: 0.5757 - val_loss: 0.6701 - val_acc: 0.5988\n",
      "Epoch 42/1024\n",
      "641/641 [==============================] - 0s 534us/sample - loss: 0.6762 - acc: 0.5788 - val_loss: 0.6700 - val_acc: 0.5988\n",
      "Epoch 43/1024\n",
      "641/641 [==============================] - 0s 531us/sample - loss: 0.6789 - acc: 0.5710 - val_loss: 0.6701 - val_acc: 0.5988\n",
      "Epoch 44/1024\n",
      "544/641 [========================>.....] - ETA: 0s - loss: 0.6792 - acc: 0.5699\n",
      "Epoch 00044: Reducing Max LR on Plateau: new max lr will be 2.5e-06 (if not early_stopping).\n",
      "Restoring model weights from the end of the best epoch.\n",
      "641/641 [==============================] - 0s 545us/sample - loss: 0.6806 - acc: 0.5710 - val_loss: 0.6704 - val_acc: 0.5988\n",
      "Epoch 00044: early stopping\n",
      "Weights from best epoch have been loaded into model.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        65\n",
      "           1       0.60      1.00      0.75        97\n",
      "\n",
      "    accuracy                           0.60       162\n",
      "   macro avg       0.30      0.50      0.37       162\n",
      "weighted avg       0.36      0.60      0.45       162\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, x_train, y_train, x_val, y_val, x_test, y_test,\n",
    "               optimizer='adam', loss='binary_crossentropy', metrics=['acc'],\n",
    "               lr=10e-6, eval_lr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
