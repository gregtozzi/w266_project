{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Graphing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch Imports\n",
    "import torch # a tensor library\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#Huggingface Transformers\n",
    "import transformers\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "from transformers import LongformerForSequenceClassification, AdamW, BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Set Up\n",
    "Set up the model and the configuration we need\n",
    "\n",
    "- max_len - how many tokens will be used from the document.\n",
    "- batch_size - reduce if memory issues. paper reccomends 16-32\n",
    "- num epochs\n",
    "\n",
    "- tokenizer - this should be a pretrined tokenizer, e.g. distilBert. \n",
    "- model - make sure it uses the same tokenizer for generating the weights\n",
    "\n",
    "#### input files\n",
    "- train.csv: heading removed, dates and URL replcaed, un-cased, sentence breaks and punctuation included  \n",
    "- train_lcase.csv: heading removed, dates and URL replcaed, lower cased, sentence breaks and punctuation removed\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_file = \"data/train.csv\"\n",
    "test_file = \"data/test.csv\"\n",
    "\n",
    "max_len = 512\n",
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096',\n",
    "                                                       num_labels = 2,\n",
    "                                                       output_attentions = False,\n",
    "                                                       output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get data\n",
    "def get_data(fname):\n",
    "    df = pd.read_csv(fname)\n",
    "    df = df[['cleaned_contents', 'Discrimination_Label']]\n",
    "    df = df.rename(columns = {'cleaned_contents':'text', 'Discrimination_Label':'label'})\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format elapsed time \n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize data and return tensors for input ids, attention mask and labels\n",
    "def tokenize_plus(df):\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    sentences = df['text'].values\n",
    "    labels = df['label'].values\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in sentences:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_len,           # Pad & truncate all sentences.\n",
    "                            truncation = True,\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SENTENCE\\n\\n\\t1.\\tYou are charged as follows:\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SENTENCE\\n\\n\\t1.\\tJOSEFA KOTOBALAVU, you were ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SENTENCE\\n\\n1. The Director of Public Prosecut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SENTENCE\\n\\n\\t1.\\tMOHOMMED NABI UD- DEAN, you ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JUDGMENT OF THE COURT\\n\\nBackground\\n\\n[1] The...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  SENTENCE\\n\\n\\t1.\\tYou are charged as follows:\\...      0\n",
       "1  SENTENCE\\n\\n\\t1.\\tJOSEFA KOTOBALAVU, you were ...      1\n",
       "2  SENTENCE\\n\\n1. The Director of Public Prosecut...      1\n",
       "3  SENTENCE\\n\\n\\t1.\\tMOHOMMED NABI UD- DEAN, you ...      1\n",
       "4  JUDGMENT OF THE COURT\\n\\nBackground\\n\\n[1] The...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_data(train_file)\n",
    "#df = df.sample(n = 10) # limit size for testing \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize test data using tokenizer.encode_plus, put into a pytorch dataloader\n",
    "input_ids, attention_masks, labels = tokenize_plus(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "# check size of embeddings\n",
    "print(len(input_ids))\n",
    "print(len(input_ids[0]))\n",
    "\n",
    "# Use torchsummary to print out layers nicely, needs the imput shape\n",
    "#p!ip install torchsummary\n",
    "#summary(your_model, input_size=(channels, H, W))\n",
    "#from torchsummary import summary\n",
    "#summary(model, input_size=(8, 647, 512))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  582 training samples\n",
      "   65 validation samples\n"
     ]
    }
   ],
   "source": [
    "# Create a 90-10 train-validation split.\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoaders for our training and validation sets.\n",
    "# Take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongformerForSequenceClassification(\n",
       "  (longformer): LongformerModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): LongformerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): LongformerClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = num_epochs\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of     73.    Elapsed: 0:01:04.\n",
      "\n",
      "  Average training loss: 0.67\n",
      "  Training epcoh took: 0:01:58\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.76\n",
      "  Validation Loss: 0.57\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of     73.    Elapsed: 0:01:06.\n",
      "\n",
      "  Average training loss: 0.65\n",
      "  Training epcoh took: 0:02:01\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.76\n",
      "  Validation Loss: 0.55\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of     73.    Elapsed: 0:01:07.\n",
      "\n",
      "  Average training loss: 0.62\n",
      "  Training epcoh took: 0:02:03\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.79\n",
      "  Validation Loss: 0.53\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:06:13 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0:01:58</td>\n",
       "      <td>0:00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0:02:01</td>\n",
       "      <td>0:00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0:02:03</td>\n",
       "      <td>0:00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.67         0.57           0.76       0:01:58         0:00:04\n",
       "2               0.65         0.55           0.76       0:02:01         0:00:04\n",
       "3               0.62         0.53           0.79       0:02:03         0:00:04"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SENTENCE\\n\\n[Name of the victim is suppressed....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JUDGMENT\\n\\nThis is an appeal against convicti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JUDGMENT\\n\\n1. On 13 May 2008, the Appellant, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JUDGMENT\\n[1] On the 17th July 2012 in the Nas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SENTENCE\\n\\nBackground \\n\\n1. The accused was ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  SENTENCE\\n\\n[Name of the victim is suppressed....      1\n",
       "1  JUDGMENT\\n\\nThis is an appeal against convicti...      1\n",
       "2  JUDGMENT\\n\\n1. On 13 May 2008, the Appellant, ...      1\n",
       "3  JUDGMENT\\n[1] On the 17th July 2012 in the Nas...      0\n",
       "4  SENTENCE\\n\\nBackground \\n\\n1. The accused was ...      1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Holdoit\n",
    "df = get_data(test_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize test data using tokenizer.encode_plus, put into a pytorch dataloader\n",
    "input_ids, attention_masks, labels = tokenize_plus(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader from input tensors, to help with memory usage\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 162 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 97 of 162 (59.88%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, Recall, F1, Support: (0.7090909090909091, 0.8041237113402062, 0.753623188405797, None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "scores = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='binary')\n",
    "\n",
    "# Calculate the P,R,F1\n",
    "print('Precision, Recall, F1, Support:', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of good predictions: 111\n",
      "number of bad predictions: 51\n",
      "accuracy 0.6851851851851852\n"
     ]
    }
   ],
   "source": [
    "# get the bad predictions index\n",
    "bad_preds = [idx for idx, elem in enumerate(flat_predictions) if elem != flat_true_labels[idx]] \n",
    "good_preds = [idx for idx, elem in enumerate(flat_predictions) if elem == flat_true_labels[idx]] \n",
    "print(\"number of good predictions:\", len(good_preds))\n",
    "print(\"number of bad predictions:\", len(bad_preds))\n",
    "print(\"accuracy\", len(good_preds)/len(flat_predictions) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1\n",
      " 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 0\n",
      " 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1\n",
      " 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0\n",
      " 0 0 1 0 0 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(flat_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_good=df.iloc[good_preds]\n",
    "df_bad = df.iloc[bad_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
