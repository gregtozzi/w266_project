{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TWWNW1Kk1jMk"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TFUEX6PB10dD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "X43U543r10g8",
    "outputId": "33531033-8b39-4d15-9818-f9eb482c6241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
      "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.0rc4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jGyZM63bBELo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "import torch # a tensor library\n",
    "# the huggingface transformers library (pre-trained deep learning for NLP models)\n",
    "# run !pip install transformers in a Jupyter Notebook cell\n",
    "import transformers as ppb \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "JwcSQCXFdRtk",
    "outputId": "8649633a-3325-4f11-dc69-e829d15564da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence 1 you are charged as follows first co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence 1 josefa kotobalavu you were charged ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence 1 the director of public prosecution ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence 1 mohommed nabi ud dean you were conv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>judgment of the court background 1 the appella...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  sentence 1 you are charged as follows first co...      0\n",
       "1  sentence 1 josefa kotobalavu you were charged ...      1\n",
       "2  sentence 1 the director of public prosecution ...      1\n",
       "3  sentence 1 mohommed nabi ud dean you were conv...      1\n",
       "4  judgment of the court background 1 the appella...      0"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IF JUST USE THE TRAINING DATA\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df['len_txt'] =df.cleaned_contents.apply(lambda x: len(x.split()))\n",
    "df = df[df.len_txt >249]\n",
    "df = df[df.len_txt <20000]\n",
    "df = df[['cleaned_contents', 'Discrimination_Label']]\n",
    "df = df.rename(columns = {'cleaned_contents':'text', 'Discrimination_Label':'label'})\n",
    "df['text'] = df['text'].str.lower()\n",
    "df['text'] = pd.Series(re.sub(r'(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)([\\s]{1,3})?([0-9]{1,2})(.{1,3})?((,)|(.))?([\\s]{1,3})?([0-9]{4})|([0-9]{1,2})(.{1,3})?([\\s]{1,3})?(day)?([\\s]{1,3})?(of)?([\\s]{1,3})?(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)((,)|(.))?(\\s{1,3})?([0-9]{4})|(first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth|eleventh|twelfth|thirteenth|fourteenth|fifteenth|sixteenth|seventeenth|eighteenth|nineteenth|twentieth|twenty-first|twenty-second|twenty-third|twenty-fourth|twenty-fifth|twenty-sixth|twenty-seventh|twenty-eighth|twenty-ninth|thirtieth|thirty-first)([\\s]{1,3})?(day)?([\\s]{1,3})?(of)?([\\s]{1,3})?(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)((,)|(.))?(\\s{1,3})?([0-9]{4})|(\\b[0-9]{1,2}(\\-|\\/)[0-9]{1,2}(\\-|\\/)[0-9]{2,4}\\b)|(\\b[0-9]{2,4}(\\-|\\/)[0-9]{1,2}(\\-|\\/)[0-9]{1,2}\\b)', '[DATE]', i) for i in df['text'])\n",
    "#remove special character\n",
    "#df['text'] = pd.Series(re.sub(\"'\", \"\", i) for i in df['text'])\n",
    "#df['text'] = pd.Series(re.sub(\"(\\\\W)+\", \" \", i) for i in df['text'])\n",
    "df = df.replace({'text': {\"'\": \"\"}}, regex=True)\n",
    "df = df.replace({'text': {\"(\\\\W)+\": \" \"}}, regex=True)\n",
    "df.dropna(subset = [\"text\"], inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Wic4G6rVBEV2",
    "outputId": "5f101823-eb78-4aef-e941-2793445a5ee9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence 1 you are charged as follows first co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence 1 josefa kotobalavu you were charged ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence 1 the director of public prosecution ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence 1 mohommed nabi ud dean you were conv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>judgment of the court background 1 the appella...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  sentence 1 you are charged as follows first co...      0\n",
       "1  sentence 1 josefa kotobalavu you were charged ...      1\n",
       "2  sentence 1 the director of public prosecution ...      1\n",
       "3  sentence 1 mohommed nabi ud dean you were conv...      1\n",
       "4  judgment of the court background 1 the appella...      0"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IF USE BOTH TRAINING AND TEST (THIS MEANS WE HAVE NOTHING TO PREDICT AGAINST AS HOLDOUT)\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "df = pd.concat([train_data,test_data])\n",
    "df['len_txt'] =df.cleaned_contents.apply(lambda x: len(x.split()))\n",
    "df = df[df.len_txt >249]\n",
    "df = df[df.len_txt <20000]\n",
    "df = df[['cleaned_contents', 'Discrimination_Label']]\n",
    "df = df.rename(columns = {'cleaned_contents':'text', 'Discrimination_Label':'label'})\n",
    "#lower case to help remove dates\n",
    "df['text'] = df['text'].str.lower()\n",
    "#remove dates\n",
    "df['text'] = pd.Series(re.sub(r'(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)([\\s]{1,3})?([0-9]{1,2})(.{1,3})?((,)|(.))?([\\s]{1,3})?([0-9]{4})|([0-9]{1,2})(.{1,3})?([\\s]{1,3})?(day)?([\\s]{1,3})?(of)?([\\s]{1,3})?(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)((,)|(.))?(\\s{1,3})?([0-9]{4})|(first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth|eleventh|twelfth|thirteenth|fourteenth|fifteenth|sixteenth|seventeenth|eighteenth|nineteenth|twentieth|twenty-first|twenty-second|twenty-third|twenty-fourth|twenty-fifth|twenty-sixth|twenty-seventh|twenty-eighth|twenty-ninth|thirtieth|thirty-first)([\\s]{1,3})?(day)?([\\s]{1,3})?(of)?([\\s]{1,3})?(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)((,)|(.))?(\\s{1,3})?([0-9]{4})|(\\b[0-9]{1,2}(\\-|\\/)[0-9]{1,2}(\\-|\\/)[0-9]{2,4}\\b)|(\\b[0-9]{2,4}(\\-|\\/)[0-9]{1,2}(\\-|\\/)[0-9]{1,2}\\b)', '[DATE]', i) for i in df['text'])\n",
    "#remove special character\n",
    "df['text'] = pd.Series(re.sub(\"'\", \"\", i) for i in df['text'])\n",
    "df['text'] = pd.Series(re.sub(\"(\\\\W)+\", \" \", i) for i in df['text'])\n",
    "\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mKyH7IXBBEtD"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# we need a BERT model and a BERT tokenizer\n",
    "# initialize the empty model and tokenizer objects\n",
    "# we are going to use the Hugging Face's DistilBert model\n",
    "BERT_model_class,BERT_tokenizer_class,BERT_pre_trained_weights = (ppb.DistilBertModel, # the pre-trained DistillBERT model\n",
    "                                                                  ppb.DistilBertTokenizer,\n",
    "                                                                  'distilbert-base-uncased') # the type of DistilBERT model\n",
    "\n",
    "# use the next line instead, if you want (Google's) BERT instead of DistillBERT\n",
    "#BERT_model_class,BERT_tokenizer_class,BERT_pre_trained_weights = (ppb.BertModel, ppb.BertTokenizer,'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer weights/values for the desired type of BERT model into their respective objects\n",
    "tokenizer = BERT_tokenizer_class.from_pretrained(BERT_pre_trained_weights)\n",
    "\n",
    "#model1 is a pytorch BERT model\n",
    "#model1 = BERT_model_class.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "F2cl1DeoBD7P",
    "outputId": "5a6f6763-74e5-458d-92ff-bb781df0e743"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  sentence 1 you are charged as follows first co...      0\n",
      "1  sentence 1 josefa kotobalavu you were charged ...      1\n",
      "2  sentence 1 the director of public prosecution ...      1\n",
      "3  sentence 1 mohommed nabi ud dean you were conv...      1\n",
      "4  judgment of the court background 1 the appella...      0\n",
      "\n",
      "\n",
      "0    [101, 6251, 1015, 2017, 2024, 5338, 2004, 4076...\n",
      "1    [101, 6251, 1015, 12947, 2050, 12849, 3406, 25...\n",
      "2    [101, 6251, 1015, 1996, 2472, 1997, 2270, 1153...\n",
      "3    [101, 6251, 1015, 9587, 23393, 7583, 6583, 563...\n",
      "4    [101, 8689, 1997, 1996, 2457, 4281, 1015, 1996...\n",
      "Name: text, dtype: object\n",
      "\n",
      "\n",
      "(641,)\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = BERT_tokenizer_class.from_pretrained(BERT_pre_trained_weights)\n",
    "tokenized = df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512, truncation=True)))\n",
    "# This turns every sentence into a list of IDs\n",
    "# tokenized is apandas Series object: <class 'pandas.core.series.Series'>\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "print(tokenized.head())\n",
    "print('\\n')\n",
    "print(tokenized.shape) #(6920,) a 1D pandas Series\n",
    "print(type(tokenized)) #<class 'pandas.core.series.Series'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "C2AthlWhDFzT",
    "outputId": "e19ac7b8-5d77-4e49-ddf1-d616f5100035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized.values.shape: (641,)\n",
      "max sentence length is : 512\n",
      "padded.shape: (641, 512)\n"
     ]
    }
   ],
   "source": [
    "print(f'tokenized.values.shape: {tokenized.values.shape}')\n",
    "# find the length of the longest sentence in the dataset\n",
    "max_len = 0\n",
    "for i in tokenized.values:  #tokenized.values is of type #<class 'numpy.ndarray'>\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "print(f'max sentence length is : {max_len}')\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "print(f'padded.shape: {padded.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "colab_type": "code",
    "id": "yiBCb_2LDF4F",
    "outputId": "d1ff78f4-0a68-46fe-cf50-2df97e0ad613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6251, 1015, 2017, 2024, 5338, 2004, 4076, 2034, 4175, 4861, 1997, 15226, 9040, 10043, 2000, 5433, 17332, 1998, 5018, 1997, 1996, 18476, 3642, 6178, 2459, 3327, 2015, 1997, 15226, 14262, 14545, 2401, 3972, 2050, 2090, 1996, 3058, 2000, 3058, 2012, 23233, 12260, 2072, 2352, 12436, 8525, 24861, 2721, 11937, 19722, 2050, 1999, 1996, 2530, 2407, 2018, 22300, 2482, 12032, 3716, 1997, 1050, 2078, 2302, 2014, 9619, 2117, 4175, 4861, 1997, 15226, 9040, 10043, 2000, 2930, 19843, 1015, 1998, 1016, 1037, 1997, 1996, 6997, 10037, 4008, 1997, 2268, 3327, 2015, 1997, 15226, 14262, 14545, 2401, 3972, 2050, 2090, 1996, 3058, 2000, 1996, 3058, 2012, 23233, 12260, 2072, 2352, 12436, 8525, 24861, 2721, 11937, 19722, 2050, 1999, 1996, 2530, 2407, 21653, 1996, 12436, 20876, 1997, 1050, 2078, 2007, 2010, 19085, 2302, 2014, 9619, 1015, 2006, 3058, 2017, 12254, 5905, 2000, 2119, 5571, 2114, 2017, 1998, 4914, 1996, 12654, 1997, 8866, 2006, 3058, 1016, 1996, 12654, 1997, 8866, 7864, 2011, 1996, 2110, 9517, 2163, 2004, 4076, 1996, 5496, 14262, 14545, 2401, 3972, 2050, 2003, 4229, 2086, 1997, 23233, 12260, 2072, 2352, 12436, 8525, 24861, 2721, 11937, 19722, 2050, 1996, 17612, 4630, 1999, 2023, 2553, 2003, 2028, 1050, 2078, 2184, 2086, 1997, 1996, 2168, 2352, 1996, 17612, 4630, 2001, 2184, 2086, 2214, 14118, 1999, 23233, 12260, 2072, 3234, 2082, 2043, 1996, 2034, 5043, 2165, 2173, 2016, 2071, 9131, 2008, 1999, 1996, 2095, 2289, 2076, 1996, 2117, 2744, 2082, 1998, 1999, 3327, 2006, 1996, 2154, 2016, 2106, 2025, 2175, 2000, 2082, 1996, 17612, 4630, 2001, 2409, 2011, 2014, 2388, 2000, 2175, 1998, 2031, 2014, 7198, 1999, 1996, 2314, 2279, 2000, 1996, 2352, 2016, 2187, 2014, 2188, 2894, 2000, 2031, 2014, 7198, 1998, 2006, 1996, 2126, 2067, 2016, 2777, 1996, 5496, 3564, 2279, 2000, 1996, 2314, 2924, 1996, 5496, 5411, 1996, 17612, 4630, 1998, 2356, 2014, 2000, 12673, 2014, 2000, 1996, 19739, 12462, 8983, 2000, 8145, 19739, 12462, 2015, 1996, 17612, 4630, 9480, 1996, 5496, 1998, 2628, 2032, 3402, 1996, 5496, 2409, 2014, 2000, 2644, 1998, 4682, 2091, 2006, 1996, 2598, 2002, 2059, 5411, 2014, 2000, 6366, 2014, 4253, 1998, 7420, 2014, 2065, 2016, 2987, 2102, 2059, 2002, 2097, 3786, 2014, 2007, 1037, 19739, 12462, 6293, 1996, 17612, 4630, 2001, 6015, 2043, 2016, 2657, 2023, 1998, 2628, 1996, 5496, 2015, 8128, 2043, 1996, 17612, 4630, 2288, 6151, 16119, 2002, 2387, 1996, 5496, 16916, 2091, 1998, 2478, 2010, 4416, 2000, 15385, 2014, 12436, 20876, 1996, 5496, 2059, 4895, 5831, 11469, 2010, 1093, 6471, 1998, 2059, 12889, 2010, 7019, 19085, 2046, 1996, 17612, 11390, 12436, 20876, 2016, 2001, 1999, 2307, 3255, 1998, 2318, 6933, 2016, 2001, 13346, 2004, 1996, 5496, 2006, 2327, 1997, 2014, 2016, 2071, 2514, 2008, 2014, 12436, 20876, 2001, 4954, 2043, 1996, 5496, 3651, 2008, 1996, 17612, 4630, 2001, 6933, 2002, 6727, 2014, 2000, 2131, 2039, 2131, 5102, 1998, 2025, 2000, 12367, 3087, 2055, 2023, 5043, 2043, 1996, 17612, 4630, 2001, 3788, 2000, 1996, 2314, 2016, 2071, 2514, 9010, 1997, 2668, 2746, 2041, 1997, 2014, 12436, 20876, 2016, 2059, 2018, 2014, 7198, 1998, 2253, 2188, 2302, 21672, 3087, 2004, 2016, 2001, 6015, 1997, 1996, 5496, 102]\n",
      "\n",
      "\n",
      "[  101  6251  1015  2017  2024  5338  2004  4076  2034  4175  4861  1997\n",
      " 15226  9040 10043  2000  5433 17332  1998  5018  1997  1996 18476  3642\n",
      "  6178  2459  3327  2015  1997 15226 14262 14545  2401  3972  2050  2090\n",
      "  1996  3058  2000  3058  2012 23233 12260  2072  2352 12436  8525 24861\n",
      "  2721 11937 19722  2050  1999  1996  2530  2407  2018 22300  2482 12032\n",
      "  3716  1997  1050  2078  2302  2014  9619  2117  4175  4861  1997 15226\n",
      "  9040 10043  2000  2930 19843  1015  1998  1016  1037  1997  1996  6997\n",
      " 10037  4008  1997  2268  3327  2015  1997 15226 14262 14545  2401  3972\n",
      "  2050  2090  1996  3058  2000  1996  3058  2012 23233 12260  2072  2352\n",
      " 12436  8525 24861  2721 11937 19722  2050  1999  1996  2530  2407 21653\n",
      "  1996 12436 20876  1997  1050  2078  2007  2010 19085  2302  2014  9619\n",
      "  1015  2006  3058  2017 12254  5905  2000  2119  5571  2114  2017  1998\n",
      "  4914  1996 12654  1997  8866  2006  3058  1016  1996 12654  1997  8866\n",
      "  7864  2011  1996  2110  9517  2163  2004  4076  1996  5496 14262 14545\n",
      "  2401  3972  2050  2003  4229  2086  1997 23233 12260  2072  2352 12436\n",
      "  8525 24861  2721 11937 19722  2050  1996 17612  4630  1999  2023  2553\n",
      "  2003  2028  1050  2078  2184  2086  1997  1996  2168  2352  1996 17612\n",
      "  4630  2001  2184  2086  2214 14118  1999 23233 12260  2072  3234  2082\n",
      "  2043  1996  2034  5043  2165  2173  2016  2071  9131  2008  1999  1996\n",
      "  2095  2289  2076  1996  2117  2744  2082  1998  1999  3327  2006  1996\n",
      "  2154  2016  2106  2025  2175  2000  2082  1996 17612  4630  2001  2409\n",
      "  2011  2014  2388  2000  2175  1998  2031  2014  7198  1999  1996  2314\n",
      "  2279  2000  1996  2352  2016  2187  2014  2188  2894  2000  2031  2014\n",
      "  7198  1998  2006  1996  2126  2067  2016  2777  1996  5496  3564  2279\n",
      "  2000  1996  2314  2924  1996  5496  5411  1996 17612  4630  1998  2356\n",
      "  2014  2000 12673  2014  2000  1996 19739 12462  8983  2000  8145 19739\n",
      " 12462  2015  1996 17612  4630  9480  1996  5496  1998  2628  2032  3402\n",
      "  1996  5496  2409  2014  2000  2644  1998  4682  2091  2006  1996  2598\n",
      "  2002  2059  5411  2014  2000  6366  2014  4253  1998  7420  2014  2065\n",
      "  2016  2987  2102  2059  2002  2097  3786  2014  2007  1037 19739 12462\n",
      "  6293  1996 17612  4630  2001  6015  2043  2016  2657  2023  1998  2628\n",
      "  1996  5496  2015  8128  2043  1996 17612  4630  2288  6151 16119  2002\n",
      "  2387  1996  5496 16916  2091  1998  2478  2010  4416  2000 15385  2014\n",
      " 12436 20876  1996  5496  2059  4895  5831 11469  2010  1093  6471  1998\n",
      "  2059 12889  2010  7019 19085  2046  1996 17612 11390 12436 20876  2016\n",
      "  2001  1999  2307  3255  1998  2318  6933  2016  2001 13346  2004  1996\n",
      "  5496  2006  2327  1997  2014  2016  2071  2514  2008  2014 12436 20876\n",
      "  2001  4954  2043  1996  5496  3651  2008  1996 17612  4630  2001  6933\n",
      "  2002  6727  2014  2000  2131  2039  2131  5102  1998  2025  2000 12367\n",
      "  3087  2055  2023  5043  2043  1996 17612  4630  2001  3788  2000  1996\n",
      "  2314  2016  2071  2514  9010  1997  2668  2746  2041  1997  2014 12436\n",
      " 20876  2016  2059  2018  2014  7198  1998  2253  2188  2302 21672  3087\n",
      "  2004  2016  2001  6015  1997  1996  5496   102]\n",
      "\n",
      "len(padded[0]): 512\n"
     ]
    }
   ],
   "source": [
    "print(tokenized[0])\n",
    "print('\\n')\n",
    "print(padded[0])\n",
    "print(f'\\nlen(padded[0]): {len(padded[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9lVv-iSfDF6T",
    "outputId": "2586c2e1-d4e5-4eb4-8104-4e45facf2cf3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(641, 512)"
      ]
     },
     "execution_count": 93,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "RD-Bb9IUDF9C",
    "outputId": "3e89c43a-406f-4628-9cf9-e5e8b1574b49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask[0])\n",
    "print(attention_mask[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5tKfb6neDYlQ"
   },
   "outputs": [],
   "source": [
    "#create a tensor for the attention_mask\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "#We now create an input tensor out of the padded token matrix, and send that to DistilBERT\n",
    "input_ids = torch.tensor(padded) \n",
    "#the model() function runs our sentence\n",
    "# convert labels to tensor \n",
    "labels = df['label']\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "#Divide up our training set to use 90% for training and 10% for validation.\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "STxqEAroDYoX",
    "outputId": "94eb3149-e3fc-4a41-c422-b922af21bd1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  576 training samples\n",
      "   65 validation samples\n"
     ]
    }
   ],
   "source": [
    "# Create a 80-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8QwNLSVkDYsH"
   },
   "outputs": [],
   "source": [
    "# USe pytorch dataloader\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 8\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ubuCwxYbQlBW"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jM6M8_7-MV4k",
    "outputId": "266785ff-1277-4925-d350-add773d3abd9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "dZDg1h0zMV_I",
    "outputId": "a06ef0e3-4171-4412-b416-530f9f111c1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ug1G0QpsMWQh"
   },
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_VDwGGOXNPpD"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jE70qHRLNPsF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gK5MxFsNPvD"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dp6fMfhzQCgt"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "EFmHuriKNPx3",
    "outputId": "2ed26c61-3e6a-4da9-f743-70b12c2d0d49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     72.    Elapsed: 0:01:00.\n",
      "\n",
      "  Average training loss: 0.77\n",
      "  Training epcoh took: 0:01:49\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.57\n",
      "  Validation Loss: 0.68\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     72.    Elapsed: 0:01:01.\n",
      "\n",
      "  Average training loss: 0.70\n",
      "  Training epcoh took: 0:01:49\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.57\n",
      "  Validation Loss: 0.68\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     72.    Elapsed: 0:01:00.\n",
      "\n",
      "  Average training loss: 0.70\n",
      "  Training epcoh took: 0:01:49\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.57\n",
      "  Validation Loss: 0.68\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of     72.    Elapsed: 0:01:01.\n",
      "\n",
      "  Average training loss: 0.69\n",
      "  Training epcoh took: 0:01:49\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.57\n",
      "  Validation Loss: 0.69\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:07:32 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "xCUwfkc8NP1Q",
    "outputId": "fa5bb965-fab6-49ca-d96f-449abc24e6af"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0:01:49</td>\n",
       "      <td>0:00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0:01:49</td>\n",
       "      <td>0:00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0:01:49</td>\n",
       "      <td>0:00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0:01:49</td>\n",
       "      <td>0:00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.77         0.68           0.57       0:01:49         0:00:04\n",
       "2               0.70         0.68           0.57       0:01:49         0:00:04\n",
       "3               0.70         0.68           0.57       0:01:49         0:00:04\n",
       "4               0.69         0.69           0.57       0:01:49         0:00:04"
      ]
     },
     "execution_count": 108,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "colab_type": "code",
    "id": "bnaXha0ocYpA",
    "outputId": "b27c3756-7f36-48c4-b22b-ed75d7a91f03"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vwnKCu_dcbY-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "dlQVaErvcbbj",
    "outputId": "3a40cb09-f8f1-4836-9290-2c940d26ec57"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence name of the victim is suppressed she ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>judgment this is an appeal against conviction ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>judgment 1 on DATE the appellant william raymo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>judgment 1 on the DATE in the nasinu magistrat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentence background 1 the accused was charged ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  sentence name of the victim is suppressed she ...      1\n",
       "1  judgment this is an appeal against conviction ...      1\n",
       "2  judgment 1 on DATE the appellant william raymo...      1\n",
       "3  judgment 1 on the DATE in the nasinu magistrat...      0\n",
       "4  sentence background 1 the accused was charged ...      1"
      ]
     },
     "execution_count": 110,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Holdout\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "# IF JUST USE THE TRAINING DATA\n",
    "df = pd.read_csv(\"test.csv\")\n",
    "df['len_txt'] =df.cleaned_contents.apply(lambda x: len(x.split()))\n",
    "df = df[df.len_txt >249]\n",
    "df = df[df.len_txt <20000]\n",
    "df = df[['cleaned_contents', 'Discrimination_Label']]\n",
    "df = df.rename(columns = {'cleaned_contents':'text', 'Discrimination_Label':'label'})\n",
    "df['text'] = df['text'].str.lower()\n",
    "df['text'] = pd.Series(re.sub(r'(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)([\\s]{1,3})?([0-9]{1,2})(.{1,3})?((,)|(.))?([\\s]{1,3})?([0-9]{4})|([0-9]{1,2})(.{1,3})?([\\s]{1,3})?(day)?([\\s]{1,3})?(of)?([\\s]{1,3})?(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)((,)|(.))?(\\s{1,3})?([0-9]{4})|(first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth|eleventh|twelfth|thirteenth|fourteenth|fifteenth|sixteenth|seventeenth|eighteenth|nineteenth|twentieth|twenty-first|twenty-second|twenty-third|twenty-fourth|twenty-fifth|twenty-sixth|twenty-seventh|twenty-eighth|twenty-ninth|thirtieth|thirty-first)([\\s]{1,3})?(day)?([\\s]{1,3})?(of)?([\\s]{1,3})?(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)((,)|(.))?(\\s{1,3})?([0-9]{4})|(\\b[0-9]{1,2}(\\-|\\/)[0-9]{1,2}(\\-|\\/)[0-9]{2,4}\\b)|(\\b[0-9]{2,4}(\\-|\\/)[0-9]{1,2}(\\-|\\/)[0-9]{1,2}\\b)', '[DATE]', i) for i in df['text'])\n",
    "#remove special character\n",
    "#df['text'] = pd.Series(re.sub(\"'\", \"\", i) for i in df['text'])\n",
    "#df['text'] = pd.Series(re.sub(\"(\\\\W)+\", \" \", i) for i in df['text'])\n",
    "df = df.replace({'text': {\"'\": \"\"}}, regex=True)\n",
    "df = df.replace({'text': {\"(\\\\W)+\": \" \"}}, regex=True)\n",
    "df.dropna(subset = [\"text\"], inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "yKdIMGDeooiQ",
    "outputId": "7f934f76-69a4-4f73-e9b9-565fc440147d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  sentence name of the victim is suppressed she ...      1\n",
      "1  judgment this is an appeal against conviction ...      1\n",
      "2  judgment 1 on DATE the appellant william raymo...      1\n",
      "3  judgment 1 on the DATE in the nasinu magistrat...      0\n",
      "4  sentence background 1 the accused was charged ...      1\n",
      "\n",
      "\n",
      "0    [101, 6251, 2171, 1997, 1996, 6778, 2003, 1371...\n",
      "1    [101, 8689, 2023, 2003, 2019, 5574, 2114, 1065...\n",
      "2    [101, 8689, 1015, 2006, 3058, 1996, 10439, 241...\n",
      "3    [101, 8689, 1015, 2006, 1996, 3058, 1999, 1996...\n",
      "4    [101, 6251, 4281, 1015, 1996, 5496, 2001, 5338...\n",
      "Name: text, dtype: object\n",
      "\n",
      "\n",
      "(162,)\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BERT_tokenizer_class.from_pretrained(BERT_pre_trained_weights)\n",
    "tokenized = df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512, truncation=True)))\n",
    "# This turns every sentence into a list of IDs\n",
    "# tokenized is apandas Series object: <class 'pandas.core.series.Series'>\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "print(tokenized.head())\n",
    "print('\\n')\n",
    "print(tokenized.shape) #(162,) a 1D pandas Series\n",
    "print(type(tokenized)) #<class 'pandas.core.series.Series'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DjExIK9Pooru"
   },
   "outputs": [],
   "source": [
    "#create a tensor for the attention_mask\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "#We now create an input tensor out of the padded token matrix, and send that to DistilBERT\n",
    "input_ids = torch.tensor(padded) \n",
    "#the model() function runs our sentence\n",
    "# convert labels to tensor \n",
    "labels = df['label']\n",
    "labels = torch.tensor(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FN9ImKVtoj94"
   },
   "outputs": [],
   "source": [
    "# Set the batch size.  \n",
    "batch_size = 8\n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_mask, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4R1EvVccpKfK",
    "outputId": "ee9de8b7-3b77-40dd-c361-62d5362197d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 162 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WpG00Uo7pKnK",
    "outputId": "2f513781-9631-4d20-be1a-291bca0ff8b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 97 of 162 (59.88%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "q7vsNmK6q6vN",
    "outputId": "5659ff6c-5116-4b57-976c-dd0e8cebd8b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 129,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "RDVD1Q4RrZED",
    "outputId": "cf58bf88-9b42-4a34-9b67-10cd0c70b3be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 1, 1, 0, 1, 1, 1, 0]), array([0, 1, 1, 0, 1, 1, 1, 1]), array([1, 1, 0, 1, 0, 1, 1, 1]), array([1, 0, 0, 0, 0, 1, 0, 1]), array([1, 0, 0, 1, 0, 1, 0, 1]), array([0, 0, 1, 1, 1, 0, 1, 1]), array([1, 1, 1, 1, 1, 0, 0, 0]), array([1, 1, 1, 1, 0, 1, 0, 1]), array([0, 0, 0, 1, 1, 1, 1, 0]), array([1, 0, 0, 1, 1, 1, 1, 1]), array([0, 0, 1, 1, 1, 1, 0, 1]), array([0, 0, 1, 0, 1, 1, 1, 1]), array([1, 1, 0, 0, 1, 1, 1, 1]), array([1, 0, 1, 0, 0, 0, 1, 0]), array([0, 1, 1, 1, 0, 1, 0, 0]), array([1, 1, 0, 0, 1, 1, 1, 1]), array([0, 0, 1, 0, 0, 0, 0, 0]), array([1, 0, 1, 1, 0, 1, 0, 1]), array([1, 1, 0, 1, 0, 0, 0, 0]), array([1, 0, 1, 1, 1, 0, 1, 1]), array([1, 1])]\n"
     ]
    }
   ],
   "source": [
    "print(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "tTdwm44ssSxA",
    "outputId": "c02848c1-df6a-4921-edfa-25ca25eeb716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.03078638,  0.46211198],\n",
      "       [-0.02976584,  0.46293426],\n",
      "       [-0.03101056,  0.46121714],\n",
      "       [-0.03012421,  0.46278536],\n",
      "       [-0.03042751,  0.46268833],\n",
      "       [-0.030663  ,  0.4622148 ],\n",
      "       [-0.02965624,  0.4632329 ],\n",
      "       [-0.03062444,  0.4620274 ]], dtype=float32), array([[-0.03071432,  0.4622516 ],\n",
      "       [-0.03002898,  0.46180037],\n",
      "       [-0.0301637 ,  0.46227825],\n",
      "       [-0.03079856,  0.46179354],\n",
      "       [-0.02964677,  0.46170032],\n",
      "       [-0.02983115,  0.4622293 ],\n",
      "       [-0.02983773,  0.46259096],\n",
      "       [-0.02982606,  0.4620067 ]], dtype=float32), array([[-0.02997181,  0.46230653],\n",
      "       [-0.03025569,  0.46303916],\n",
      "       [-0.0309477 ,  0.46114233],\n",
      "       [-0.03091851,  0.4612416 ],\n",
      "       [-0.02949068,  0.46219367],\n",
      "       [-0.03024698,  0.46110308],\n",
      "       [-0.03019177,  0.462166  ],\n",
      "       [-0.02950762,  0.46187317]], dtype=float32), array([[-0.03016361,  0.46160263],\n",
      "       [-0.03181884,  0.46091896],\n",
      "       [-0.03049608,  0.46193135],\n",
      "       [-0.03081063,  0.4622715 ],\n",
      "       [-0.03068995,  0.46257007],\n",
      "       [-0.0307055 ,  0.46179575],\n",
      "       [-0.02990591,  0.46295902],\n",
      "       [-0.03006053,  0.46260205]], dtype=float32), array([[-0.03054085,  0.4615802 ],\n",
      "       [-0.02989316,  0.46248135],\n",
      "       [-0.03051456,  0.463076  ],\n",
      "       [-0.03012126,  0.4625478 ],\n",
      "       [-0.03009896,  0.46187776],\n",
      "       [-0.0301471 ,  0.46150494],\n",
      "       [-0.02877582,  0.46374378],\n",
      "       [-0.03013429,  0.4623948 ]], dtype=float32), array([[-0.03043929,  0.4621577 ],\n",
      "       [-0.03147379,  0.46106598],\n",
      "       [-0.02982005,  0.46304354],\n",
      "       [-0.02994826,  0.4622404 ],\n",
      "       [-0.03110007,  0.4615949 ],\n",
      "       [-0.02926746,  0.4639722 ],\n",
      "       [-0.03025494,  0.4611773 ],\n",
      "       [-0.03032826,  0.46174878]], dtype=float32), array([[-0.029649  ,  0.46230483],\n",
      "       [-0.03022913,  0.4621531 ],\n",
      "       [-0.03048524,  0.46129397],\n",
      "       [-0.03032755,  0.46143275],\n",
      "       [-0.03017243,  0.46214604],\n",
      "       [-0.0300865 ,  0.46219623],\n",
      "       [-0.03026017,  0.46185616],\n",
      "       [-0.03009398,  0.46251938]], dtype=float32), array([[-0.02978136,  0.4629471 ],\n",
      "       [-0.03089976,  0.46096042],\n",
      "       [-0.03047345,  0.4624095 ],\n",
      "       [-0.03012333,  0.46179095],\n",
      "       [-0.03123277,  0.46136853],\n",
      "       [-0.03020796,  0.46175188],\n",
      "       [-0.03052688,  0.46064624],\n",
      "       [-0.02940348,  0.46188462]], dtype=float32), array([[-0.03031957,  0.46257737],\n",
      "       [-0.03057906,  0.46079764],\n",
      "       [-0.03030544,  0.46203345],\n",
      "       [-0.03037261,  0.46209258],\n",
      "       [-0.03010383,  0.46204615],\n",
      "       [-0.03058789,  0.4618691 ],\n",
      "       [-0.03059178,  0.46197304],\n",
      "       [-0.03071742,  0.46166074]], dtype=float32), array([[-0.03112043,  0.461752  ],\n",
      "       [-0.03098885,  0.46152082],\n",
      "       [-0.03039219,  0.4624529 ],\n",
      "       [-0.02963134,  0.46263975],\n",
      "       [-0.03040819,  0.46073493],\n",
      "       [-0.03001065,  0.4617822 ],\n",
      "       [-0.03048639,  0.46237087],\n",
      "       [-0.02975143,  0.46240175]], dtype=float32), array([[-0.03120678,  0.46261007],\n",
      "       [-0.03012043,  0.4615857 ],\n",
      "       [-0.02990355,  0.4629505 ],\n",
      "       [-0.0307746 ,  0.46143666],\n",
      "       [-0.03075015,  0.46169552],\n",
      "       [-0.03050263,  0.46213925],\n",
      "       [-0.02953079,  0.46243018],\n",
      "       [-0.02986248,  0.46256092]], dtype=float32), array([[-0.03124276,  0.46395162],\n",
      "       [-0.02958616,  0.46255022],\n",
      "       [-0.03067133,  0.46169513],\n",
      "       [-0.0310045 ,  0.46161503],\n",
      "       [-0.03091272,  0.46165186],\n",
      "       [-0.02970154,  0.4633128 ],\n",
      "       [-0.03094362,  0.46238163],\n",
      "       [-0.02895785,  0.46299088]], dtype=float32), array([[-0.02988817,  0.462712  ],\n",
      "       [-0.02972687,  0.46279386],\n",
      "       [-0.02984522,  0.46280247],\n",
      "       [-0.02980384,  0.46341124],\n",
      "       [-0.02993151,  0.46216255],\n",
      "       [-0.03103391,  0.4605502 ],\n",
      "       [-0.03052281,  0.46263453],\n",
      "       [-0.02975576,  0.46225902]], dtype=float32), array([[-0.03054264,  0.4611211 ],\n",
      "       [-0.03051317,  0.46157965],\n",
      "       [-0.03054819,  0.46190146],\n",
      "       [-0.03043918,  0.46189824],\n",
      "       [-0.02988081,  0.46268326],\n",
      "       [-0.02995492,  0.46312743],\n",
      "       [-0.02964985,  0.46161044],\n",
      "       [-0.03139171,  0.46071398]], dtype=float32), array([[-0.02977195,  0.4624683 ],\n",
      "       [-0.03026511,  0.4623413 ],\n",
      "       [-0.0301257 ,  0.46300927],\n",
      "       [-0.03283888,  0.4602061 ],\n",
      "       [-0.03050849,  0.46154767],\n",
      "       [-0.03052591,  0.461845  ],\n",
      "       [-0.0300953 ,  0.462249  ],\n",
      "       [-0.03025711,  0.46142614]], dtype=float32), array([[-0.02980609,  0.46334943],\n",
      "       [-0.03010992,  0.46277988],\n",
      "       [-0.03186132,  0.46055305],\n",
      "       [-0.03070023,  0.4623285 ],\n",
      "       [-0.0308624 ,  0.46166244],\n",
      "       [-0.03070881,  0.46154404],\n",
      "       [-0.03057212,  0.46178016],\n",
      "       [-0.03033965,  0.4615545 ]], dtype=float32), array([[-0.03036338,  0.46167308],\n",
      "       [-0.0302892 ,  0.46236977],\n",
      "       [-0.02987353,  0.46230304],\n",
      "       [-0.03007473,  0.46171096],\n",
      "       [-0.03104978,  0.46077985],\n",
      "       [-0.03156871,  0.4615022 ],\n",
      "       [-0.03055942,  0.46145314],\n",
      "       [-0.03024942,  0.46164107]], dtype=float32), array([[-0.03069435,  0.46135285],\n",
      "       [-0.03041738,  0.46084276],\n",
      "       [-0.03032603,  0.46363387],\n",
      "       [-0.03069656,  0.46188554],\n",
      "       [-0.02982158,  0.46252963],\n",
      "       [-0.03113469,  0.46113476],\n",
      "       [-0.03056918,  0.46155548],\n",
      "       [-0.03032921,  0.46167335]], dtype=float32), array([[-0.03052155,  0.4612773 ],\n",
      "       [-0.03027647,  0.46187535],\n",
      "       [-0.03020574,  0.46227992],\n",
      "       [-0.03065474,  0.46212128],\n",
      "       [-0.03127312,  0.46128663],\n",
      "       [-0.03040759,  0.46187207],\n",
      "       [-0.02964614,  0.46206594],\n",
      "       [-0.0299738 ,  0.46183193]], dtype=float32), array([[-0.03083727,  0.4626234 ],\n",
      "       [-0.02999814,  0.4614219 ],\n",
      "       [-0.03031565,  0.46186757],\n",
      "       [-0.03142388,  0.4608288 ],\n",
      "       [-0.02998907,  0.46141008],\n",
      "       [-0.03047279,  0.46210846],\n",
      "       [-0.03013993,  0.4623235 ],\n",
      "       [-0.02943717,  0.46307743]], dtype=float32), array([[-0.03012176,  0.46283013],\n",
      "       [-0.03045381,  0.46177748]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PCUoVBxK1inm"
   },
   "outputs": [],
   "source": [
    "preds = np.concatenate( predictions, axis=0 )\n",
    "tl = np.concatenate( true_labels, axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MMJMMtk91iq0",
    "outputId": "3b4db20b-4cd6-4d11-cefc-b73715dba114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03078638  0.46211198]\n",
      " [-0.02976584  0.46293426]\n",
      " [-0.03101056  0.46121714]\n",
      " [-0.03012421  0.46278536]\n",
      " [-0.03042751  0.46268833]\n",
      " [-0.030663    0.4622148 ]\n",
      " [-0.02965624  0.4632329 ]\n",
      " [-0.03062444  0.4620274 ]\n",
      " [-0.03071432  0.4622516 ]\n",
      " [-0.03002898  0.46180037]\n",
      " [-0.0301637   0.46227825]\n",
      " [-0.03079856  0.46179354]\n",
      " [-0.02964677  0.46170032]\n",
      " [-0.02983115  0.4622293 ]\n",
      " [-0.02983773  0.46259096]\n",
      " [-0.02982606  0.4620067 ]\n",
      " [-0.02997181  0.46230653]\n",
      " [-0.03025569  0.46303916]\n",
      " [-0.0309477   0.46114233]\n",
      " [-0.03091851  0.4612416 ]\n",
      " [-0.02949068  0.46219367]\n",
      " [-0.03024698  0.46110308]\n",
      " [-0.03019177  0.462166  ]\n",
      " [-0.02950762  0.46187317]\n",
      " [-0.03016361  0.46160263]\n",
      " [-0.03181884  0.46091896]\n",
      " [-0.03049608  0.46193135]\n",
      " [-0.03081063  0.4622715 ]\n",
      " [-0.03068995  0.46257007]\n",
      " [-0.0307055   0.46179575]\n",
      " [-0.02990591  0.46295902]\n",
      " [-0.03006053  0.46260205]\n",
      " [-0.03054085  0.4615802 ]\n",
      " [-0.02989316  0.46248135]\n",
      " [-0.03051456  0.463076  ]\n",
      " [-0.03012126  0.4625478 ]\n",
      " [-0.03009896  0.46187776]\n",
      " [-0.0301471   0.46150494]\n",
      " [-0.02877582  0.46374378]\n",
      " [-0.03013429  0.4623948 ]\n",
      " [-0.03043929  0.4621577 ]\n",
      " [-0.03147379  0.46106598]\n",
      " [-0.02982005  0.46304354]\n",
      " [-0.02994826  0.4622404 ]\n",
      " [-0.03110007  0.4615949 ]\n",
      " [-0.02926746  0.4639722 ]\n",
      " [-0.03025494  0.4611773 ]\n",
      " [-0.03032826  0.46174878]\n",
      " [-0.029649    0.46230483]\n",
      " [-0.03022913  0.4621531 ]\n",
      " [-0.03048524  0.46129397]\n",
      " [-0.03032755  0.46143275]\n",
      " [-0.03017243  0.46214604]\n",
      " [-0.0300865   0.46219623]\n",
      " [-0.03026017  0.46185616]\n",
      " [-0.03009398  0.46251938]\n",
      " [-0.02978136  0.4629471 ]\n",
      " [-0.03089976  0.46096042]\n",
      " [-0.03047345  0.4624095 ]\n",
      " [-0.03012333  0.46179095]\n",
      " [-0.03123277  0.46136853]\n",
      " [-0.03020796  0.46175188]\n",
      " [-0.03052688  0.46064624]\n",
      " [-0.02940348  0.46188462]\n",
      " [-0.03031957  0.46257737]\n",
      " [-0.03057906  0.46079764]\n",
      " [-0.03030544  0.46203345]\n",
      " [-0.03037261  0.46209258]\n",
      " [-0.03010383  0.46204615]\n",
      " [-0.03058789  0.4618691 ]\n",
      " [-0.03059178  0.46197304]\n",
      " [-0.03071742  0.46166074]\n",
      " [-0.03112043  0.461752  ]\n",
      " [-0.03098885  0.46152082]\n",
      " [-0.03039219  0.4624529 ]\n",
      " [-0.02963134  0.46263975]\n",
      " [-0.03040819  0.46073493]\n",
      " [-0.03001065  0.4617822 ]\n",
      " [-0.03048639  0.46237087]\n",
      " [-0.02975143  0.46240175]\n",
      " [-0.03120678  0.46261007]\n",
      " [-0.03012043  0.4615857 ]\n",
      " [-0.02990355  0.4629505 ]\n",
      " [-0.0307746   0.46143666]\n",
      " [-0.03075015  0.46169552]\n",
      " [-0.03050263  0.46213925]\n",
      " [-0.02953079  0.46243018]\n",
      " [-0.02986248  0.46256092]\n",
      " [-0.03124276  0.46395162]\n",
      " [-0.02958616  0.46255022]\n",
      " [-0.03067133  0.46169513]\n",
      " [-0.0310045   0.46161503]\n",
      " [-0.03091272  0.46165186]\n",
      " [-0.02970154  0.4633128 ]\n",
      " [-0.03094362  0.46238163]\n",
      " [-0.02895785  0.46299088]\n",
      " [-0.02988817  0.462712  ]\n",
      " [-0.02972687  0.46279386]\n",
      " [-0.02984522  0.46280247]\n",
      " [-0.02980384  0.46341124]\n",
      " [-0.02993151  0.46216255]\n",
      " [-0.03103391  0.4605502 ]\n",
      " [-0.03052281  0.46263453]\n",
      " [-0.02975576  0.46225902]\n",
      " [-0.03054264  0.4611211 ]\n",
      " [-0.03051317  0.46157965]\n",
      " [-0.03054819  0.46190146]\n",
      " [-0.03043918  0.46189824]\n",
      " [-0.02988081  0.46268326]\n",
      " [-0.02995492  0.46312743]\n",
      " [-0.02964985  0.46161044]\n",
      " [-0.03139171  0.46071398]\n",
      " [-0.02977195  0.4624683 ]\n",
      " [-0.03026511  0.4623413 ]\n",
      " [-0.0301257   0.46300927]\n",
      " [-0.03283888  0.4602061 ]\n",
      " [-0.03050849  0.46154767]\n",
      " [-0.03052591  0.461845  ]\n",
      " [-0.0300953   0.462249  ]\n",
      " [-0.03025711  0.46142614]\n",
      " [-0.02980609  0.46334943]\n",
      " [-0.03010992  0.46277988]\n",
      " [-0.03186132  0.46055305]\n",
      " [-0.03070023  0.4623285 ]\n",
      " [-0.0308624   0.46166244]\n",
      " [-0.03070881  0.46154404]\n",
      " [-0.03057212  0.46178016]\n",
      " [-0.03033965  0.4615545 ]\n",
      " [-0.03036338  0.46167308]\n",
      " [-0.0302892   0.46236977]\n",
      " [-0.02987353  0.46230304]\n",
      " [-0.03007473  0.46171096]\n",
      " [-0.03104978  0.46077985]\n",
      " [-0.03156871  0.4615022 ]\n",
      " [-0.03055942  0.46145314]\n",
      " [-0.03024942  0.46164107]\n",
      " [-0.03069435  0.46135285]\n",
      " [-0.03041738  0.46084276]\n",
      " [-0.03032603  0.46363387]\n",
      " [-0.03069656  0.46188554]\n",
      " [-0.02982158  0.46252963]\n",
      " [-0.03113469  0.46113476]\n",
      " [-0.03056918  0.46155548]\n",
      " [-0.03032921  0.46167335]\n",
      " [-0.03052155  0.4612773 ]\n",
      " [-0.03027647  0.46187535]\n",
      " [-0.03020574  0.46227992]\n",
      " [-0.03065474  0.46212128]\n",
      " [-0.03127312  0.46128663]\n",
      " [-0.03040759  0.46187207]\n",
      " [-0.02964614  0.46206594]\n",
      " [-0.0299738   0.46183193]\n",
      " [-0.03083727  0.4626234 ]\n",
      " [-0.02999814  0.4614219 ]\n",
      " [-0.03031565  0.46186757]\n",
      " [-0.03142388  0.4608288 ]\n",
      " [-0.02998907  0.46141008]\n",
      " [-0.03047279  0.46210846]\n",
      " [-0.03013993  0.4623235 ]\n",
      " [-0.02943717  0.46307743]\n",
      " [-0.03012176  0.46283013]\n",
      " [-0.03045381  0.46177748]]\n"
     ]
    }
   ],
   "source": [
    "# The model has predicted everything as positive. \n",
    "print(preds)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FineTuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
