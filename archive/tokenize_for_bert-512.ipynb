{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import transformers as ppb\n",
    "from transformers import RobertaTokenizer, DistilBertTokenizer, BertTokenizer, RobertaModel, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Variables\n",
    "chunk_len = 512 #200\n",
    "overlap_len = 128 #50\n",
    "\n",
    "BERT_tokenizer_class = ppb.BertTokenizer\n",
    "BERT_pre_trained_weights = 'bert-base-cased'\n",
    "tokenizer = BERT_tokenizer_class.from_pretrained(BERT_pre_trained_weights)\n",
    "model1 = ppb.BertModel.from_pretrained(BERT_pre_trained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>len_txt</th>\n",
       "      <th>doc_use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>79466</td>\n",
       "      <td>judgment the appellant was convicted in the la...</td>\n",
       "      <td>0</td>\n",
       "      <td>476</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>71087</td>\n",
       "      <td>judgment on appeal the appellant was convicted...</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>75461</td>\n",
       "      <td>sentence 1 you josateki taliga are here today ...</td>\n",
       "      <td>1</td>\n",
       "      <td>869</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>82484</td>\n",
       "      <td>sentence 1 the accused tomasi tiko bulivou is ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2132</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>73984</td>\n",
       "      <td>sentence the accused has been convicted by thi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1649</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>73541</td>\n",
       "      <td>judgment 1 the appellant was charged before th...</td>\n",
       "      <td>0</td>\n",
       "      <td>1724</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>71842</td>\n",
       "      <td>decision 1 on DATE the appellant was convicted...</td>\n",
       "      <td>1</td>\n",
       "      <td>841</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>281131</td>\n",
       "      <td>sentence 1 on DATE in the presence of your cou...</td>\n",
       "      <td>1</td>\n",
       "      <td>907</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>284308</td>\n",
       "      <td>sentence 1 the accused is before the court for...</td>\n",
       "      <td>0</td>\n",
       "      <td>1114</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>80433</td>\n",
       "      <td>j u d g m e n t gamalath ja 1 the appellant wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>1513</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      docid                                               text  label  \\\n",
       "770   79466  judgment the appellant was convicted in the la...      0   \n",
       "198   71087  judgment on appeal the appellant was convicted...      0   \n",
       "704   75461  sentence 1 you josateki taliga are here today ...      1   \n",
       "68    82484  sentence 1 the accused tomasi tiko bulivou is ...      1   \n",
       "35    73984  sentence the accused has been convicted by thi...      1   \n",
       "787   73541  judgment 1 the appellant was charged before th...      0   \n",
       "86    71842  decision 1 on DATE the appellant was convicted...      1   \n",
       "427  281131  sentence 1 on DATE in the presence of your cou...      1   \n",
       "515  284308  sentence 1 the accused is before the court for...      0   \n",
       "407   80433  j u d g m e n t gamalath ja 1 the appellant wa...      1   \n",
       "\n",
       "     len_txt doc_use  \n",
       "770      476    test  \n",
       "198      299   train  \n",
       "704      869    test  \n",
       "68      2132   train  \n",
       "35      1649   train  \n",
       "787     1724    test  \n",
       "86       841   train  \n",
       "427      907   train  \n",
       "515     1114   train  \n",
       "407     1513   train  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset into a pandas dataframe.\n",
    "# Because we want to get embeddings for both train and test sets we are going to do together\n",
    "# Add a column to preserve where the doc came from\n",
    "\n",
    "train_raw = pd.read_csv(\"../../w266_project/data/train_lcase.csv\")\n",
    "train_raw['doc_use'] = 'train'\n",
    "test_raw = pd.read_csv(\"../../w266_project/data/test_lcase.csv\")\n",
    "test_raw['doc_use'] = 'test'\n",
    "\n",
    "df_raw = pd.concat([train_raw, test_raw])\n",
    "df_raw.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df_raw.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Extract all the tokenize elements out of data_tokenize from the above tokenizer.encode_plus\n",
    "# gives us input ids, attention mask and critically overflow tokens\n",
    "\n",
    "def extract_tokens(data_tokenize, targets):\n",
    "\n",
    "    previous_input_ids = data_tokenize[\"input_ids\"].reshape(-1) # a tensor of the input IDs )\n",
    "    previous_attention_mask = data_tokenize[\"attention_mask\"].reshape(-1) # a tensor of the attention mask (200 * 1)\n",
    "    previous_token_type_ids = data_tokenize[\"token_type_ids\"].reshape(-1) # a tensor of the attention mask (200 * 0)\n",
    "    remain = data_tokenize.get(\"overflowing_tokens\") # list of the overflow tokens\n",
    "    targets = torch.tensor(targets, dtype=torch.int) # a tensor of current target (1)\n",
    "\n",
    "    return previous_input_ids, previous_attention_mask, previous_token_type_ids, remain, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Tokenize Runtime: 0:00:23.339582\n"
     ]
    }
   ],
   "source": [
    "# Do the tokenization\n",
    "# This returns a transformers object with 5 elements\n",
    "# We only really need the input_ids and attention mask for modelling\n",
    "# We will use these IDS to get out embeddings\n",
    "\n",
    "# overflowing_tokens (list) - all the elements after our 200 word split\n",
    "# num_truncated_tokens (integer) - how many overflow tokens we have, for text[0] it is 1822\n",
    "# input_ids (tensor) - the first 200 tokens, with special token 101 at the beginning and 102 at end\n",
    "# token_type_ids (tensor) - the token types for the input - there are 200, ours are all zero's (why?)\n",
    "# attention_mask (tensor) - attention mask in case our text < 200 tokens\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Reset lists\n",
    "long_terms_token = []\n",
    "input_ids_list = []\n",
    "attention_mask_list = []\n",
    "token_type_ids_list = []\n",
    "targets_list = []\n",
    "\n",
    "input_ids_list_head = []\n",
    "attention_mask_list_head = []\n",
    "token_type_ids_list_head = []\n",
    "targets_list_head = []\n",
    "\n",
    "input_ids_list_olap = []\n",
    "attention_mask_list_olap = []\n",
    "token_type_ids_list_olap= []\n",
    "targets_list_olap= []\n",
    "\n",
    "input_ids_list_tail = []\n",
    "attention_mask_list_tail = []\n",
    "token_type_ids_list_tail = []\n",
    "targets_list_tail= []\n",
    "\n",
    "for idx in range(len(df_raw)): \n",
    "    \n",
    "    long_terms_token = []\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    token_type_ids_list = []\n",
    "    targets_list = []\n",
    "        \n",
    "    # tokenize for this row in train_raw\n",
    "    data = tokenizer.encode_plus(\n",
    "        df_raw['text'][idx],\n",
    "        max_length=chunk_len,\n",
    "        pad_to_max_length=True,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_tensors='pt')\n",
    "\n",
    "    # grab the targets for this row in train_raw\n",
    "    targets = int(df_raw['label'][idx])\n",
    "    \n",
    "    # extract the tokens\n",
    "    input_ids, attention_mask, token_type_ids, remain, targets = extract_tokens(data, targets)\n",
    "    remain = [] if remain is None else remain # For cases where there is no overflow\n",
    "    \n",
    "    # CREATE LISTS FOR THE HEAD\n",
    "    input_ids_list_head.append(input_ids)\n",
    "    attention_mask_list_head.append(attention_mask)\n",
    "    token_type_ids_list_head.append(token_type_ids)\n",
    "    targets_list_head.append(targets)\n",
    "    \n",
    "    # GET OVERLAPPING TOKEN LISTS *****************************\n",
    "    remain = torch.tensor(remain, dtype=torch.long)\n",
    "    idxs = range(len(remain)+ chunk_len)\n",
    "    idxs = idxs[(chunk_len-overlap_len-2)::(chunk_len-overlap_len-2)]\n",
    "    input_ids_first_overlap = input_ids[-(overlap_len+1):-1]\n",
    "    start_token = torch.tensor([101], dtype=torch.long)\n",
    "    end_token = torch.tensor([102], dtype=torch.long)\n",
    "    \n",
    "    # Get the initial 200 word tensors (same as head)\n",
    "    input_ids_list.append(input_ids)\n",
    "    attention_mask_list.append(attention_mask)\n",
    "    token_type_ids_list.append(token_type_ids)\n",
    "    targets_list.append(targets)\n",
    "    \n",
    "    # For each overlapping section create a tensor of input_ids, attention_masks, token_type_ids and targets (labels)\n",
    "    # add to a list\n",
    "    for i, idx in enumerate(idxs):\n",
    "        if i == 0:\n",
    "            input_ids = torch.cat((input_ids_first_overlap, remain[:idx]))\n",
    "        elif i == len(idxs):\n",
    "            input_ids = remain[idx:]\n",
    "        elif previous_idx >= len(remain):\n",
    "            break\n",
    "        else:\n",
    "            input_ids = remain[(previous_idx-overlap_len):idx]\n",
    "\n",
    "        previous_idx = idx\n",
    "\n",
    "        nb_token = len(input_ids)+2\n",
    "        attention_mask = torch.ones(chunk_len, dtype=torch.long)\n",
    "        attention_mask[nb_token:chunk_len] = 0\n",
    "        token_type_ids = torch.zeros(chunk_len, dtype=torch.long)\n",
    "        input_ids = torch.cat((start_token, input_ids, end_token))\n",
    "        if chunk_len-nb_token > 0:\n",
    "            padding = torch.zeros(chunk_len-nb_token, dtype=torch.long)\n",
    "            input_ids = torch.cat((input_ids, padding))\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        token_type_ids_list.append(token_type_ids)\n",
    "        targets_list.append(targets)\n",
    "    \n",
    "    # Add to the overlap list\n",
    "    input_ids_list_olap.append([input_ids_list])\n",
    "    attention_mask_list_olap.append([attention_mask_list])\n",
    "    token_type_ids_list_olap.append([token_type_ids_list])\n",
    "    targets_list_olap.append([targets_list])      \n",
    "  \n",
    "\n",
    "    # GET LISTS FOR THE HEAD\n",
    "    input_ids_list_tail.append([input_ids_list[-1]])\n",
    "    attention_mask_list_tail.append([attention_mask_list[-1]])\n",
    "    token_type_ids_list_tail.append([token_type_ids_list[-1]])\n",
    "    targets_list_tail.append([targets_list[-1]])\n",
    "    \n",
    "print(f'BERT Tokenize Runtime: {datetime.datetime.now() - start_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 803\n",
      "\n",
      "Head input_ids length: 803\n",
      "Head attention mask length: 803\n",
      "Head target length: 803\n",
      "\n",
      "Tail input_ids length: 803\n",
      "Tail attention mask length: 803\n",
      "Tail target length: 803\n",
      "\n",
      "Overlap input_ids length: 803\n",
      "Overlap attention mask length: 803\n",
      "Overlap target length: 803\n"
     ]
    }
   ],
   "source": [
    "# check we have the correct sizes of things (641)\n",
    "print(\"Train length:\", len(df_raw))\n",
    "print(\"\")\n",
    "print(\"Head input_ids length:\", len(input_ids_list_head))\n",
    "print(\"Head attention mask length:\", len(attention_mask_list_head))\n",
    "print(\"Head target length:\", len(targets_list_head))\n",
    "print(\"\")\n",
    "print(\"Tail input_ids length:\", len(input_ids_list_tail))\n",
    "print(\"Tail attention mask length:\", len(attention_mask_list_tail))\n",
    "print(\"Tail target length:\", len(targets_list_tail))\n",
    "print(\"\")\n",
    "print(\"Overlap input_ids length:\", len(input_ids_list_olap))\n",
    "print(\"Overlap attention mask length:\", len(attention_mask_list_olap))\n",
    "print(\"Overlap target length:\", len(targets_list_olap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lists so we can smash to a dataframe and save for reuse\n",
    "input_ids_np_head = []\n",
    "attention_mask_np_head = []\n",
    "\n",
    "input_ids_np_tail = []\n",
    "attention_mask_np_tail = []\n",
    "\n",
    "for i in range(len(input_ids_list_head)):\n",
    "    input_ids_np_head.append(input_ids_list_head[i].numpy())\n",
    "    attention_mask_np_head.append(attention_mask_list_head[i].numpy())\n",
    "    input_ids_np_tail.append(input_ids_list_tail[i][0].numpy())\n",
    "    attention_mask_np_tail.append(attention_mask_list_tail[i][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the lists to the dataframe for future use\n",
    "df_raw['input_ids_np_head'] = input_ids_np_head\n",
    "df_raw['input_ids_np_tail'] = input_ids_np_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start =  0  end =  100  id =  0\n",
      "BERT Model Runtime: 0:01:14.097850\n",
      "start =  100  end =  200  id =  1\n",
      "BERT Model Runtime: 0:01:10.939066\n",
      "start =  200  end =  300  id =  2\n",
      "BERT Model Runtime: 0:01:10.843425\n",
      "start =  300  end =  400  id =  3\n",
      "BERT Model Runtime: 0:01:10.951512\n",
      "start =  400  end =  500  id =  4\n",
      "BERT Model Runtime: 0:01:11.660280\n",
      "start =  500  end =  600  id =  5\n",
      "BERT Model Runtime: 0:01:12.639403\n",
      "start =  600  end =  700  id =  6\n",
      "BERT Model Runtime: 0:01:11.528518\n",
      "start =  700  end =  800  id =  7\n",
      "BERT Model Runtime: 0:01:12.460135\n",
      "start =  800  end =  900  id =  8\n",
      "BERT Model Runtime: 0:00:01.502017\n"
     ]
    }
   ],
   "source": [
    "# Model Head tokens to get the last_hidden_state\n",
    "# Do one row at a time because it crashes otherwise\n",
    "\n",
    "l_start = [0,100,200,300,400,500,600,700,800]\n",
    "l_end =   [100,200,300,400,500,600,700,800,900]\n",
    "l_id =    [0,1,2,3,4,5,6,7,8]\n",
    "\n",
    "l_head = []\n",
    "\n",
    "for i in range(len(l_start)):\n",
    "    \n",
    "    print(\"start = \", l_start[i], \" end = \", l_end[i], \" id = \", l_id[i])\n",
    "    \n",
    "    input_ids_np_run = input_ids_np_head[l_start[i]:l_end[i]]\n",
    "    attention_mask_np_run =  attention_mask_np_head[l_start[i]:l_end[i]]\n",
    "    \n",
    "    # Now we can run the model to get the Bert embedding\n",
    "    input_ids = torch.tensor(input_ids_np_run)\n",
    "    attention_mask = torch.tensor(attention_mask_np_run)\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "    with torch.no_grad(): #deactivates autograd engine\n",
    "        last_hidden_states = model1(input_ids, attention_mask=attention_mask)\n",
    "    print(f'BERT Model Runtime: {datetime.datetime.now() - start_time}')\n",
    "    \n",
    "    features = last_hidden_states[0][:,0,:].numpy()\n",
    "    l_head.append(features)\n",
    "    \n",
    "# Flatten out the ebeddngs - list. Now it is the same size as the overflow dimension\n",
    "l_head_flat = [item for sublist in l_head for item in sublist]\n",
    "df_raw['head_embeddings'] = l_head_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start =  0  end =  100  id =  0\n",
      "BERT Model Runtime: 0:01:08.344804\n",
      "start =  100  end =  200  id =  1\n",
      "BERT Model Runtime: 0:25:39.807769\n",
      "start =  200  end =  300  id =  2\n",
      "BERT Model Runtime: 0:01:08.830317\n",
      "start =  300  end =  400  id =  3\n",
      "BERT Model Runtime: 0:01:07.750817\n",
      "start =  400  end =  500  id =  4\n",
      "BERT Model Runtime: 0:01:07.766886\n",
      "start =  500  end =  600  id =  5\n",
      "BERT Model Runtime: 0:01:07.825672\n",
      "start =  600  end =  700  id =  6\n",
      "BERT Model Runtime: 0:01:08.403189\n",
      "start =  700  end =  800  id =  7\n",
      "BERT Model Runtime: 0:01:07.986288\n",
      "start =  800  end =  900  id =  8\n",
      "BERT Model Runtime: 0:00:01.406201\n"
     ]
    }
   ],
   "source": [
    "# Model Head tokens to get the last_hidden_state\n",
    "# Do one row at a time because it crashes otherwise\n",
    "\n",
    "l_start = [0,100,200,300,400,500,600,700,800]\n",
    "l_end =   [100,200,300,400,500,600,700,800,900]\n",
    "l_id =    [0,1,2,3,4,5,6,7,8]\n",
    "\n",
    "l_tail = []\n",
    "\n",
    "for i in range(len(l_start)):\n",
    "    \n",
    "    print(\"start = \", l_start[i], \" end = \", l_end[i], \" id = \", l_id[i])\n",
    "    \n",
    "    input_ids_np_run = input_ids_np_tail[l_start[i]:l_end[i]]\n",
    "    attention_mask_np_run =  attention_mask_np_tail[l_start[i]:l_end[i]]\n",
    "    \n",
    "    # Now we can run the model to get the Bert embedding\n",
    "    input_ids = torch.tensor(input_ids_np_run)\n",
    "    attention_mask = torch.tensor(attention_mask_np_run)\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "    with torch.no_grad(): #deactivates autograd engine\n",
    "        last_hidden_states = model1(input_ids, attention_mask=attention_mask)\n",
    "    print(f'BERT Model Runtime: {datetime.datetime.now() - start_time}')\n",
    "    \n",
    "    features = last_hidden_states[0][:,0,:].numpy()\n",
    "    l_tail.append(features)\n",
    "    \n",
    "# Flatten out the ebeddngs - list. Now it is the same size as the overflow dimension\n",
    "l_tail_flat = [item for sublist in l_tail for item in sublist]\n",
    "df_raw['tail_embeddings'] = l_tail_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to disk as pickle file\n",
    "#df_raw.to_csv(r'../data/distilBert_embeddings_all_200.csv', index = False)\n",
    "\n",
    "fname = '../data/baseBert_embeddings_headtail_' + str(chunk_len) + '.pkl'\n",
    "\n",
    "with open(fname, 'wb') as fp:\n",
    "    pickle.dump(df_raw, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlap STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lists so we can smash to a dataframe and save for reuse\n",
    "input_ids_l2_olap = []\n",
    "attention_mask_l2_olap = []\n",
    "\n",
    "for i in range(len(input_ids_list_olap)):\n",
    "    input_ids_l2_olap.append(input_ids_list_olap[i][0])\n",
    "    attention_mask_l2_olap.append(attention_mask_list_olap[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_use</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73277</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[[tensor(101), tensor(5650), tensor(122), tens...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79776</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[[tensor(101), tensor(5650), tensor(122), tens...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75870</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[[tensor(101), tensor(5650), tensor(122), tens...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79299</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[[tensor(101), tensor(5650), tensor(122), tens...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80603</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[[tensor(101), tensor(9228), tensor(1104), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>74009</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>[[tensor(101), tensor(9228), tensor(1103), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>79379</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>[[tensor(101), tensor(5650), tensor(2666), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>251317</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>[[tensor(101), tensor(5650), tensor(122), tens...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>79318</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>[[tensor(101), tensor(5650), tensor(122), tens...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>255439</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>[[tensor(101), tensor(5650), tensor(122), tens...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>803 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      docid  label doc_use                                          input_ids  \\\n",
       "0     73277      0   train  [[tensor(101), tensor(5650), tensor(122), tens...   \n",
       "1     79776      1   train  [[tensor(101), tensor(5650), tensor(122), tens...   \n",
       "2     75870      1   train  [[tensor(101), tensor(5650), tensor(122), tens...   \n",
       "3     79299      1   train  [[tensor(101), tensor(5650), tensor(122), tens...   \n",
       "4     80603      0   train  [[tensor(101), tensor(9228), tensor(1104), ten...   \n",
       "..      ...    ...     ...                                                ...   \n",
       "798   74009      0    test  [[tensor(101), tensor(9228), tensor(1103), ten...   \n",
       "799   79379      1    test  [[tensor(101), tensor(5650), tensor(2666), ten...   \n",
       "800  251317      1    test  [[tensor(101), tensor(5650), tensor(122), tens...   \n",
       "801   79318      1    test  [[tensor(101), tensor(5650), tensor(122), tens...   \n",
       "802  255439      1    test  [[tensor(101), tensor(5650), tensor(122), tens...   \n",
       "\n",
       "                                        attention_mask  \n",
       "0    [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "1    [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "2    [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "3    [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "4    [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "..                                                 ...  \n",
       "798  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "799  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "800  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "801  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "802  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "\n",
       "[803 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to dataframe\n",
    "df_olap = pd.DataFrame()\n",
    "df_olap['docid'] = df_raw['docid']\n",
    "df_olap['label'] = df_raw['label']\n",
    "df_olap['doc_use'] = df_raw['doc_use']\n",
    "df_olap['input_ids'] = input_ids_l2_olap\n",
    "df_olap['attention_mask'] = attention_mask_l2_olap\n",
    "df_olap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two new dataframes with exploded input_ids and attention_masks\n",
    "\n",
    "df_olap_explode = df_olap.explode('input_ids')\n",
    "df_olap_explode.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df_olap_explode_attention_mask = df_olap.explode('attention_mask')\n",
    "df_olap_explode_attention_mask.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get out two lists of numpy arrays so we can create models\n",
    "arr_input_ids = df_olap_explode['input_ids'].to_numpy()\n",
    "arr_attention_mask = df_olap_explode_attention_mask['attention_mask'].to_numpy()\n",
    "\n",
    "input_ids_np_olap = []\n",
    "attention_mask_np_olap = []\n",
    "\n",
    "for i in range(len(arr_input_ids)):\n",
    "    input_ids_np_olap.append(arr_input_ids[i].numpy())\n",
    "    attention_mask_np_olap.append(arr_attention_mask[i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the exploded tokens and add back to the dataframe in case we want them later\n",
    "df_olap_explode['input_ids_np_olap'] = input_ids_np_olap\n",
    "df_olap_explode['attention_mask_np_olap'] = attention_mask_np_olap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_start = list(range(0,3700,100))\n",
    "l_end =   list(range(100,3800,100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some start and end chunk sizes to do our BERT embeddings as we don't have enough memory to do at once\n",
    "l_start = list(range(0,3700,100))\n",
    "l_end =   list(range(100,3800,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start =  0  end =  100\n",
      "BERT Model Runtime: 0:01:08.010337\n",
      "start =  100  end =  200\n",
      "BERT Model Runtime: 0:01:11.065932\n",
      "start =  200  end =  300\n",
      "BERT Model Runtime: 0:01:08.695092\n",
      "start =  300  end =  400\n",
      "BERT Model Runtime: 0:01:08.444483\n",
      "start =  400  end =  500\n",
      "BERT Model Runtime: 0:01:08.260005\n",
      "start =  500  end =  600\n",
      "BERT Model Runtime: 0:01:08.360355\n",
      "start =  600  end =  700\n",
      "BERT Model Runtime: 0:01:08.527455\n",
      "start =  700  end =  800\n",
      "BERT Model Runtime: 0:01:08.333234\n",
      "start =  800  end =  900\n",
      "BERT Model Runtime: 0:01:08.500196\n",
      "start =  900  end =  1000\n",
      "BERT Model Runtime: 0:01:08.333071\n",
      "start =  1000  end =  1100\n",
      "BERT Model Runtime: 0:01:08.313147\n",
      "start =  1100  end =  1200\n",
      "BERT Model Runtime: 0:17:07.013473\n",
      "start =  1200  end =  1300\n",
      "BERT Model Runtime: 0:01:08.412440\n",
      "start =  1300  end =  1400\n",
      "BERT Model Runtime: 0:01:09.504785\n",
      "start =  1400  end =  1500\n",
      "BERT Model Runtime: 0:01:10.748542\n",
      "start =  1500  end =  1600\n",
      "BERT Model Runtime: 0:01:11.496825\n",
      "start =  1600  end =  1700\n",
      "BERT Model Runtime: 0:01:09.515666\n",
      "start =  1700  end =  1800\n",
      "BERT Model Runtime: 0:01:08.651096\n",
      "start =  1800  end =  1900\n",
      "BERT Model Runtime: 0:01:08.608926\n",
      "start =  1900  end =  2000\n",
      "BERT Model Runtime: 0:01:08.796478\n",
      "start =  2000  end =  2100\n",
      "BERT Model Runtime: 0:01:08.982809\n",
      "start =  2100  end =  2200\n",
      "BERT Model Runtime: 0:01:08.272005\n",
      "start =  2200  end =  2300\n",
      "BERT Model Runtime: 0:01:08.337438\n",
      "start =  2300  end =  2400\n",
      "BERT Model Runtime: 0:01:08.547477\n",
      "start =  2400  end =  2500\n",
      "BERT Model Runtime: 0:01:08.951412\n",
      "start =  2500  end =  2600\n",
      "BERT Model Runtime: 0:01:08.336015\n",
      "start =  2600  end =  2700\n",
      "BERT Model Runtime: 0:01:09.411798\n",
      "start =  2700  end =  2800\n",
      "BERT Model Runtime: 0:01:09.621401\n",
      "start =  2800  end =  2900\n",
      "BERT Model Runtime: 0:01:10.549997\n",
      "start =  2900  end =  3000\n",
      "BERT Model Runtime: 0:01:08.675860\n",
      "start =  3000  end =  3100\n",
      "BERT Model Runtime: 0:01:08.539340\n",
      "start =  3100  end =  3200\n",
      "BERT Model Runtime: 0:01:09.485021\n",
      "start =  3200  end =  3300\n",
      "BERT Model Runtime: 0:01:08.245557\n",
      "start =  3300  end =  3400\n",
      "BERT Model Runtime: 0:01:08.340390\n",
      "start =  3400  end =  3500\n",
      "BERT Model Runtime: 0:01:08.317601\n",
      "start =  3500  end =  3600\n",
      "BERT Model Runtime: 0:01:09.475150\n",
      "start =  3600  end =  3700\n",
      "BERT Model Runtime: 0:00:46.765068\n"
     ]
    }
   ],
   "source": [
    "# Create Bert embeddings on the exploded documents and save to a list\n",
    "l_olap = []\n",
    "\n",
    "for i in range(len(l_start)):\n",
    "    \n",
    "    print(\"start = \", l_start[i], \" end = \", l_end[i])\n",
    "    \n",
    "    input_ids_np_olap_run = input_ids_np_olap[l_start[i]:l_end[i]]\n",
    "    attention_mask_np_olap_run =  attention_mask_np_olap[l_start[i]:l_end[i]]\n",
    "    \n",
    "    # Now we can run the model to get the Bert embedding\n",
    "    input_ids = torch.tensor(input_ids_np_olap_run)\n",
    "    attention_mask = torch.tensor(attention_mask_np_olap_run)\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "    with torch.no_grad(): #deactivates autograd engine\n",
    "        last_hidden_states_olap = model1(input_ids, attention_mask=attention_mask)\n",
    "    print(f'BERT Model Runtime: {datetime.datetime.now() - start_time}')\n",
    "    \n",
    "    olap_features = last_hidden_states_olap[0][:,0,:].numpy()\n",
    "    l_olap.append(olap_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten out the ebeddngs - list. Now it is the same size as the overflow dimension\n",
    "l_olap_flat = [item for sublist in l_olap for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embeddings back to our dataframe\n",
    "df_olap_explode['embeddings'] = l_olap_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_olap_explode1 = df_olap_explode[['docid', 'label', 'doc_use','input_ids_np_olap', 'embeddings']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to disk as pickle file\n",
    "\n",
    "fname = '../data/baseBert_embeddings_olap_' + str(chunk_len) + '.pkl'\n",
    "\n",
    "with open(fname, 'wb') as fp:\n",
    "    pickle.dump(df_olap_explode1, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266",
   "language": "python",
   "name": "w266"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
