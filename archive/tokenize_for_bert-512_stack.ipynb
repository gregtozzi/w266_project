{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import transformers as ppb\n",
    "from transformers import LongformerTokenizer, LongformerModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(fname):\n",
    "    df = pd.read_csv(fname)\n",
    "    df = df[['docid', 'cleaned_contents', 'Discrimination_Label']]\n",
    "    df = df.rename(columns = {'cleaned_contents':'text', 'Discrimination_Label':'label'})\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>264935</td>\n",
       "      <td>SENTENCE\\n \\n \\n[1] Mr. Fuatia Monise (Accused...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>264885</td>\n",
       "      <td>S E N T E N C E\\n \\n \\nIntroduction\\n  \\n  \\n•...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>73669</td>\n",
       "      <td>JUDGMENT\\n\\n1. On 13 May 2008, the Appellant, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>281338</td>\n",
       "      <td>SENTENCE\\n \\n1. Imanueli Senikuba, you have be...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>256798</td>\n",
       "      <td>SENTENCE\\n \\n \\n[1] The accused has been convi...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>79711</td>\n",
       "      <td>JUDGMENT\\n\\n[1] The Appellant was sentenced to...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>82658</td>\n",
       "      <td>SENTENCE\\n\\n\\t1.\\tOn 5th November, 2013, in th...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>247031</td>\n",
       "      <td>SENTENCE\\n \\n  \\n• On 16 July 2018, the court ...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>71173</td>\n",
       "      <td>SENTENCE\\n\\n1. To prevent the identity of the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>264151</td>\n",
       "      <td>SENTENCE\\n \\n  \\n• Mr. SAMUELA TAWANANUMI, aft...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      docid                                               text  label doc_use\n",
       "383  264935  SENTENCE\\n \\n \\n[1] Mr. Fuatia Monise (Accused...      1   train\n",
       "157  264885  S E N T E N C E\\n \\n \\nIntroduction\\n  \\n  \\n•...      0   train\n",
       "649   73669  JUDGMENT\\n\\n1. On 13 May 2008, the Appellant, ...      1    test\n",
       "30   281338  SENTENCE\\n \\n1. Imanueli Senikuba, you have be...      0   train\n",
       "787  256798  SENTENCE\\n \\n \\n[1] The accused has been convi...      0    test\n",
       "499   79711  JUDGMENT\\n\\n[1] The Appellant was sentenced to...      1   train\n",
       "211   82658  SENTENCE\\n\\n\\t1.\\tOn 5th November, 2013, in th...      1   train\n",
       "271  247031  SENTENCE\\n \\n  \\n• On 16 July 2018, the court ...      1   train\n",
       "465   71173  SENTENCE\\n\\n1. To prevent the identity of the ...      1   train\n",
       "167  264151  SENTENCE\\n \\n  \\n• Mr. SAMUELA TAWANANUMI, aft...      0   train"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Variables\n",
    "chunk_len = 512\n",
    "overlap_len = 0 #50\n",
    "\n",
    "train_raw = get_data(\"data/train.csv\")\n",
    "test_raw = get_data(\"data/test.csv\")\n",
    "train_raw['doc_use'] = 'train'\n",
    "test_raw['doc_use'] = 'test'\n",
    "\n",
    "BERT_tokenizer_class = ppb.LongformerTokenizer\n",
    "BERT_pre_trained_weights = 'allenai/longformer-base-4096'\n",
    "tokenizer = BERT_tokenizer_class.from_pretrained(BERT_pre_trained_weights)\n",
    "model1 = ppb.LongformerModel.from_pretrained(BERT_pre_trained_weights)\n",
    "\n",
    "\n",
    "#distilBert\n",
    "#fname = 'data/embeddings/distilBert_stack_lcase_' + str(chunk_len) + '.pkl'\n",
    "#BERT_tokenizer_class = ppb.DistilBertTokenizer\n",
    "#BERT_pre_trained_weights = 'distilbert-base-cased'\n",
    "\n",
    "df_raw = pd.concat([train_raw, test_raw])\n",
    "df_raw.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df_raw.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Extract all the tokenize elements out of data_tokenize from the above tokenizer.encode_plus\n",
    "# gives us input ids, attention mask and critically overflow tokens\n",
    "\n",
    "def extract_tokens(data_tokenize, targets):\n",
    "\n",
    "    previous_input_ids = data_tokenize[\"input_ids\"].reshape(-1) # a tensor of the input IDs )\n",
    "    previous_attention_mask = data_tokenize[\"attention_mask\"].reshape(-1) # a tensor of the attention mask (200 * 1)\n",
    "    previous_token_type_ids = data_tokenize[\"token_type_ids\"].reshape(-1) # a tensor of the attention mask (200 * 0)\n",
    "    remain = data_tokenize.get(\"overflowing_tokens\") # list of the overflow tokens\n",
    "    # remain = data_tokenize.get(\"overflowing_tokens\").numpy()[0] # list of the overflow tokens for google cloud\n",
    "    targets = torch.tensor(targets, dtype=torch.int) # a tensor of current target (1)\n",
    "\n",
    "    return previous_input_ids, previous_attention_mask, previous_token_type_ids, remain, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Tokenize Runtime: 0:00:18.159872\n"
     ]
    }
   ],
   "source": [
    "# Do the tokenization\n",
    "# This returns a transformers object with 5 elements\n",
    "# We only really need the input_ids and attention mask for modelling\n",
    "# We will use these IDS to get out embeddings\n",
    "\n",
    "# overflowing_tokens (list) - all the elements after our 200 word split\n",
    "# num_truncated_tokens (integer) - how many overflow tokens we have, for text[0] it is 1822\n",
    "# input_ids (tensor) - the first 200 tokens, with special token 101 at the beginning and 102 at end\n",
    "# token_type_ids (tensor) - the token types for the input - there are 200, ours are all zero's (why?)\n",
    "# attention_mask (tensor) - attention mask in case our text < 200 tokens\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "input_ids_list_head = []\n",
    "attention_mask_list_head = []\n",
    "token_type_ids_list_head = []\n",
    "targets_list_head = []\n",
    "\n",
    "input_ids_list_olap = []\n",
    "attention_mask_list_olap = []\n",
    "token_type_ids_list_olap= []\n",
    "targets_list_olap= []\n",
    "\n",
    "input_ids_list_tail = []\n",
    "attention_mask_list_tail = []\n",
    "token_type_ids_list_tail = []\n",
    "targets_list_tail= []\n",
    "\n",
    "for idx in range(len(df_raw)): \n",
    "    \n",
    "    long_terms_token = []\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    token_type_ids_list = []\n",
    "    targets_list = []\n",
    "        \n",
    "    # tokenize for this row in train_raw\n",
    "    data = tokenizer.encode_plus(\n",
    "        df_raw['text'][idx],\n",
    "        max_length=chunk_len,\n",
    "        pad_to_max_length=True,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_tensors='pt')\n",
    "\n",
    "    # grab the targets for this row in train_raw\n",
    "    targets = int(df_raw['label'][idx])\n",
    "    \n",
    "    # extract the tokens\n",
    "    input_ids, attention_mask, token_type_ids, remain, targets = extract_tokens(data, targets)\n",
    "    remain = [] if remain is None else remain # For cases where there is no overflow\n",
    "    \n",
    "    # CREATE LISTS FOR THE HEAD\n",
    "    input_ids_list_head.append(input_ids)\n",
    "    attention_mask_list_head.append(attention_mask)\n",
    "    token_type_ids_list_head.append(token_type_ids)\n",
    "    targets_list_head.append(targets)\n",
    "    \n",
    "    # GET OVERLAPPING TOKEN LISTS *****************************\n",
    "    remain = torch.tensor(remain, dtype=torch.long)\n",
    "    idxs = range(len(remain)+ chunk_len)\n",
    "    idxs = idxs[(chunk_len-overlap_len-2)::(chunk_len-overlap_len-2)]\n",
    "    input_ids_first_overlap = input_ids[-(overlap_len+1):-1]\n",
    "    start_token = torch.tensor([101], dtype=torch.long)\n",
    "    end_token = torch.tensor([102], dtype=torch.long)\n",
    "    \n",
    "    # Get the initial 200 word tensors (same as head)\n",
    "    input_ids_list.append(input_ids)\n",
    "    attention_mask_list.append(attention_mask)\n",
    "    token_type_ids_list.append(token_type_ids)\n",
    "    targets_list.append(targets)\n",
    "    \n",
    "    # For each overlapping section create a tensor of input_ids, attention_masks, token_type_ids and targets (labels)\n",
    "    # add to a list\n",
    "    for i, idx in enumerate(idxs):\n",
    "        if i == 0:\n",
    "            input_ids = torch.cat((input_ids_first_overlap, remain[:idx]))\n",
    "        elif i == len(idxs):\n",
    "            input_ids = remain[idx:]\n",
    "        elif previous_idx >= len(remain):\n",
    "            break\n",
    "        else:\n",
    "            input_ids = remain[(previous_idx-overlap_len):idx]\n",
    "\n",
    "        previous_idx = idx\n",
    "\n",
    "        nb_token = len(input_ids)+2\n",
    "        attention_mask = torch.ones(chunk_len, dtype=torch.long)\n",
    "        attention_mask[nb_token:chunk_len] = 0\n",
    "        token_type_ids = torch.zeros(chunk_len, dtype=torch.long)\n",
    "        input_ids = torch.cat((start_token, input_ids, end_token))\n",
    "        if chunk_len-nb_token > 0:\n",
    "            padding = torch.zeros(chunk_len-nb_token, dtype=torch.long)\n",
    "            input_ids = torch.cat((input_ids, padding))\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        token_type_ids_list.append(token_type_ids)\n",
    "        targets_list.append(targets)\n",
    "    \n",
    "    # Add to the overlap list\n",
    "    input_ids_list_olap.append([input_ids_list])\n",
    "    attention_mask_list_olap.append([attention_mask_list])\n",
    "    token_type_ids_list_olap.append([token_type_ids_list])\n",
    "    targets_list_olap.append([targets_list])      \n",
    "  \n",
    "\n",
    "    # GET LISTS FOR THE HEAD\n",
    "    input_ids_list_tail.append([input_ids_list[-1]])\n",
    "    attention_mask_list_tail.append([attention_mask_list[-1]])\n",
    "    token_type_ids_list_tail.append([token_type_ids_list[-1]])\n",
    "    targets_list_tail.append([targets_list[-1]])\n",
    "    \n",
    "print(f'BERT Tokenize Runtime: {datetime.datetime.now() - start_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 809\n",
      "\n",
      "Head input_ids length: 809\n",
      "Head attention mask length: 809\n",
      "Head target length: 809\n",
      "\n",
      "Tail input_ids length: 809\n",
      "Tail attention mask length: 809\n",
      "Tail target length: 809\n",
      "\n",
      "Overlap input_ids length: 809\n",
      "Overlap attention mask length: 809\n",
      "Overlap target length: 809\n"
     ]
    }
   ],
   "source": [
    "# check we have the correct sizes of things (803)\n",
    "print(\"Train length:\", len(df_raw))\n",
    "print(\"\")\n",
    "print(\"Head input_ids length:\", len(input_ids_list_head))\n",
    "print(\"Head attention mask length:\", len(attention_mask_list_head))\n",
    "print(\"Head target length:\", len(targets_list_head))\n",
    "print(\"\")\n",
    "print(\"Tail input_ids length:\", len(input_ids_list_tail))\n",
    "print(\"Tail attention mask length:\", len(attention_mask_list_tail))\n",
    "print(\"Tail target length:\", len(targets_list_tail))\n",
    "print(\"\")\n",
    "print(\"Overlap input_ids length:\", len(input_ids_list_olap))\n",
    "print(\"Overlap attention mask length:\", len(attention_mask_list_olap))\n",
    "print(\"Overlap target length:\", len(targets_list_olap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlap STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lists so we can smash to a dataframe and save for reuse\n",
    "input_ids_l2_olap = []\n",
    "attention_mask_l2_olap = []\n",
    "\n",
    "for i in range(len(input_ids_list_olap)):\n",
    "    input_ids_l2_olap.append(input_ids_list_olap[i][0])\n",
    "    attention_mask_l2_olap.append(attention_mask_list_olap[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_use</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73277</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[[tensor(0), tensor(208), tensor(5382), tensor...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79776</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[[tensor(0), tensor(208), tensor(5382), tensor...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75870</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[[tensor(0), tensor(208), tensor(5382), tensor...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79299</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[[tensor(0), tensor(208), tensor(5382), tensor...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80603</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[[tensor(0), tensor(344), tensor(13083), tenso...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>74009</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>[[tensor(0), tensor(344), tensor(13083), tenso...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>79379</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>[[tensor(0), tensor(208), tensor(5382), tensor...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>251317</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>[[tensor(0), tensor(208), tensor(5382), tensor...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>79318</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>[[tensor(0), tensor(208), tensor(5382), tensor...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>255439</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>[[tensor(0), tensor(208), tensor(5382), tensor...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>809 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      docid  label doc_use                                          input_ids  \\\n",
       "0     73277      0   train  [[tensor(0), tensor(208), tensor(5382), tensor...   \n",
       "1     79776      1   train  [[tensor(0), tensor(208), tensor(5382), tensor...   \n",
       "2     75870      1   train  [[tensor(0), tensor(208), tensor(5382), tensor...   \n",
       "3     79299      1   train  [[tensor(0), tensor(208), tensor(5382), tensor...   \n",
       "4     80603      0   train  [[tensor(0), tensor(344), tensor(13083), tenso...   \n",
       "..      ...    ...     ...                                                ...   \n",
       "804   74009      0    test  [[tensor(0), tensor(344), tensor(13083), tenso...   \n",
       "805   79379      1    test  [[tensor(0), tensor(208), tensor(5382), tensor...   \n",
       "806  251317      1    test  [[tensor(0), tensor(208), tensor(5382), tensor...   \n",
       "807   79318      1    test  [[tensor(0), tensor(208), tensor(5382), tensor...   \n",
       "808  255439      1    test  [[tensor(0), tensor(208), tensor(5382), tensor...   \n",
       "\n",
       "                                        attention_mask  \n",
       "0    [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "1    [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "2    [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "3    [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "4    [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "..                                                 ...  \n",
       "804  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "805  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "806  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "807  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "808  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "\n",
       "[809 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to dataframe for saving later\n",
    "df_olap = pd.DataFrame()\n",
    "df_olap['docid'] = df_raw['docid']\n",
    "df_olap['label'] = df_raw['label']\n",
    "df_olap['doc_use'] = df_raw['doc_use']\n",
    "df_olap['input_ids'] = input_ids_l2_olap\n",
    "df_olap['attention_mask'] = attention_mask_l2_olap\n",
    "df_olap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two new dataframes with exploded input_ids and attention_masks\n",
    "\n",
    "df_olap_explode = df_olap.explode('input_ids')\n",
    "df_olap_explode.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df_olap_explode_attention_mask = df_olap.explode('attention_mask')\n",
    "df_olap_explode_attention_mask.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get out two lists of numpy arrays so we can create models\n",
    "arr_input_ids = df_olap_explode['input_ids'].to_numpy()\n",
    "arr_attention_mask = df_olap_explode_attention_mask['attention_mask'].to_numpy()\n",
    "\n",
    "input_ids_np_olap = []\n",
    "attention_mask_np_olap = []\n",
    "\n",
    "for i in range(len(arr_input_ids)):\n",
    "    input_ids_np_olap.append(arr_input_ids[i].numpy())\n",
    "    attention_mask_np_olap.append(arr_attention_mask[i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the exploded tokens and add back to the dataframe in case we want them later\n",
    "df_olap_explode['input_ids_np_olap'] = input_ids_np_olap\n",
    "df_olap_explode['attention_mask_np_olap'] = attention_mask_np_olap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some start and end chunk sizes to do our BERT embeddings as we don't have enough memory to do at once\n",
    "\n",
    "interval = 100\n",
    "end_max_range = len(df_olap_explode['input_ids_np_olap']) - len(df_olap_explode['input_ids_np_olap']) % -interval\n",
    "start_max_range = end_max_range - interval\n",
    "\n",
    "l_start  = list(range(0, start_max_range + interval, interval))\n",
    "l_end  = list(range(interval, end_max_range + interval, interval ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start =  0  end =  100\n",
      "BERT Model Runtime: 0:01:53.046787\n",
      "start =  100  end =  200\n",
      "BERT Model Runtime: 0:01:51.227889\n",
      "start =  200  end =  300\n",
      "BERT Model Runtime: 0:01:51.479214\n",
      "start =  300  end =  400\n",
      "BERT Model Runtime: 0:01:51.501168\n",
      "start =  400  end =  500\n",
      "BERT Model Runtime: 0:01:51.260638\n",
      "start =  500  end =  600\n",
      "BERT Model Runtime: 0:35:22.192533\n",
      "start =  600  end =  700\n",
      "BERT Model Runtime: 0:01:57.908175\n",
      "start =  700  end =  800\n",
      "BERT Model Runtime: 0:01:52.196988\n",
      "start =  800  end =  900\n",
      "BERT Model Runtime: 0:01:52.474836\n",
      "start =  900  end =  1000\n",
      "BERT Model Runtime: 0:01:52.453293\n",
      "start =  1000  end =  1100\n",
      "BERT Model Runtime: 0:01:52.532297\n",
      "start =  1100  end =  1200\n",
      "BERT Model Runtime: 0:01:54.086203\n",
      "start =  1200  end =  1300\n",
      "BERT Model Runtime: 0:01:52.904174\n",
      "start =  1300  end =  1400\n",
      "BERT Model Runtime: 0:01:52.896357\n",
      "start =  1400  end =  1500\n",
      "BERT Model Runtime: 0:01:52.944171\n",
      "start =  1500  end =  1600\n",
      "BERT Model Runtime: 0:01:52.862902\n",
      "start =  1600  end =  1700\n",
      "BERT Model Runtime: 0:25:43.939710\n",
      "start =  1700  end =  1800\n",
      "BERT Model Runtime: 0:01:52.682057\n",
      "start =  1800  end =  1900\n",
      "BERT Model Runtime: 0:01:53.804958\n",
      "start =  1900  end =  2000\n",
      "BERT Model Runtime: 0:01:54.103462\n",
      "start =  2000  end =  2100\n",
      "BERT Model Runtime: 0:01:54.484242\n",
      "start =  2100  end =  2200\n",
      "BERT Model Runtime: 0:01:54.361574\n",
      "start =  2200  end =  2300\n",
      "BERT Model Runtime: 0:40:18.488878\n",
      "start =  2300  end =  2400\n",
      "BERT Model Runtime: 0:02:11.854813\n",
      "start =  2400  end =  2500\n",
      "BERT Model Runtime: 0:01:52.063481\n",
      "start =  2500  end =  2600\n",
      "BERT Model Runtime: 0:01:52.442897\n",
      "start =  2600  end =  2700\n",
      "BERT Model Runtime: 0:01:54.771643\n",
      "start =  2700  end =  2800\n",
      "BERT Model Runtime: 0:01:52.794226\n",
      "start =  2800  end =  2900\n",
      "BERT Model Runtime: 0:01:52.614892\n",
      "start =  2900  end =  3000\n",
      "BERT Model Runtime: 0:01:53.091704\n",
      "start =  3000  end =  3100\n",
      "BERT Model Runtime: 0:01:52.954742\n",
      "start =  3100  end =  3200\n",
      "BERT Model Runtime: 0:01:52.633450\n",
      "start =  3200  end =  3300\n",
      "BERT Model Runtime: 0:01:52.658152\n",
      "start =  3300  end =  3400\n",
      "BERT Model Runtime: 0:01:52.773539\n",
      "start =  3400  end =  3500\n",
      "BERT Model Runtime: 0:01:52.768264\n",
      "start =  3500  end =  3600\n",
      "BERT Model Runtime: 0:01:54.191103\n",
      "start =  3600  end =  3700\n",
      "BERT Model Runtime: 0:00:46.397352\n"
     ]
    }
   ],
   "source": [
    "# Create Bert embeddings on the exploded documents and save to a list\n",
    "l_olap = []\n",
    "\n",
    "for i in range(len(l_end)):\n",
    "    \n",
    "    print(\"start = \", l_start[i], \" end = \", l_end[i])\n",
    "    \n",
    "    input_ids_np_olap_run = input_ids_np_olap[l_start[i]:l_end[i]]\n",
    "    attention_mask_np_olap_run =  attention_mask_np_olap[l_start[i]:l_end[i]]\n",
    "    \n",
    "    # Now we can run the model to get the Bert embedding\n",
    "    input_ids = torch.tensor(input_ids_np_olap_run)\n",
    "    attention_mask = torch.tensor(attention_mask_np_olap_run)\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "    with torch.no_grad(): #deactivates autograd engine\n",
    "        last_hidden_states_olap = model1(input_ids, attention_mask=attention_mask)\n",
    "    print(f'BERT Model Runtime: {datetime.datetime.now() - start_time}')\n",
    "    \n",
    "    olap_features = last_hidden_states_olap[0][:,0,:].numpy()\n",
    "    l_olap.append(olap_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten out the ebeddngs - list. Now it is the same size as the overflow dimension\n",
    "l_olap_flat = [item for sublist in l_olap for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embeddings back to our dataframe\n",
    "df_olap_explode['embeddings'] = l_olap_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_olap_explode1 = df_olap_explode[['docid', 'label', 'doc_use','input_ids_np_olap', 'embeddings']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to disk as pickle file\n",
    "fname = 'data/embeddings/longformer_stack_lcase_' + str(chunk_len) + '.pkl'\n",
    "with open(fname, 'wb') as fp:\n",
    "    pickle.dump(df_olap_explode1, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_use</th>\n",
       "      <th>input_ids_np_olap</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73277</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[0, 208, 5382, 13471, 50140, 50117, 134, 4, 50...</td>\n",
       "      <td>[-0.03490465, 0.064865395, -0.017987669, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73277</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[101, 5505, 4726, 9304, 8, 172, 24557, 39, 235...</td>\n",
       "      <td>[-0.100065574, 0.022632826, 0.027329236, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73277</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[101, 4, 50140, 50117, 306, 4, 50117, 1121, 10...</td>\n",
       "      <td>[-0.041281126, 0.08823079, -0.007453665, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73277</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[101, 50117, 41562, 5, 1065, 6, 38, 19403, 110...</td>\n",
       "      <td>[-0.07864474, 0.06377331, -0.023088403, -0.045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73277</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[101, 746, 95, 7, 192, 549, 24, 1326, 1593, 23...</td>\n",
       "      <td>[-0.06478642, 0.09257552, -0.008796655, -0.075...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3639</th>\n",
       "      <td>79318</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>[0, 208, 5382, 13471, 50118, 50118, 10975, 134...</td>\n",
       "      <td>[-0.10326244, 0.022347506, 0.035206728, -0.061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3640</th>\n",
       "      <td>79318</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>[101, 16, 276, 25, 11, 5, 29643, 31486, 8302, ...</td>\n",
       "      <td>[-0.07819697, 0.04468355, -0.004176976, -0.078...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3641</th>\n",
       "      <td>79318</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>[101, 14804, 7, 4227, 5, 31904, 3724, 8, 5, 63...</td>\n",
       "      <td>[-0.07878052, 0.07486821, 0.0101483315, -0.024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3642</th>\n",
       "      <td>255439</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>[0, 208, 5382, 13471, 50118, 1437, 50118, 1437...</td>\n",
       "      <td>[-0.0740111, 0.0101187825, 0.015575906, -0.059...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3643</th>\n",
       "      <td>255439</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>[101, 20, 29223, 1295, 2433, 32, 35, 50118, 14...</td>\n",
       "      <td>[-0.12289338, 0.030791473, 0.019904718, 0.0086...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3644 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       docid  label doc_use  \\\n",
       "0      73277      0   train   \n",
       "1      73277      0   train   \n",
       "2      73277      0   train   \n",
       "3      73277      0   train   \n",
       "4      73277      0   train   \n",
       "...      ...    ...     ...   \n",
       "3639   79318      1    test   \n",
       "3640   79318      1    test   \n",
       "3641   79318      1    test   \n",
       "3642  255439      1    test   \n",
       "3643  255439      1    test   \n",
       "\n",
       "                                      input_ids_np_olap  \\\n",
       "0     [0, 208, 5382, 13471, 50140, 50117, 134, 4, 50...   \n",
       "1     [101, 5505, 4726, 9304, 8, 172, 24557, 39, 235...   \n",
       "2     [101, 4, 50140, 50117, 306, 4, 50117, 1121, 10...   \n",
       "3     [101, 50117, 41562, 5, 1065, 6, 38, 19403, 110...   \n",
       "4     [101, 746, 95, 7, 192, 549, 24, 1326, 1593, 23...   \n",
       "...                                                 ...   \n",
       "3639  [0, 208, 5382, 13471, 50118, 50118, 10975, 134...   \n",
       "3640  [101, 16, 276, 25, 11, 5, 29643, 31486, 8302, ...   \n",
       "3641  [101, 14804, 7, 4227, 5, 31904, 3724, 8, 5, 63...   \n",
       "3642  [0, 208, 5382, 13471, 50118, 1437, 50118, 1437...   \n",
       "3643  [101, 20, 29223, 1295, 2433, 32, 35, 50118, 14...   \n",
       "\n",
       "                                             embeddings  \n",
       "0     [-0.03490465, 0.064865395, -0.017987669, -0.03...  \n",
       "1     [-0.100065574, 0.022632826, 0.027329236, -0.07...  \n",
       "2     [-0.041281126, 0.08823079, -0.007453665, -0.07...  \n",
       "3     [-0.07864474, 0.06377331, -0.023088403, -0.045...  \n",
       "4     [-0.06478642, 0.09257552, -0.008796655, -0.075...  \n",
       "...                                                 ...  \n",
       "3639  [-0.10326244, 0.022347506, 0.035206728, -0.061...  \n",
       "3640  [-0.07819697, 0.04468355, -0.004176976, -0.078...  \n",
       "3641  [-0.07878052, 0.07486821, 0.0101483315, -0.024...  \n",
       "3642  [-0.0740111, 0.0101187825, 0.015575906, -0.059...  \n",
       "3643  [-0.12289338, 0.030791473, 0.019904718, 0.0086...  \n",
       "\n",
       "[3644 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_olap_explode1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266",
   "language": "python",
   "name": "w266"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
