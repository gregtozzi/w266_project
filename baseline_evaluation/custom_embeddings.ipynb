{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(doc, top_start=8, short_block=60,\n",
    "                strip_line_nos=True,\n",
    "                reduce_times=True, reduce_dates=True,\n",
    "                legal_abbr=True, min_sent_len=6):\n",
    "    \"\"\"\n",
    "    Generic document processor.\n",
    "    \n",
    "    Args: doc -> str: A document expressed in a single\n",
    "                 string\n",
    "                 \n",
    "          top_start -> int: The first carriage return-\n",
    "                    delimited block of text to consider\n",
    "                      \n",
    "          short_block -> int: Length that blocks of text\n",
    "                         in the document must be longer\n",
    "                         than to be retained\n",
    "                         \n",
    "          min_sent_len -> int: Minimum sentence lenth in\n",
    "                       characters\n",
    "                              \n",
    "          <flag> -> bool: A series of largely self-\n",
    "                 explanitary Boolean flags\n",
    "                 \n",
    "    Returns -> str: Single string containing the processed text\n",
    "    \n",
    "    Notes: 1. A lot of this is probably getting done in subsequent\n",
    "              calls to functions in the gensim library.\n",
    "    \n",
    "    TODO:\n",
    "           DO: Review stop words and consider removing others that\n",
    "               express negation.\n",
    "               \n",
    "           DO: Review how gensim processes text more closely. It\n",
    "               is likely that the numbers I'm removing get cut\n",
    "               during the creation of the embeddings.\n",
    "               \n",
    "           DO: Consider the effectiveness of the legal tokens.\n",
    "               They frequently preserve trailing numbers.  This\n",
    "               May be useful to preserve key citations, or it\n",
    "               may confuse things.\n",
    "               \n",
    "           MAYBE: Tokenize dollar amounts as 'sumtoken'.\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    doc = doc.lower()\n",
    "    \n",
    "    # Break document into carriage return-delimited blocks\n",
    "    blocks = doc.split('\\n')[top_start:]\n",
    "    long_blocks = [block for block in blocks if len(block) > short_block][:-1]\n",
    "    text = ' '.join(long_blocks)\n",
    "    \n",
    "    # Remove a host of line and paragraph numbers.\n",
    "    if strip_line_nos:\n",
    "        text = re.sub(\"\\[[0-9]\\]\", \" \", text)\n",
    "        text = re.sub(\"\\[[0-9]\\}\", \" \", text)\n",
    "        text = re.sub(\"\\[[0-9][0-9]\\]\", \" \", text)\n",
    "        text = re.sub(\" [a-z]\\) \", \" \", text)\n",
    "        text = re.sub(\" [0-9]. \", \" \", text)\n",
    "\n",
    "    # Reduce times to a '[TIME]' token\n",
    "    if reduce_times:\n",
    "        text = re.sub(\"[0-9].[0-9][0-9]pm\", \"timetoken\", text)\n",
    "        text = re.sub(\"[0-9].[0-9][0-9]am\", \"timetoken\", text)\n",
    "        \n",
    "    # Reduce dates to a '[DATE]' token\n",
    "    if reduce_dates:\n",
    "        text = re.sub(r'(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)([\\s]{1,3})?([0-9]{1,2})(.{1,3})?((,)|(.))?([\\s]{1,3})?([0-9]{4})|([0-9]{1,2})(.{1,3})?([\\s]{1,3})?(day)?([\\s]{1,3})?(of)?([\\s]{1,3})?(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)((,)|(.))?(\\s{1,3})?([0-9]{4})|(first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth|eleventh|twelfth|thirteenth|fourteenth|fifteenth|sixteenth|seventeenth|eighteenth|nineteenth|twentieth|twenty-first|twenty-second|twenty-third|twenty-fourth|twenty-fifth|twenty-sixth|twenty-seventh|twenty-eighth|twenty-ninth|thirtieth|thirty-first)([\\s]{1,3})?(day)?([\\s]{1,3})?(of)?([\\s]{1,3})?(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(t)?(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)((,)|(.))?(\\s{1,3})?([0-9]{4})|(\\b[0-9]{1,2}(\\-|\\/)[0-9]{1,2}(\\-|\\/)[0-9]{2,4}\\b)|(\\b[0-9]{2,4}(\\-|\\/)[0-9]{1,2}(\\-|\\/)[0-9]{1,2}\\b)', 'datetoken', text)\n",
    "\n",
    "    # Handle some specific legal abbreviations\n",
    "    if legal_abbr:\n",
    "        text = re.sub(\"Cap\\.\", \"capabbr\", text)\n",
    "        text = re.sub(\"Crim\\.\", \"crimabbr\", text)\n",
    "        text = re.sub(\"App\\.\", \"appabbr\", text)\n",
    "        text = re.sub(\"No\\.\", \"number\", text)\n",
    "    \n",
    "    # Get rid of tabs\n",
    "    text = re.sub(\"\\t\", \" \", text)\n",
    "\n",
    "    # Reduce extraneous whitespace to a single space\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Remove short headers and the like\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    sentences = [(\"startsent \" + x + \" endsent\") for x in sentences if len(x) > min_sent_len]\n",
    "\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "\n",
    "def process_corpus(corpus_path, doc_col='contents',\n",
    "                   exclude_test=True, test_path=None,\n",
    "                   rm_stopwords=True):\n",
    "    \"\"\"\n",
    "    Applies `process_doc` to a corpus contained in a csv.\n",
    "    \n",
    "    Args: corpus_path -> str\n",
    "    \n",
    "          doc_col -> str: The column in the csv that\n",
    "                  contains the documents\n",
    "                 \n",
    "    Returns -> list: List of processed documents\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(corpus_path)\n",
    "    \n",
    "    if exclude_test:\n",
    "        print(len(data))\n",
    "        test_data = pd.read_csv(test_path)\n",
    "        print(len(test_data))\n",
    "        data = data[~data.docid.isin(test_data.docid)]\n",
    "        \n",
    "    corpus = data[doc_col]\n",
    "    print('Corpus length - ' + str(len(corpus)))\n",
    "    \n",
    "    # Remove stop words using NLTK's English list\n",
    "    if rm_stopwords:\n",
    "        documents = []\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        # Unfortunately, NLTK's list includes 'no'.\n",
    "        # Exclude this to preserve negation.\n",
    "        stop_words.remove('no')\n",
    "        for doc in corpus:\n",
    "            processed_doc = utils.simple_preprocess(process_doc(doc))\n",
    "            keep_words = []\n",
    "            for word in processed_doc:\n",
    "                if word not in stop_words:\n",
    "                    keep_words.append(word)\n",
    "            if len(doc) > 0:\n",
    "                documents.append(keep_words)\n",
    "    else:\n",
    "        documents = [utils.simple_preprocess(process_doc(doc)) for doc in corpus]\n",
    "            \n",
    "    return documents\n",
    "\n",
    "\n",
    "def build_model(corpus, dims=300, workers=8):\n",
    "    WV = Word2Vec(sentences=corpus, size=dims, workers=8)\n",
    "    vocab = WV.wv.index2word\n",
    "    vocab_len = len(vocab)\n",
    "    embeddings = np.array([WV.wv.get_vector(word) for word in vocab])\n",
    "    text_to_token = {word: i for word, i in zip(vocab, range(vocab_len))}\n",
    "    token_to_text = {i: word for word, i in zip(vocab, range(vocab_len))}\n",
    "    model = {'embeddings': embeddings,\n",
    "             'text_to_token': text_to_token,\n",
    "             'token_to_text': token_to_text,\n",
    "             'vocab': vocab,\n",
    "             'vocab_len': len(vocab)}\n",
    "    return model\n",
    "\n",
    "    \n",
    "def tokenize_doc(doc, model):\n",
    "    tokens = []\n",
    "    for word in doc:\n",
    "        try:\n",
    "            tokens.append(model['text_to_token'][word])\n",
    "        except:\n",
    "            pass\n",
    "    return(tokens)\n",
    "\n",
    "\n",
    "def embed_tokens(doc, model):\n",
    "    tokenized_doc = tokenize_doc(doc, model)\n",
    "    embeddings = []\n",
    "    for token in tokenized_doc:\n",
    "        embeddings.append(model['embeddings'][token])\n",
    "    return np.stack(embeddings)\n",
    "    \n",
    "    \n",
    "def embed_doc(doc, model):\n",
    "    embeddings = embed_tokens(doc, model)\n",
    "    return embeddings.mean(axis=0)\n",
    "\n",
    "\n",
    "def embed_corpus(corpus, model):\n",
    "    embedded_corpus = [embed_doc(doc, model) for doc in corpus if len(doc) > 0]\n",
    "    return np.stack(embedded_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13384\n",
      "162\n",
      "Corpus length - 13292\n",
      "CPU times: user 3min 46s, sys: 780 ms, total: 3min 46s\n",
      "Wall time: 3min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus = process_corpus(corpus_path='../data/ICAAD_FIJI.csv', test_path='../data/test.csv',\n",
    "                        doc_col='contents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = build_model(corpus, 300, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33300, 300)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo['embeddings'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4861"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo['text_to_token']['bread']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bread = foo['embeddings'][4861,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4425457 ,  0.16182797, -1.0246683 ,  0.631806  , -0.8348473 ,\n",
       "       -0.46839136,  0.5780923 , -0.20096205, -0.46400702,  0.2968429 ,\n",
       "       -0.4325057 ,  0.6767891 , -0.03729293,  0.9588748 , -0.0384476 ,\n",
       "       -0.01007176,  0.33341667, -1.7993168 , -1.8348106 ,  0.5617358 ,\n",
       "       -0.24403776, -0.01262853,  0.33668387, -0.08049419,  0.04921189,\n",
       "       -1.7788202 , -0.73616195,  0.24110165,  0.81963897,  0.7799278 ,\n",
       "       -0.66614276,  0.16142029,  0.38969666, -0.27847752,  0.26986897,\n",
       "        0.33329672,  1.2911944 , -1.0098555 ,  0.4762035 ,  0.1025757 ,\n",
       "        0.29415452, -0.64394087, -0.29753718,  0.04163301, -0.295754  ,\n",
       "        0.1606689 , -0.6477798 ,  0.31304285, -0.25657564, -0.9719063 ,\n",
       "       -0.31546354, -0.5486953 , -0.28487316, -0.06243191,  0.16549812,\n",
       "        0.12494312, -0.03370988,  0.15589952,  0.02093038, -0.27809498,\n",
       "        0.1289665 , -0.8324646 ,  0.3968013 , -0.16357818,  0.26121345,\n",
       "       -0.68512166, -0.31802827, -0.28400838,  0.17055993,  0.36001065,\n",
       "       -0.05512115, -0.82069033, -0.19146699, -0.53127766,  0.87855434,\n",
       "        0.29749477, -0.18805577,  0.48821706,  0.20925224,  0.32959387,\n",
       "       -0.6608475 ,  0.9281449 ,  0.5441508 ,  0.4263035 ,  0.10906865,\n",
       "       -0.49461403, -0.10897716,  0.02600689, -0.6074425 , -0.34889683,\n",
       "       -0.10183998,  0.0218358 , -0.5370726 , -0.20698495, -0.18291868,\n",
       "       -0.63925844, -0.0621841 ,  1.3599844 , -0.3389076 , -0.32275537,\n",
       "        0.35093758,  0.30012444,  0.63669735,  0.17969653,  0.46246347,\n",
       "        0.70035887, -0.5354128 , -0.3461737 ,  0.0919925 ,  0.74070907,\n",
       "        0.05700295,  0.8717614 , -0.36094114,  0.49924204,  0.2964137 ,\n",
       "       -0.23636761,  0.07237674,  0.4085311 ,  0.16153666,  0.2731012 ,\n",
       "       -0.55813974, -0.12888192, -0.27707374, -0.61713827,  0.4783576 ,\n",
       "       -0.4435737 ,  0.5293008 , -0.30426288, -0.36280245,  0.42234448,\n",
       "       -0.52933484, -0.34617326,  0.3478494 ,  0.9848579 ,  0.26641878,\n",
       "        0.05158794, -0.3338913 ,  0.13170466,  0.39654604, -0.10139339,\n",
       "       -0.3243284 , -0.24630345,  0.81960595, -0.0455785 , -0.19349298,\n",
       "        0.02154917, -0.5139851 ,  0.29415268,  0.21542042, -0.24628259,\n",
       "        0.41379645, -0.18711652,  0.52018356,  0.21950574, -0.3409853 ,\n",
       "        0.01788345, -0.46720946, -0.12872285, -0.8810972 , -0.30568916,\n",
       "       -0.02606739,  0.11948229,  0.27221876, -0.52881414, -0.4259258 ,\n",
       "        0.41650102, -0.15389182,  0.02934496, -0.6352873 ,  0.32474402,\n",
       "       -0.26683256, -0.49505913, -0.16175579,  0.8818131 ,  0.2595341 ,\n",
       "        0.86174184,  0.02753459, -0.33733895,  0.70683104, -0.11353868,\n",
       "        0.24454863, -0.2157256 , -0.8657876 , -1.1171352 , -0.02220033,\n",
       "        0.15512446,  0.22437336, -0.01236017,  0.6065987 , -0.35662514,\n",
       "        0.66009563,  0.28859836, -0.1262713 , -0.2111151 , -0.22803454,\n",
       "        0.08761378,  0.4209012 ,  0.74855375,  0.20781215,  0.61807764,\n",
       "       -0.17365997,  0.214003  , -0.4686314 ,  0.5212136 ,  0.5153594 ,\n",
       "       -0.44022372, -0.7664265 ,  0.41347426, -0.09752589, -0.2872647 ,\n",
       "       -0.557281  , -0.71399504, -0.65589553, -0.2603704 , -0.11371176,\n",
       "       -0.609448  , -0.07001646,  0.636112  , -0.25325775,  0.28790256,\n",
       "       -0.33169198,  0.05294959,  0.1805896 ,  0.2615829 , -0.24582227,\n",
       "        0.33856547, -0.0512499 , -0.02957587,  0.64063257, -0.1829093 ,\n",
       "       -0.73242825, -0.2704433 ,  0.36278155, -0.35974804,  0.39430586,\n",
       "       -0.27230272, -0.6519182 , -0.93138534, -1.1512866 , -0.7323836 ,\n",
       "        0.61302245, -0.22740413, -0.00990492, -0.84488845, -0.1056196 ,\n",
       "        0.70396966,  0.14775543,  0.53270894, -0.4396598 , -0.79736084,\n",
       "       -0.3853714 ,  0.40474775,  0.3046405 , -0.04313217,  0.39985287,\n",
       "       -0.15978505, -0.3043094 , -0.2252863 , -0.21601665, -0.136644  ,\n",
       "       -0.31073892, -0.32637554,  0.11053207, -0.1036809 , -0.14625134,\n",
       "       -0.18076512, -0.42690063,  0.22205207,  0.3163671 ,  0.15833367,\n",
       "       -0.07255705,  0.59325534,  0.43786815,  0.3686835 , -0.27351955,\n",
       "        0.3875064 , -0.02609326,  0.28309336, -0.7678588 , -0.27235356,\n",
       "        0.4552346 , -0.4227279 ,  0.48779547,  0.2978846 , -0.23085308,\n",
       "       -0.13512912,  0.06021459,  0.3124325 , -0.31322917, -0.11257487,\n",
       "        0.03157337, -0.0156257 , -0.23530649, -0.3209274 , -1.162374  ,\n",
       "       -0.0201298 ,  0.49156043,  0.33720937, -0.25781918,  0.93776095],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1530, 2229, 5881, ..., 5068, 6335, 4861]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(bread.reshape(1,-1), foo['embeddings']).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'breadwinner'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo['token_to_text'][5068]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-b77c507d6328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "foo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(foo['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bread.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
