{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "809\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load the matching document contents\n",
    "df_text = pd.read_csv(\"data/trackGBV_xls_match.csv\")\n",
    "print(len(df_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "809\n"
     ]
    }
   ],
   "source": [
    "df_labels = pd.read_csv(\"data/trackGBV_labels.csv\")\n",
    "print(len(df_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get placeholder for start of actual text\n",
    "import re \n",
    "  \n",
    "df_text['contents']= df_text['contents'].str.replace(\"\\nJUDGMENT\", \"\\nDF_CUTOFF_MARK\", case = True) \n",
    "df_text['contents']= df_text['contents'].str.replace(\"\\nSENTENCE\", \"\\nDF_CUTOFF_MARK\", case = True)\n",
    "df_text['contents']= df_text['contents'].str.replace(\"\\nRULING\", \"\\nDF_CUTOFF_MARK\", case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a long time (> 30 minutes)\n",
    "text2 = [re.sub(r'((.|\\n)*)\\nDF_CUTOFF_MARK','', i) for i in text] # placeholder for religon MAKES IT WORSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "import csv\n",
    "\n",
    "with open('cleaned_text.csv', 'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = text\n",
    "labels = list(df_labels['Discrimination_Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.20, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import *\n",
    "cv = CountVectorizer(strip_accents='ascii', \n",
    "                     lowercase=True, \n",
    "                     token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b',\n",
    "                     stop_words='english')\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary is 16210\n",
      "The average number of non-zero features per example is 449.1422\n",
      "The fraction of the entries in the matrix that are non-zero is 290595/10487870(0.0277)\n",
      "The 0th and last feature strings are \"00\" and \"½cm\"\n",
      "The size of the vocabulary with bigram and trigrams is 20829\n"
     ]
    }
   ],
   "source": [
    "print('The size of the vocabulary is ', vect.shape[1], sep = '')\n",
    "print('The average number of non-zero features per example is ', round(vect.nnz/vect.shape[0],4), sep = '')\n",
    "print('The fraction of the entries in the matrix that are non-zero is ', vect.nnz, '/', vect.shape[0] * vect.shape[1], '(', round(vect.nnz / (vect.shape[0] * vect.shape[1]),4), ')',  sep = '')\n",
    "print('The 0th and last feature strings are \"', vectorizer.get_feature_names()[0], '\" and \"', vectorizer.get_feature_names()[-1], '\"', sep = '')\n",
    "\n",
    "bt_vectorizer = CountVectorizer(analyzer = 'char', ngram_range=(2, 3))\n",
    "bt_vect = bt_vectorizer.fit_transform(X_train)\n",
    "print('The size of the vocabulary with bigram and trigrams is ', len(bt_vectorizer.vocabulary_), sep = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>court</th>\n",
       "      <td>8888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>years</th>\n",
       "      <td>7924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence</th>\n",
       "      <td>7134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accused</th>\n",
       "      <td>5082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>4780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lcj</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laxity</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lawgivers</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lawfulness</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lawanuku</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15219 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "court       8888\n",
       "years       7924\n",
       "sentence    7134\n",
       "accused     5082\n",
       "state       4780\n",
       "...          ...\n",
       "lcj            1\n",
       "laxity         1\n",
       "lawgivers      1\n",
       "lawfulness     1\n",
       "lawanuku       1\n",
       "\n",
       "[15219 rows x 1 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check training data\n",
    "word_freq_df = pd.DataFrame(X_train_cv.toarray(), columns=cv.get_feature_names())\n",
    "top_words_df = pd.DataFrame(word_freq_df.sum()).sort_values(0, ascending=False)\n",
    "top_words_df.head()\n",
    "top_words_df.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and predict\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes =MultinomialNB()\n",
    "naive_bayes.fit(X_train_cv, y_train)\n",
    "predictions = naive_bayes.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.522\n",
      "Precision:  0.571\n",
      "Recall:  0.596\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "print('Accuracy: ', round(accuracy_score(y_test, predictions),3))\n",
    "print('Precision: ', round(precision_score(y_test, predictions),3))\n",
    "print('Recall: ', round(recall_score(y_test, predictions),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear', C = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_cv,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_svc = clf.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.4567901234567901\n",
      "Precision score:  0.5227272727272727\n",
      "Recall score:  0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "print('Accuracy score: ', accuracy_score(y_test, predictions_svc))\n",
    "print('Precision score: ', precision_score(y_test, predictions_svc))\n",
    "print('Recall score: ', recall_score(y_test, predictions_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.4382716049382716\n",
      "Precision score:  0.5054945054945055\n",
      "Recall score:  0.5\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(random_state=0, solver='liblinear',multi_class='ovr').fit(X_train_cv, y_train)\n",
    "predictions_LR = LR.predict(X_test_cv)\n",
    "print('Accuracy score: ', accuracy_score(y_test, predictions_LR))\n",
    "print('Precision score: ', precision_score(y_test, predictions_LR))\n",
    "print('Recall score: ', recall_score(y_test, predictions_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Home | Databases | WorldLII | Search | Feedbac...\n",
       "1      Home | Databases | WorldLII | Search | Feedbac...\n",
       "2         State v Lagivere - Sentence [2017] FJHC 386...\n",
       "3         State v Goundar - Sentence [2018] FJHC 438;...\n",
       "4      Home | Databases | WorldLII | Search | Feedbac...\n",
       "                             ...                        \n",
       "804    Home | Databases | WorldLII | Search | Feedbac...\n",
       "805       State v Khelawan [2016] FJMC 41; Criminal C...\n",
       "806       Bulivou v State [2014] FJCA 215; AAU78.2010...\n",
       "807       State v Lal [2015] FJMC 58; Criminal Case 1...\n",
       "808       Ram v State [2015] FJSC 26; CAV12.2015 (23 ...\n",
       "Name: contents, Length: 809, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text['contents']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
