{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import ktrain\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_cut(x, max_len):\n",
    "    \"\"\"\n",
    "    Either pads or cuts an document's embedding matrix.\n",
    "    \"\"\"\n",
    "    # Cut to the maximum length; cheaper than testing\n",
    "    x = x[0:max_len,:]\n",
    "    \n",
    "    # Pad with zeros\n",
    "    len_diff = max_len - len(x)\n",
    "    if len_diff > 0:\n",
    "        x = np.concatenate((x, np.zeros((len_diff, x.shape[1]))))\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedded_text(data_path, max_len='max',\n",
    "                        val_split=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Returns training, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        len -> either str or int.  If str, must be 'max'.\n",
    "               'max' indicates that the documents should be\n",
    "               padded to the max document length in the training\n",
    "               set.\n",
    "        val_split -> either float or bool.  If bool, must be\n",
    "                     False. If False, there is no true train/val\n",
    "                     split.  Rather, the test set is returned as\n",
    "                     the val set for consistency.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    fname = data_path\n",
    "    with open(fname, 'rb') as fp:\n",
    "        df = pickle.load(fp)\n",
    "    \n",
    "    # Break into train and test\n",
    "    train_mask = df.doc_use == 'train'\n",
    "    train = df[train_mask]\n",
    "    test = df[~train_mask]\n",
    "    \n",
    "    # Stack documents\n",
    "    train_embeddings = [np.stack(train.embeddings[train.docid == ID]) for ID in train.docid.unique()]\n",
    "    test_embeddings = [np.stack(test.embeddings[test.docid == ID]) for ID in test.docid.unique()]\n",
    "    \n",
    "    # Pad documents\n",
    "    if max_len == 'max':\n",
    "        max_len = max([len(doc) for doc in train_embeddings])\n",
    "    \n",
    "    x_train = [pad_or_cut(doc, max_len) for doc in train_embeddings]\n",
    "    x_train = np.stack(x_train)#.transpose(0, 2, 1)\n",
    "    \n",
    "    x_test = [pad_or_cut(doc, max_len) for doc in test_embeddings]\n",
    "    x_test = np.stack(x_test)#.transpose(0, 2, 1)\n",
    "    \n",
    "    y_train = train.groupby('docid').first()['label'].values\n",
    "    y_test = test.groupby('docid').first()['label'].values\n",
    "    \n",
    "    # Build a validation set from the training set\n",
    "    if val_split is False:\n",
    "        return x_train, y_train, x_test, y_test, x_test, y_test\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_split,\n",
    "                                                      random_state=random_state)\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_train, y_train,\n",
    "                   x_val, y_val,\n",
    "                   x_test, y_test,\n",
    "                   optimizer, loss, metrics,\n",
    "                   lr, eval_lr=False):\n",
    "    \"\"\"\n",
    "    Runs a model evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=metrics)\n",
    "    \n",
    "    learner = ktrain.get_learner(model,\n",
    "                                train_data=(x_train, y_train),\n",
    "                                val_data=(x_val, y_val))\n",
    "    learner.reset_weights()\n",
    "    \n",
    "    if eval_lr:\n",
    "        learner.lr_find(show_plot=True)\n",
    "        plt.show()\n",
    "    \n",
    "    learner.autofit(lr, reduce_on_plateau=5, early_stopping=10)\n",
    "    \n",
    "    y_hat = learner.model.predict(x_test).flatten() > 0.5\n",
    "    \n",
    "    print(classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val, x_test, y_test = build_embedded_text('data/baseBert_embeddings_olap_200.pkl', val_split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.keras.layers.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "conv2 = tf.keras.layers.Conv1D(filters=32,\n",
    "                               kernel_size=2,\n",
    "                               activation='relu')(embeddings)\n",
    "maxpool2 = tf.keras.layers.MaxPool1D(pool_size=x_train.shape[1]-1)(conv2)\n",
    "\n",
    "conv3 = tf.keras.layers.Conv1D(filters=32,\n",
    "                               kernel_size=3,\n",
    "                               activation='relu')(embeddings)\n",
    "maxpool3 = tf.keras.layers.MaxPool1D(pool_size=x_train.shape[1]-2)(conv3)\n",
    "\n",
    "conv4 = tf.keras.layers.Conv1D(filters=32,\n",
    "                               kernel_size=4,\n",
    "                               activation='relu')(embeddings)\n",
    "maxpool4 = tf.keras.layers.MaxPool1D(pool_size=x_train.shape[1]-3)(conv4)\n",
    "\n",
    "conv5 = tf.keras.layers.Conv1D(filters=32,\n",
    "                               kernel_size=5,\n",
    "                               activation='relu')(embeddings)\n",
    "maxpool5 = tf.keras.layers.MaxPool1D(pool_size=x_train.shape[1]-4)(conv5)\n",
    "\n",
    "concat = tf.keras.layers.concatenate([maxpool2, maxpool3, maxpool4, maxpool5])\n",
    "\n",
    "dense1 = tf.keras.layers.Dense(64, activation='relu')(concat)\n",
    "dropout = tf.keras.layers.Dropout(0.1)(dense1)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=embeddings, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 109, 768)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 108, 32)      49184       input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 107, 32)      73760       input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 106, 32)      98336       input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 105, 32)      122912      input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling1D) (None, 1, 32)        0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling1D) (None, 1, 32)        0           conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling1D) (None, 1, 32)        0           conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling1D) (None, 1, 32)        0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1, 128)       0           max_pooling1d_44[0][0]           \n",
      "                                                                 max_pooling1d_45[0][0]           \n",
      "                                                                 max_pooling1d_46[0][0]           \n",
      "                                                                 max_pooling1d_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 1, 64)        8256        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 1, 64)        0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 1, 1)         65          dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 352,513\n",
      "Trainable params: 352,513\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights have been reset.\n",
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 1e-05...\n",
      "Train on 641 samples, validate on 162 samples\n",
      "Epoch 1/1024\n",
      "641/641 [==============================] - 1s 2ms/sample - loss: 0.6982 - acc: 0.5413 - val_loss: 0.6728 - val_acc: 0.5988\n",
      "Epoch 2/1024\n",
      "641/641 [==============================] - 0s 528us/sample - loss: 0.6814 - acc: 0.5491 - val_loss: 0.6738 - val_acc: 0.5988\n",
      "Epoch 3/1024\n",
      "641/641 [==============================] - 0s 500us/sample - loss: 0.6886 - acc: 0.5538 - val_loss: 0.6754 - val_acc: 0.5741\n",
      "Epoch 4/1024\n",
      "641/641 [==============================] - 0s 514us/sample - loss: 0.6915 - acc: 0.5694 - val_loss: 0.6769 - val_acc: 0.5741\n",
      "Epoch 5/1024\n",
      "641/641 [==============================] - 0s 527us/sample - loss: 0.6856 - acc: 0.5413 - val_loss: 0.6750 - val_acc: 0.5988\n",
      "Epoch 6/1024\n",
      "544/641 [========================>.....] - ETA: 0s - loss: 0.6901 - acc: 0.5643\n",
      "Epoch 00006: Reducing Max LR on Plateau: new max lr will be 5e-06 (if not early_stopping).\n",
      "641/641 [==============================] - 0s 551us/sample - loss: 0.6862 - acc: 0.5725 - val_loss: 0.6742 - val_acc: 0.5988\n",
      "Epoch 7/1024\n",
      "641/641 [==============================] - 0s 509us/sample - loss: 0.6797 - acc: 0.5523 - val_loss: 0.6750 - val_acc: 0.5988\n",
      "Epoch 8/1024\n",
      "641/641 [==============================] - 0s 519us/sample - loss: 0.6799 - acc: 0.5772 - val_loss: 0.6759 - val_acc: 0.5802\n",
      "Epoch 9/1024\n",
      "641/641 [==============================] - 0s 513us/sample - loss: 0.6860 - acc: 0.5694 - val_loss: 0.6767 - val_acc: 0.5741\n",
      "Epoch 10/1024\n",
      "641/641 [==============================] - 0s 523us/sample - loss: 0.6772 - acc: 0.5803 - val_loss: 0.6754 - val_acc: 0.5926\n",
      "Epoch 11/1024\n",
      "576/641 [=========================>....] - ETA: 0s - loss: 0.6889 - acc: 0.5556\n",
      "Epoch 00011: Reducing Max LR on Plateau: new max lr will be 2.5e-06 (if not early_stopping).\n",
      "Restoring model weights from the end of the best epoch.\n",
      "641/641 [==============================] - 0s 544us/sample - loss: 0.6888 - acc: 0.5601 - val_loss: 0.6745 - val_acc: 0.5988\n",
      "Epoch 00011: early stopping\n",
      "Weights from best epoch have been loaded into model.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        65\n",
      "           1       0.60      1.00      0.75        97\n",
      "\n",
      "    accuracy                           0.60       162\n",
      "   macro avg       0.30      0.50      0.37       162\n",
      "weighted avg       0.36      0.60      0.45       162\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, x_train, y_train, x_val, y_val, x_test, y_test,\n",
    "               optimizer='adam', loss='binary_crossentropy', metrics=['acc'],\n",
    "               lr=10e-6, eval_lr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
