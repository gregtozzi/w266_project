{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import transformers as ppb\n",
    "from transformers import RobertaTokenizer, DistilBertTokenizer, BertTokenizer, RobertaModel, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Variables\n",
    "chunk_len = 512 #200\n",
    "overlap_len = 0 #50\n",
    "\n",
    "BERT_tokenizer_class = ppb.DistilBertTokenizer\n",
    "BERT_pre_trained_weights = 'distilbert-base-cased'\n",
    "tokenizer = BERT_tokenizer_class.from_pretrained(BERT_pre_trained_weights)\n",
    "model1 = ppb.DistilBertModel.from_pretrained(BERT_pre_trained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>len_txt</th>\n",
       "      <th>doc_use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>253149</td>\n",
       "      <td>judgment casey kapi and handley jj the appella...</td>\n",
       "      <td>1</td>\n",
       "      <td>834</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>77351</td>\n",
       "      <td>sentence 1 mr pita tiqatabua stands convicted ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1168</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>69881</td>\n",
       "      <td>judgment on the DATE the appellant was convict...</td>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>268783</td>\n",
       "      <td>judgment and sentence from 22 to 25 and DATE t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1211</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>83088</td>\n",
       "      <td>sentence 1 the director of public prosecution ...</td>\n",
       "      <td>1</td>\n",
       "      <td>511</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>284255</td>\n",
       "      <td>sentence mr kaverieli vatuorooro you tendered ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1266</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>71610</td>\n",
       "      <td>sentence mr kalaveti ratu nawaqamate you stand...</td>\n",
       "      <td>1</td>\n",
       "      <td>850</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>79602</td>\n",
       "      <td>sentence the names of the victim and the accus...</td>\n",
       "      <td>1</td>\n",
       "      <td>2212</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>82347</td>\n",
       "      <td>sentence 1 name and identity of the virtual co...</td>\n",
       "      <td>1</td>\n",
       "      <td>1398</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>81538</td>\n",
       "      <td>sentence 1 taniela vakalaca you appear for sen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1887</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      docid                                               text  label  \\\n",
       "361  253149  judgment casey kapi and handley jj the appella...      1   \n",
       "777   77351  sentence 1 mr pita tiqatabua stands convicted ...      1   \n",
       "572   69881  judgment on the DATE the appellant was convict...      1   \n",
       "569  268783  judgment and sentence from 22 to 25 and DATE t...      1   \n",
       "611   83088  sentence 1 the director of public prosecution ...      1   \n",
       "583  284255  sentence mr kaverieli vatuorooro you tendered ...      0   \n",
       "509   71610  sentence mr kalaveti ratu nawaqamate you stand...      1   \n",
       "187   79602  sentence the names of the victim and the accus...      1   \n",
       "32    82347  sentence 1 name and identity of the virtual co...      1   \n",
       "548   81538  sentence 1 taniela vakalaca you appear for sen...      1   \n",
       "\n",
       "     len_txt doc_use  \n",
       "361      834   train  \n",
       "777     1168    test  \n",
       "572      665   train  \n",
       "569     1211   train  \n",
       "611      511   train  \n",
       "583     1266   train  \n",
       "509      850   train  \n",
       "187     2212   train  \n",
       "32      1398   train  \n",
       "548     1887   train  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset into a pandas dataframe.\n",
    "# Because we want to get embeddings for both train and test sets we are going to do together\n",
    "# Add a column to preserve where the doc came from\n",
    "\n",
    "train_raw = pd.read_csv(\"data/train_lcase.csv\")\n",
    "train_raw['doc_use'] = 'train'\n",
    "test_raw = pd.read_csv(\"data/test_lcase.csv\")\n",
    "test_raw['doc_use'] = 'test'\n",
    "\n",
    "df_raw = pd.concat([train_raw, test_raw])\n",
    "df_raw.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df_raw.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Extract all the tokenize elements out of data_tokenize from the above tokenizer.encode_plus\n",
    "# gives us input ids, attention mask and critically overflow tokens\n",
    "\n",
    "def extract_tokens(data_tokenize, targets):\n",
    "\n",
    "    previous_input_ids = data_tokenize[\"input_ids\"].reshape(-1) # a tensor of the input IDs )\n",
    "    previous_attention_mask = data_tokenize[\"attention_mask\"].reshape(-1) # a tensor of the attention mask (200 * 1)\n",
    "    previous_token_type_ids = data_tokenize[\"token_type_ids\"].reshape(-1) # a tensor of the attention mask (200 * 0)\n",
    "    remain = data_tokenize.get(\"overflowing_tokens\") # list of the overflow tokens\n",
    "    targets = torch.tensor(targets, dtype=torch.int) # a tensor of current target (1)\n",
    "\n",
    "    return previous_input_ids, previous_attention_mask, previous_token_type_ids, remain, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Tokenize Runtime: 0:00:22.232795\n"
     ]
    }
   ],
   "source": [
    "# Do the tokenization\n",
    "# This returns a transformers object with 5 elements\n",
    "# We only really need the input_ids and attention mask for modelling\n",
    "# We will use these IDS to get out embeddings\n",
    "\n",
    "# overflowing_tokens (list) - all the elements after our 200 word split\n",
    "# num_truncated_tokens (integer) - how many overflow tokens we have, for text[0] it is 1822\n",
    "# input_ids (tensor) - the first 200 tokens, with special token 101 at the beginning and 102 at end\n",
    "# token_type_ids (tensor) - the token types for the input - there are 200, ours are all zero's (why?)\n",
    "# attention_mask (tensor) - attention mask in case our text < 200 tokens\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Reset lists\n",
    "long_terms_token = []\n",
    "input_ids_list = []\n",
    "attention_mask_list = []\n",
    "token_type_ids_list = []\n",
    "targets_list = []\n",
    "\n",
    "input_ids_list_head = []\n",
    "attention_mask_list_head = []\n",
    "token_type_ids_list_head = []\n",
    "targets_list_head = []\n",
    "\n",
    "input_ids_list_olap = []\n",
    "attention_mask_list_olap = []\n",
    "token_type_ids_list_olap= []\n",
    "targets_list_olap= []\n",
    "\n",
    "input_ids_list_tail = []\n",
    "attention_mask_list_tail = []\n",
    "token_type_ids_list_tail = []\n",
    "targets_list_tail= []\n",
    "\n",
    "for idx in range(len(df_raw)): \n",
    "    \n",
    "    long_terms_token = []\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    token_type_ids_list = []\n",
    "    targets_list = []\n",
    "        \n",
    "    # tokenize for this row in train_raw\n",
    "    data = tokenizer.encode_plus(\n",
    "        df_raw['text'][idx],\n",
    "        max_length=chunk_len,\n",
    "        pad_to_max_length=True,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_tensors='pt')\n",
    "\n",
    "    # grab the targets for this row in train_raw\n",
    "    targets = int(df_raw['label'][idx])\n",
    "    \n",
    "    # extract the tokens\n",
    "    input_ids, attention_mask, token_type_ids, remain, targets = extract_tokens(data, targets)\n",
    "    remain = [] if remain is None else remain # For cases where there is no overflow\n",
    "    \n",
    "    # CREATE LISTS FOR THE HEAD\n",
    "    input_ids_list_head.append(input_ids)\n",
    "    attention_mask_list_head.append(attention_mask)\n",
    "    token_type_ids_list_head.append(token_type_ids)\n",
    "    targets_list_head.append(targets)\n",
    "    \n",
    "    # GET OVERLAPPING TOKEN LISTS *****************************\n",
    "    remain = torch.tensor(remain, dtype=torch.long)\n",
    "    idxs = range(len(remain)+ chunk_len)\n",
    "    idxs = idxs[(chunk_len-overlap_len-2)::(chunk_len-overlap_len-2)]\n",
    "    input_ids_first_overlap = input_ids[-(overlap_len+1):-1]\n",
    "    start_token = torch.tensor([101], dtype=torch.long)\n",
    "    end_token = torch.tensor([102], dtype=torch.long)\n",
    "    \n",
    "    # Get the initial 200 word tensors (same as head)\n",
    "    input_ids_list.append(input_ids)\n",
    "    attention_mask_list.append(attention_mask)\n",
    "    token_type_ids_list.append(token_type_ids)\n",
    "    targets_list.append(targets)\n",
    "    \n",
    "    # For each overlapping section create a tensor of input_ids, attention_masks, token_type_ids and targets (labels)\n",
    "    # add to a list\n",
    "    for i, idx in enumerate(idxs):\n",
    "        if i == 0:\n",
    "            input_ids = torch.cat((input_ids_first_overlap, remain[:idx]))\n",
    "        elif i == len(idxs):\n",
    "            input_ids = remain[idx:]\n",
    "        elif previous_idx >= len(remain):\n",
    "            break\n",
    "        else:\n",
    "            input_ids = remain[(previous_idx-overlap_len):idx]\n",
    "\n",
    "        previous_idx = idx\n",
    "\n",
    "        nb_token = len(input_ids)+2\n",
    "        attention_mask = torch.ones(chunk_len, dtype=torch.long)\n",
    "        attention_mask[nb_token:chunk_len] = 0\n",
    "        token_type_ids = torch.zeros(chunk_len, dtype=torch.long)\n",
    "        input_ids = torch.cat((start_token, input_ids, end_token))\n",
    "        if chunk_len-nb_token > 0:\n",
    "            padding = torch.zeros(chunk_len-nb_token, dtype=torch.long)\n",
    "            input_ids = torch.cat((input_ids, padding))\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        token_type_ids_list.append(token_type_ids)\n",
    "        targets_list.append(targets)\n",
    "    \n",
    "    # Add to the overlap list\n",
    "    input_ids_list_olap.append([input_ids_list])\n",
    "    attention_mask_list_olap.append([attention_mask_list])\n",
    "    token_type_ids_list_olap.append([token_type_ids_list])\n",
    "    targets_list_olap.append([targets_list])      \n",
    "  \n",
    "\n",
    "    # GET LISTS FOR THE HEAD\n",
    "    input_ids_list_tail.append([input_ids_list[-1]])\n",
    "    attention_mask_list_tail.append([attention_mask_list[-1]])\n",
    "    token_type_ids_list_tail.append([token_type_ids_list[-1]])\n",
    "    targets_list_tail.append([targets_list[-1]])\n",
    "    \n",
    "print(f'BERT Tokenize Runtime: {datetime.datetime.now() - start_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 803\n",
      "\n",
      "Head input_ids length: 803\n",
      "Head attention mask length: 803\n",
      "Head target length: 803\n",
      "\n",
      "Tail input_ids length: 803\n",
      "Tail attention mask length: 803\n",
      "Tail target length: 803\n",
      "\n",
      "Overlap input_ids length: 803\n",
      "Overlap attention mask length: 803\n",
      "Overlap target length: 803\n"
     ]
    }
   ],
   "source": [
    "# check we have the correct sizes of things (641)\n",
    "print(\"Train length:\", len(df_raw))\n",
    "print(\"\")\n",
    "print(\"Head input_ids length:\", len(input_ids_list_head))\n",
    "print(\"Head attention mask length:\", len(attention_mask_list_head))\n",
    "print(\"Head target length:\", len(targets_list_head))\n",
    "print(\"\")\n",
    "print(\"Tail input_ids length:\", len(input_ids_list_tail))\n",
    "print(\"Tail attention mask length:\", len(attention_mask_list_tail))\n",
    "print(\"Tail target length:\", len(targets_list_tail))\n",
    "print(\"\")\n",
    "print(\"Overlap input_ids length:\", len(input_ids_list_olap))\n",
    "print(\"Overlap attention mask length:\", len(attention_mask_list_olap))\n",
    "print(\"Overlap target length:\", len(targets_list_olap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlap STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lists so we can smash to a dataframe and save for reuse\n",
    "input_ids_l2_olap = []\n",
    "attention_mask_l2_olap = []\n",
    "\n",
    "for i in range(len(input_ids_list_olap)):\n",
    "    input_ids_l2_olap.append(input_ids_list_olap[i][0])\n",
    "    attention_mask_l2_olap.append(attention_mask_list_olap[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_use</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73277</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[[tensor(101), tensor(5650), tensor(122), tens...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79776</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[[tensor(101), tensor(5650), tensor(122), tens...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75870</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[[tensor(101), tensor(5650), tensor(122), tens...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79299</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>[[tensor(101), tensor(5650), tensor(122), tens...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80603</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[[tensor(101), tensor(9228), tensor(1104), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>74009</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>[[tensor(101), tensor(9228), tensor(1103), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>79379</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>[[tensor(101), tensor(5650), tensor(2666), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>251317</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>[[tensor(101), tensor(5650), tensor(122), tens...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>79318</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>[[tensor(101), tensor(5650), tensor(122), tens...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>255439</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>[[tensor(101), tensor(5650), tensor(122), tens...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>803 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      docid  label doc_use                                          input_ids  \\\n",
       "0     73277      0   train  [[tensor(101), tensor(5650), tensor(122), tens...   \n",
       "1     79776      1   train  [[tensor(101), tensor(5650), tensor(122), tens...   \n",
       "2     75870      1   train  [[tensor(101), tensor(5650), tensor(122), tens...   \n",
       "3     79299      1   train  [[tensor(101), tensor(5650), tensor(122), tens...   \n",
       "4     80603      0   train  [[tensor(101), tensor(9228), tensor(1104), ten...   \n",
       "..      ...    ...     ...                                                ...   \n",
       "798   74009      0    test  [[tensor(101), tensor(9228), tensor(1103), ten...   \n",
       "799   79379      1    test  [[tensor(101), tensor(5650), tensor(2666), ten...   \n",
       "800  251317      1    test  [[tensor(101), tensor(5650), tensor(122), tens...   \n",
       "801   79318      1    test  [[tensor(101), tensor(5650), tensor(122), tens...   \n",
       "802  255439      1    test  [[tensor(101), tensor(5650), tensor(122), tens...   \n",
       "\n",
       "                                        attention_mask  \n",
       "0    [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "1    [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "2    [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "3    [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "4    [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "..                                                 ...  \n",
       "798  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "799  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "800  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "801  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "802  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "\n",
       "[803 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to dataframe\n",
    "df_olap = pd.DataFrame()\n",
    "df_olap['docid'] = df_raw['docid']\n",
    "df_olap['label'] = df_raw['label']\n",
    "df_olap['doc_use'] = df_raw['doc_use']\n",
    "df_olap['input_ids'] = input_ids_l2_olap\n",
    "df_olap['attention_mask'] = attention_mask_l2_olap\n",
    "df_olap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two new dataframes with exploded input_ids and attention_masks\n",
    "\n",
    "df_olap_explode = df_olap.explode('input_ids')\n",
    "df_olap_explode.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df_olap_explode_attention_mask = df_olap.explode('attention_mask')\n",
    "df_olap_explode_attention_mask.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get out two lists of numpy arrays so we can create models\n",
    "arr_input_ids = df_olap_explode['input_ids'].to_numpy()\n",
    "arr_attention_mask = df_olap_explode_attention_mask['attention_mask'].to_numpy()\n",
    "\n",
    "input_ids_np_olap = []\n",
    "attention_mask_np_olap = []\n",
    "\n",
    "for i in range(len(arr_input_ids)):\n",
    "    input_ids_np_olap.append(arr_input_ids[i].numpy())\n",
    "    attention_mask_np_olap.append(arr_attention_mask[i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the exploded tokens and add back to the dataframe in case we want them later\n",
    "df_olap_explode['input_ids_np_olap'] = input_ids_np_olap\n",
    "df_olap_explode['attention_mask_np_olap'] = attention_mask_np_olap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some start and end chunk sizes to do our BERT embeddings as we don't have enough memory to do at once\n",
    "l_start = list(range(0,3000,100))\n",
    "l_end =   list(range(100,3100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3065"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids_np_olap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start =  0  end =  100\n",
      "BERT Model Runtime: 0:00:29.369143\n",
      "start =  100  end =  200\n",
      "BERT Model Runtime: 0:00:28.732952\n",
      "start =  200  end =  300\n",
      "BERT Model Runtime: 0:00:28.609379\n",
      "start =  300  end =  400\n",
      "BERT Model Runtime: 0:00:28.313254\n",
      "start =  400  end =  500\n",
      "BERT Model Runtime: 0:00:28.091394\n",
      "start =  500  end =  600\n",
      "BERT Model Runtime: 0:00:27.913035\n",
      "start =  600  end =  700\n",
      "BERT Model Runtime: 0:00:27.985982\n",
      "start =  700  end =  800\n",
      "BERT Model Runtime: 0:00:28.066032\n",
      "start =  800  end =  900\n",
      "BERT Model Runtime: 0:00:27.998753\n",
      "start =  900  end =  1000\n",
      "BERT Model Runtime: 0:00:27.958962\n",
      "start =  1000  end =  1100\n",
      "BERT Model Runtime: 0:00:27.918748\n",
      "start =  1100  end =  1200\n",
      "BERT Model Runtime: 0:00:27.998035\n",
      "start =  1200  end =  1300\n",
      "BERT Model Runtime: 0:00:28.269361\n",
      "start =  1300  end =  1400\n",
      "BERT Model Runtime: 0:00:28.095833\n",
      "start =  1400  end =  1500\n",
      "BERT Model Runtime: 0:00:28.080640\n",
      "start =  1500  end =  1600\n",
      "BERT Model Runtime: 0:00:28.193252\n",
      "start =  1600  end =  1700\n",
      "BERT Model Runtime: 0:00:28.380129\n",
      "start =  1700  end =  1800\n",
      "BERT Model Runtime: 0:00:28.538661\n",
      "start =  1800  end =  1900\n",
      "BERT Model Runtime: 0:00:28.303234\n",
      "start =  1900  end =  2000\n",
      "BERT Model Runtime: 0:00:28.295740\n",
      "start =  2000  end =  2100\n",
      "BERT Model Runtime: 0:00:28.485336\n",
      "start =  2100  end =  2200\n",
      "BERT Model Runtime: 0:00:28.412248\n",
      "start =  2200  end =  2300\n",
      "BERT Model Runtime: 0:00:28.652417\n",
      "start =  2300  end =  2400\n",
      "BERT Model Runtime: 0:00:28.887122\n",
      "start =  2400  end =  2500\n",
      "BERT Model Runtime: 0:00:28.651630\n",
      "start =  2500  end =  2600\n",
      "BERT Model Runtime: 0:00:28.666433\n",
      "start =  2600  end =  2700\n",
      "BERT Model Runtime: 0:00:28.685602\n",
      "start =  2700  end =  2800\n",
      "BERT Model Runtime: 0:00:28.741174\n",
      "start =  2800  end =  2900\n",
      "BERT Model Runtime: 0:00:28.767795\n",
      "start =  2900  end =  3000\n",
      "BERT Model Runtime: 0:00:30.306136\n"
     ]
    }
   ],
   "source": [
    "# Create Bert embeddings on the exploded documents and save to a list\n",
    "l_olap = []\n",
    "\n",
    "for i in range(len(l_start)):\n",
    "    \n",
    "    print(\"start = \", l_start[i], \" end = \", l_end[i])\n",
    "    \n",
    "    input_ids_np_olap_run = input_ids_np_olap[l_start[i]:l_end[i]]\n",
    "    attention_mask_np_olap_run =  attention_mask_np_olap[l_start[i]:l_end[i]]\n",
    "    \n",
    "    # Now we can run the model to get the Bert embedding\n",
    "    input_ids = torch.tensor(input_ids_np_olap_run)\n",
    "    attention_mask = torch.tensor(attention_mask_np_olap_run)\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "    with torch.no_grad(): #deactivates autograd engine\n",
    "        last_hidden_states_olap = model1(input_ids, attention_mask=attention_mask)\n",
    "    print(f'BERT Model Runtime: {datetime.datetime.now() - start_time}')\n",
    "    \n",
    "    olap_features = last_hidden_states_olap[0][:,0,:].numpy()\n",
    "    l_olap.append(olap_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model Runtime: 0:00:16.069901\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n",
    "input_ids_np_olap_run = input_ids_np_olap[3000:3100]\n",
    "attention_mask_np_olap_run =  attention_mask_np_olap[3000:3100]\n",
    "\n",
    "# Now we can run the model to get the Bert embedding\n",
    "input_ids = torch.tensor(input_ids_np_olap_run)\n",
    "attention_mask = torch.tensor(attention_mask_np_olap_run)\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "with torch.no_grad(): #deactivates autograd engine\n",
    "    last_hidden_states_olap = model1(input_ids, attention_mask=attention_mask)\n",
    "print(f'BERT Model Runtime: {datetime.datetime.now() - start_time}')\n",
    "\n",
    "olap_features = last_hidden_states_olap[0][:,0,:].numpy()\n",
    "l_olap.append(olap_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten out the ebeddngs - list. Now it is the same size as the overflow dimension\n",
    "l_olap_flat = [item for sublist in l_olap for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3065"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l_olap_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embeddings back to our dataframe\n",
    "df_olap_explode['embeddings'] = l_olap_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_olap_explode1 = df_olap_explode[['docid', 'label', 'doc_use','input_ids_np_olap', 'embeddings']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to disk as pickle file\n",
    "\n",
    "#fname = 'data/baseBert_embeddings_stack' + str(chunk_len) + '.pkl'\n",
    "fname = 'data/distilBert_embeddings_stack' + str(chunk_len) + '.pkl'\n",
    "\n",
    "with open(fname, 'wb') as fp:\n",
    "    pickle.dump(df_olap_explode1, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266",
   "language": "python",
   "name": "w266"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
